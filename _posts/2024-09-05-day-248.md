---
layout: post
title: (Day 248) New LLM project from my advisor
categories: [nlp,applying-knowledge]
---

# Hello :) Today is Day 248!
A quick summary of today:
* create a fake movie review dataset
* use LoRA to fine-tune llama3

So ~ I cannot share what the exact project is at this moment, or the process. However, to learn more about it I created the below dataset and learned how to fine-tune Llama3.

Who is my advising professor? Song Hee-Seok (송희석) of the Management Information Systems department at my university. His latest interests are also AI, ML and LLMs and their applications. Today I went to meet with him for the 1st time this semester and he shared with me about the project. As mentioned at the start I will not share the exact details as the project, and even the dataset is still being created.

Nevertheless, to prepare myself for the project I created some basic dataset and learned how to fine-tune Llama3. Below is all the info.

### [Creating a sample movie review dataset](https://huggingface.co/datasets/divakaivan/fake_movie_review_kr)

The dataset is in Korean as the project data will be in Korean too. Below is info about the type of data in the dataset. 

Using an online movie review dataset, I modified it so that each row of data consists of 50 individual reviews for 1 movie. Each review is some short piece of comment about a movie.

Once I did that, I passed each row of data (contained 50 movie reviews) to gpt-4o-mini (cost me ~$0.25) using the following prompt:

```
You are a helpful assistant that helps me evaluate Korean reviews. For each movie you are given 50 reviews. Analyze the reviews, and for the movie itself return a score(1 to 10) and explanation for each of the following criteria: Emotional, Characters, Plot, Visuals, Pacing. Return the review in Korean.
```

So the resulting data is all in the link above, uploaded to HuggingFace. There are two columns: 1 for the reviews, and a 2nd where the model evaluating the movie based on the factors in the prompt. I cannot say exactly why I did it this way, but it will be helpful later for the actual project data. 

### [Using Parameter-Efficient Fine-Tuning (PEFT) to fine-tune Llama3](https://huggingface.co/divakaivan/llama3-finetuned-reviewer-kr)

The above link will take you to the fine-tuned llama3-8b that I uploaded to HuggingFace. 

On HF, I found this company Unsloth that has a [nice notebook](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp) for PEFT on popular models, and llama3-8b is one of them.

After I created the dataset, I started following the code in the notebook to fine-tune llama3-8b on my own data and get a fine-tuned model (which I uploaded to HF as well). 

---

Very excited for this project, but the official data that will come from the customer (again I cannot share any details) will come after around a month. During that time I will keep learning, and explore other PEFT methods and try them out on my sample dataset.

That is all for today!

See you tomorrow :)
