---
layout: post
title: (Day 67) Build a LLM from scratch chapter 3 - self-attention from scratch
categories: [applying-knowledge,theory,nlp]
---

## Hello :) Today is Day 67!
A quick summary of today:
* Covered chapter 3 of [Build a LLM from scratch](https://www.manning.com/books/build-a-large-language-model-from-scratch) by Sebastian Raschka

Below are just notes with general overview, but the bigger part - with all the code implementations, developing each self-attention part from scratch is on [my github repo](https://github.com/divakaivan/llm-from-scratch-learn). I uploaded yesterday's chapter's content too as well.

In this chapter, the attention mechanism is explored in depth.

![](https://blogger.googleusercontent.com/img/a/AVvXsEhGJBs0uVu_zXeX5AaFHDX4dcKZbyyiNUDkeZqkRo4uQWKrSIi5dF3FGUkNLH6_OBFd34HG_1QTTRfpja7jdVrQimng-dijYcdQA_ZW44VJcF3x5AuLmN7i0nG1eB9kghOQbIKcfJdG4dV8LwZeQxHr-3p8_px1ub-lN_LXhwG4AZO06efjnw3LN5KTmmgJ)

Attention can be split into 4 parts that build on each other.

![](https://blogger.googleusercontent.com/img/a/AVvXsEhveKSyaICVtTYPWEr9ZE4DFrEPiHES7VmEGXeeZDLUD1FRGt7SuOOkMUnz9o8rHd8fRcmfg-4SaXzHl6beE4gHVjcrudz8iabSab9F2lJPmCLfKmjVFebyfGcd2DPrbtYXtvIP5w4b9i5D7-jpMnIrLHCcW7Jj8DGZQ8TpX5pZvJGLZ1H6qawVAwFB4sXo)

3.1 The problem with modeling long sequences

Imagine we want to develop a machine translation model - in different languages word order might be different (even if it follows the same verb object subject format). When translating we would want our model to see words that appear earlier/later, and not translate word by word because that might result in something incorrect.

![](https://blogger.googleusercontent.com/img/a/AVvXsEhmUH155iTjvqZ1B3-9eA4v7XFIYLPw0s9hpZRs3t2YQFU1w1AKzU2Db6nbYAPLToMaykAc7rWgu0khrAd9J0Rb63pHSmtzbrv6Ij8P0abscAANFnMLin4mRhWFt49B54gAdS36CCnrukpg4cicyGsV4n5Wa7YuL2lvUHaL5w_jHPTGDvUZwmyUCu1GU8Yi)

Before Transformers, there were RNNs but the problem with them was that before the info is passed to the decoder, all that information is stored in the final hidden state of the encoder. After each token in a sentence the hidden state is computed sequentially, and by the time we get to the end, some of the information from the start might have been lost (RNNs have a struggle with capturing long-distance dependencies). To solve this problem, attention was designed.

3.2 Capturing data dependencies with attention mechanisms

In 2014, a Bahdanau attention was developed to help RNNs to selectively access different parts of the input at each decoding step. Below is a general idea of how attention works:

![](https://blogger.googleusercontent.com/img/a/AVvXsEiJVPNyemxTkuX1NjhTl2NAg0cZa2Z1hyCUaD0JMBua0Zcccz2JQFs6HduCrGEu_SwBQdfasDwzy1zh4_rknMPMuUoqmw00fuPrtlXphc7OzQPiLQKFC_9HJyx_cYvRqasbft0HNbb8jPvoB9E7cJLqvKIns8m7_RKprSaoXa44OIqtPmEDTpgjANPjLPM-)

Three years later, the original transformer architecture that uses self-attention was proposed. Self-attention allows each token in the input to attend to all other positions in the input when computing the representation for that particular sequence. 

3.3 Attending to different parts of the input with self-attention

The traditional attention focuses on relationships between elements of 2 sequences, like in seq2seq where the attention might be between an input and output sequence. 

However, the self-attention mechanism refers to its ability to compute attention weights by learning the relationship between different positions within a single input. This allows the model to determine the importance of each token in the input sequence w.r.t. every other token in the same sequence.

![](https://blogger.googleusercontent.com/img/a/AVvXsEgoJgzEdmT7GfRrGIUq06LVYe98K6_5vxg35FEdxA2sJ1ZBRS1S4cjd4HbV_Jtj3l8xXHfA2DkfHSgRwzwp6RM3ccFUauK9H6eg8FMkq492BVvz62OqtkMbkuu5xzKA53SmYpj7espceisFPPnKNbddSece0uwnvecbbHUGxTzfY6if37Hx-EV0PtQDkDPr)

3.4 Implementing self-attention with trainable weights

The self-attention used in GPT models and other popular LLMs is called scaled dot-product attention. This is the second step in the self-attention map.

![](https://blogger.googleusercontent.com/img/a/AVvXsEj50_swyBipiNEn55L8nbPtaKEcI61F0KewExrj4ckO9YtvzlDKpVIxmXzdeZvBXCe5L5TDtm9BuHGobPxAMxemYkDbojYG_EXI9TP6cXNKY54QV8FsKHsu69GyCj71OXVk8oDj8E7-KAinbfZ30Srll-tOK462upyQSdeFghbAYc0-9HvDLrVjqqiAzAmB)

The biggest difference is that here, weight matrices are introduced that are updated in training time, and are crucial for the model to learn to produce good context vectors. 

In code:

![](https://blogger.googleusercontent.com/img/a/AVvXsEg0CbFudVAhks0tLQbTpIyoazho7FFI8Zt4mGaGLxCKvV2j3GJeRkXEHCqbCHN6dwEGRDX9elmXQiDvKCAkMFxLW-sup7kSPFbRWmSJTxZau90fN4w4QWodfQWLliZOHzzzMtd-jGqbvVmVwtyvzcGeR5WbX2FId88kq52nodY6nacZmSG-jKdx80fXypS6)

3.5 Hiding future words with causal attention

The next step in achieving the self-attention used in GPT-like models is to create a causal attention machanism (a special form of self-attention). This is also known as masked attention, where while processing the current token, the model can see only previous and current inputs from a sequence (stopping it from seeing the future).

![](https://blogger.googleusercontent.com/img/a/AVvXsEjlR-sl5HohiGL1YfDgmjS10y1pdwOhHguvl5bIsuK5HTTf7MoycOyKpH3549ehEH2PQs01-WWx3djph9Og0gw50jfOsvIF3ecmkrS8b_zLLgSfnPSPq-ePWgBi4a5KxDLbvBDk3g4ObqNv_KK5hdoo5If26fbvVfnhBuyYLxFT4YGQh_axYoKqfVWIBAfI)

This is the 3rd step in the self-attention path:

![](https://blogger.googleusercontent.com/img/a/AVvXsEhPS7WsdXiKBQihI3HhimYySz__uwllTytF-uKpZKRTtdcmDeIDaFB_bk_ZBJYKsuyrzfQdstLrUmZ2NCH7EaRFAWPZKKNcjik13eKPifF9cW61qyz7ivh5b-t0FwTko6Q3nUpNSnlUzqk7bgEYPak_CDdlSa2E1KUhY2eNtgwO75FfqaxD-Q36cUEvlr3l)

In code:

![](https://blogger.googleusercontent.com/img/a/AVvXsEjrEG3IleCMSa_iIL0JXE0U6O_gy4v1TZLqHIsQGrfAfWQ5ii0hUhDKBugyNJfKLPC043AJ8nW30nL_vVIleH4NeBzc4PKnsQcs3pfNKo7558J_0drRyqyt_lbYQzAdJgUTMZxULfYLFuOWAENBZyBnrO4Z9CBLtQURF1sCmGxucgn8WlALF7-fXhBYsdUK)

3.6 Extending single-head attention to multi-head attention

Extending the previously developed causal attention to multiple heads. 'Multi-head' refers to dividing the model's attention into multiple heads, each performing 'magic' independently. 
The main idea is to run multiple attentions in parallel, using different linear projections, and each head learning different aspects of our data.

Idea:

![](https://blogger.googleusercontent.com/img/a/AVvXsEjnmwTcPdmwFJNx7jpA3DV1E-V139NQ-bgFxoJrFk9mxBmU1z508yvJurln8v_AWvjolm13PdBbsO7HFoJCT6u_rVsO-J4jOBC3vQx1eQ8JIlNx8FfwSNkJJIQzOXQirFUGW8A7bxOD63VEYF8PdCFIGhN-RSszU49oYHkjA37yKWc5_c5hxEVlgA4LcKWc)

Execution (2 pics):

![](https://blogger.googleusercontent.com/img/a/AVvXsEh4wGJzB3ulAck7T7RQTU-vRdXxn8iDuX1LicvPtisHmYGu0YueilZCqCc9ef52Gigv0yd77hfvBYvFgHH2dpM_6rupj-PoCGYWWBzQtH4K3LJIJLsMRUKJx15mPamRBGy6SA6Rfs2j-ATUhPpn0z4kmcx-j2fRGhqw4UBj_jzR2If_4PmzxmHIwh51Zoh-)

![](https://blogger.googleusercontent.com/img/a/AVvXsEjqPhXpoh9pjx1bFoerfKALWbvm6BQ7VXNf_5QtKBeP1SHHRqRQaqJ6NnzK9_jz4nt5v2JoZv98cPlKOPZYkB-7Z3GmGC8bFXiOjrFeQX8FTLOPmw7Hw64HcUq5VjvnfXngpKZzyeTBOKHC2FfYSwFZr810FImM5z8fGHw4JtfAK0IWbIO9CXH0MMGDiIrp)

Creating the GPT-2 multi head attention with this class:

![](https://blogger.googleusercontent.com/img/a/AVvXsEhpLZP5O0y_57lKhqR_QziYGtitUAgN1OP7s4SW0uGOAVKCCe_qN7b2B2Yz-LPk27irOAc1XhUqtYrW9O3SnPFyWCFEgD7qfsh8sxZFyI-0DctqUb10RWQIjIzrJYzBJyS7-lxOqZDAAsVUn0RxqAG6QOuTS_fvdgtmEQnHnCCV2-QRRMJoX5Xg460Dpy5F)

The resulting params count is 2.36m, but the GPT-2 model has 117m, which means that most of the parameters come from some other place ~ (we will see later)


That is all for today!

See you tomorrow :)
