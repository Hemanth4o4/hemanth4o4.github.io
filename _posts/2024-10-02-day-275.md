---
layout: post
title: (Day 275) Finding new books to read + stream
categories: [theory,math,nlp,applying-knowledge]
---

# Hello :) Today is Day 275!
A quick summary of today:
* showed my mini paper to my lab professor
* read a bit of Deep Learning by Bishop&Bishop
* found many new books to read
* next steps in the movie review PEFT LLM project
* [streamed again](https://www.youtube.com/watch?v=bez0EsQ30jc&t=246s)

## Firstly about the [paper](https://github.com/divakaivan/taxi-demand-video-models-paper)

I talked about it on stream, but tldr is: I had the paper living in my repo for 2-3 months and I read a lot about taxi demand matrices and predicting them but when I finished writing the literature review, and briefly the rest of the 'paper' I kind of lost hope that this will be a good paper so I gave up on submitting it and talking more about it with my lab professor. But after 2-3 months, my SO finally convinced me to send it so that I can get help and continue writing it. 

## [Movie review project update](https://huggingface.co/divakaivan/llama3-finetuned-reviewer-kr)

For this project I had LoRA fine-tuned llama3 and a t5 model. Among the two, llama3 performed way better and the code uses the unsloth library which specialises in faster fine-tuning. So going forward as per my professor's suggestion I will select a few models, along with a few base prompts and compare the performance across all combinations. A reminder, this movie review project is a cover-up for the actual project idea which I cannot share at the moment but is something similar to this idea. 

## [DL by Bishop&Bishop](https://www.bishopbook.com/)

Talk about it on stream but unfortunately I did not take notes and as I was in the lab I did not want to put my phone to film myself reading as I did not want to film my labmates. I read chapter 5: single layer networks for classification, chapter 6: deep neural nets, and then skimmed through some of the other chapters. 

## New books

Thanks to Bishop&Bishop I was motivated to go and find more books to read. But I wanted to read more foundation knowledge books - math, stats, probability (again). I ended up finding a lot:

* [Before Machine Learning](https://www.amazon.com/dp/B0CLKVHK7Y?binding=paperback&ref=dbs_dp_rwt_sb_pc_tpbk) - 3 book series on linear algebra, calculus and stats&probs
* [The 100-page ML book](https://themlbook.com/) and [MLE](https://leanpub.com/MLE) by Andryi Burkov 

Not foundational knowledge, but seem interesting:
* [Machine Learning for High-Risk Applications](https://learning.oreilly.com/library/view/machine-learning-for/9781098102425/)
* [Machine Learning for Financial Risk Management with Python](https://learning.oreilly.com/library/view/machine-learning-for/9781492085249/)

## After stream I rested for a bit, and read the 1st book of the Before Machine Learning series

It is *very* beginner friendly and written in a casual way.

[Here is the link to it on o'Reilly](https://learning.oreilly.com/library/view/before-machine-learning/9781836208952/). 

## Chapter 3

### What Is a Vector?

A vector is essentially a list of numbers where the order of the elements matters, often used in machine learning to represent data points like height and weight. For instance, a two-dimensional vector could represent a student as (height, weight). Each vector has both direction and magnitude, with its origin typically at (0,0) in a coordinate system.

<img width="255" alt="image" src="https://github.com/user-attachments/assets/7a29a5c2-55d3-4f5c-955c-3b1eb1c5f5f0">
<img width="306" alt="image" src="https://github.com/user-attachments/assets/dd1fda3d-2070-4dbf-b144-82d138d9278b">


Vectors can have multiple dimensions; for example, a three-dimensional vector might be represented as (x, y, z). Understanding vectors involves visualizing their components: the starting point (origin), direction (based on coordinates), and magnitude (the length of the vector).

<img width="292" alt="image" src="https://github.com/user-attachments/assets/28aa4476-807f-4430-bd99-0ece09405748">

Vectors can describe real-world phenomena like wind by representing direction and speed. They also allow for mathematical operations, such as addition, which can represent paths or movements. Overall, vectors are foundational elements in mathematics and have wide applications in various fields.

### Vector Addition

<img width="282" alt="image" src="https://github.com/user-attachments/assets/2410ce2b-09eb-4cc5-b9d4-c487bbcde341">
<img width="347" alt="image" src="https://github.com/user-attachments/assets/93b9ea63-af61-477f-beaf-0989fab316df">
<img width="359" alt="image" src="https://github.com/user-attachments/assets/c6a7acc7-2747-4672-96ba-633d533e480d">

### Scalar Multiplication

Œª ‚àà ‚Ñù

A scalar is any real number, such as œÄ, which can be used to scale vectors. The symbol ‚àà means "belongs to," and ‚Ñù represents the set of real numbers. When multiplying a vector by a scalar (Œª), there are four possible outcomes based on Œª's value. If Œª is greater than 1, the vector stretches while maintaining direction. If 0 < Œª < 1, the vector shrinks without changing direction. If Œª is less than -1, the vector stretches but reverses direction. If -1 < Œª < 0, the vector shrinks and reverses direction. Multiplying a vector by a scalar alters its length and, in some cases, its direction. Finally, vector multiplication has not yet been covered.

### The Dot Product

The dot product is a method of multiplying two vectors that results in a scalar value. It is calculated by multiplying corresponding elements of two vectors and summing the results. For example, given two vectors, v = (2, 3) and w = (2, 1), their dot product is 2 * 2 + 3 * 1 = 7.

In linear algebra, the dot product has a geometric interpretation. It measures how much one vector aligns with another by projecting one vector onto the other and multiplying their magnitudes. The angle \(\theta\) between vectors determines this relationship, where the dot product \( \mathbf{v} \cdot \mathbf{w} = |\mathbf{v}| |\mathbf{w}| \cos(\theta) \).

If two vectors point in the same direction, the dot product is positive; if they are perpendicular, the dot product is zero; if they point in opposite directions, it is negative.

In machine learning, the dot product is used to measure similarity. For example, when recommending movies based on user preferences, you can compare movie vectors by calculating the dot product or using cosine similarity, which focuses on vector direction rather than magnitude. Cosine similarity is preferred when the magnitude is not crucial, as in text analysis or machine learning tasks. 

Both the dot product and cosine similarity are valuable tools for comparing and manipulating vectors, with their applications depending on whether vector length or direction is more relevant.

### The Vector Space

A **vector space** is a structured set of vectors that follow specific rules (axioms) for operations like vector addition and scalar multiplication. These operations must produce results that remain within the vector space. For any collection of vectors to qualify as a vector space, they must adhere to these axioms, ensuring operations like vector addition and scalar multiplication behave as expected:

<img width="715" alt="image" src="https://github.com/user-attachments/assets/d36962b0-a92a-418a-a76d-6235a6c54393">

The **basis** of a vector space is a set of linearly independent vectors that can represent all vectors within that space through a combination of scalar multiples. Changing the basis provides a new perspective on the vectors, useful in different contexts such as machine learning, where altering the basis can reveal patterns or simplify computations. For example, changing the basis for data like house features can help better understand trends such as the total number of rooms or the balance between bedrooms and bathrooms.

## Chapter 4: Matrices

Maybe because I was exhausted, but the topic is math are there was *so much* over-the-top casual explanations that kind of confused me as to why are they there. I am not sure if this make sense, but in its attempts to explain things too simply it added maybe a bit too much abstractions and deviations from the main point that at some points I needed to re-read üòÜ even though I know the material. 

### Linear Transformations

<img width="459" alt="image" src="https://github.com/user-attachments/assets/056ef628-348a-4ceb-b703-fc78bb59b553">

L : ‚Ñùn  ‚Üí  ‚Ñùm 

It must satisfy:

L(v + u) = L(v) + L(u)
L(c ‚ãÖ v) = c ‚ãÖ L(v)

where v and u are vectors that belon to ‚Ñùn, and c is a scalar that is a real number

### The Eigen 'Stuff'

The book tries, but [2blue1brown](https://www.youtube.com/watch?v=PFDu9oVAE-g&t=530s&pp=ygUXMmJsdWUxYnJvd24gZWlnZW52ZWN0b3I%3D)'s explanation is the best by far. 

To be honest when I was reading this part, and after remembering 2blue1brown's channel, I remembered how great that youtube channel is and that it explains everything so nicely. 

### Matrix decomposition

![image](https://github.com/user-attachments/assets/306f3953-690a-44b3-a6d1-b301c021249a)

Where:

U represents a rotation (it preserves the angles and lengths of vectors)
Œ£ is the scaling matrix (a diagonal matrix containing the singular values, which represent scaling factors)
V^T is another rotation (the transpose of an orthogonal matrix V which can also be interpreted as a rotation or reflection. It defines the basis for the column space of the original matrix)

This transformation applied to a circle represents the operation:

![image](https://github.com/user-attachments/assets/fefa2d9a-3522-4b47-8fc3-240673b3d573)

### Principal Component Analysis

The principal component analysis will create a new set of axes called the principal axes, where we will project the data and get these so-called principal components. These are a linear combination of the original features that will be equipped with outstanding characteristics. Characteristics which are not only uncorrelated, but the first components also capture most of the variance in the data, which makes this methodology a good technique for reducing the dimensions of complex data sets.

Start with some data:

<img width="646" alt="image" src="https://github.com/user-attachments/assets/0733ad9d-9f83-4d02-b518-d578b9c5d310">

Normalise it using the mean and st.d.:

<img width="689" alt="image" src="https://github.com/user-attachments/assets/08df1117-8c16-4386-b710-455a3c1ea49c">

If we imagine the table as a matrix A^T, multiply it by A and divide by N (8, normalising it) and we get:

<img width="339" alt="image" src="https://github.com/user-attachments/assets/ed372de4-d42b-46ad-b3a0-df7da38cccb9">

Next we need to find the eigenvectors and eigenvalues. That can be done through eigen decomposition

Why?

1. Eigenvectors define the principal components (the directions of max variance):
   - eigenvectors represent the directions in which the data varies the most
   - when you perform PCA, you aim to transform your data into a new coordinate system where the axes (called principal components) correspond to these directions of maximum variance
   - the eigenvectors of the covariance matrix (or the correlation matrix) point in these directions of maximum variance
   - these eigenvectors become the principal components in PCA, where each principal component is a linear combination of the original features, and they are orthogonal to each other

2. Eigenvalues represent the magnitude of variance:
   - eigenvalues correspond to the amount of variance (or the 'importance') along each principal component (eigenvector)
   - larger eigenvalues indicate that the corresponding eigenvector (principal component) captures a larger amount of the data's variance
   - by sorting the eigenvalues in descending order, we can rank the principal components by how much variance they explain. This helps determine which principal components are most important for dimensionality reduction

<img width="621" alt="image" src="https://github.com/user-attachments/assets/485285fc-95c2-4932-b3bf-5667fd95ca70">

P is a matrix with the eigenvectors, and this will be where we find our principal axes. It happens to be already sorted by eigenvalue magnitude. On the other hand, in the matrix Œ£ we have all the eigenvalues. These represent what can be called the explainability of the variance, how much of the variance present in the data is ‚Äùcaptured‚Äù by each component.

The sum of the eigenvalues is 5, and by dividing each of them by 5 we find the individual percentage of variance that each distinct eigenvalue explains

<img width="422" alt="image" src="https://github.com/user-attachments/assets/d0e36ad4-6e08-4114-aa06-5024fea25bc1">

The 1st two components explain ~76% of the variance so we can use the two for as our principal components to reduce our dims. 

<img width="242" alt="image" src="https://github.com/user-attachments/assets/3942d50e-c1a0-4daa-9990-ff16634c99fc">

So now we just need to multiply our original 8x5 data matrix by this 5x2 matrix to get the resulting dimenson reduced feature set:

<img width="424" alt="image" src="https://github.com/user-attachments/assets/976bf4ca-071e-4933-8f47-821e84c6e771">

These components are also called latent or hidden variables; relationships that are hidden in the data and are the result of linear combinations.

---

That is all for today!

See you tomorrow :)
