---
layout: post
title: (Day 62) Stanford CS224N (NLP with DL) - Transformers, Pretraining, RLHF
categories: [nlp,theory]
---

## Hello :) Today is Day 62!
A quick summary of today:
* [Lecture 8](https://youtu.be/LWMzyfvuehA?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4): Self-Attention and Transformers
* [Lecture 9](https://youtu.be/DGfCRXuNA2w?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4): Pretraining
* [Lecture 10](https://youtu.be/SXpJ9EmG3s4?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4): Prompting, Reinforcement Learning from Human Feedback

I watched this [State of GPT](https://youtu.be/bZQun8Y4L2A) talk by Andrej Karpathy a week or so ago and saw the below slide:

![image](https://github.com/user-attachments/assets/14f22a19-d002-43be-b7a1-2ce2a8374068)

And that was the first time I saw this GPT pipeline. Today, I got to learn about each of these steps. Feels nice knowing how GPT/ChatGPT works. 

Anyway ~ my notes are below :)

**Lecture 8: Self-Attention and Transformers**

![image](https://github.com/user-attachments/assets/605f14d3-ca63-4ff0-9edf-f1d0ae18a97b)
![image](https://github.com/user-attachments/assets/96e0b2f7-0173-4f37-9fe2-ba4dc7ca86e7)
![image](https://github.com/user-attachments/assets/256ef7bb-0c5d-4fd8-b8f7-2614323d67e3)
![image](https://github.com/user-attachments/assets/22511968-6125-4280-a19f-765686600fe6)
![image](https://github.com/user-attachments/assets/c414c587-3a33-48e3-90e9-db144f5064d7)
![image](https://github.com/user-attachments/assets/1914b878-1081-4566-b89b-04cb5e35d27d)
![image](https://github.com/user-attachments/assets/2e90f7e0-8eab-43aa-92f8-13ac3fa26dfa)
![image](https://github.com/user-attachments/assets/5c868bcf-bc77-4ba8-a432-7371c5c9b4fa)
![image](https://github.com/user-attachments/assets/54cfdc45-bed6-4c41-9410-a3d238ebbd00)
![image](https://github.com/user-attachments/assets/1429c8ed-3cd3-4401-83d1-1ea630c5bda0)
![image](https://github.com/user-attachments/assets/68135587-0d5f-4c08-8024-9e1a6d841896)
![image](https://github.com/user-attachments/assets/3811fc89-3b03-42ab-8efe-47f34d76817d)

**Lecture 9: Pretraining**

![image](https://github.com/user-attachments/assets/4599af29-73e5-47d3-b2b5-5ed76de76dd5)
![image](https://github.com/user-attachments/assets/b5529876-a802-4756-b57f-a657c0b1e74b)
![image](https://github.com/user-attachments/assets/bb85908d-0a9e-4dde-b2b2-89bcd7916ff7)
![image](https://github.com/user-attachments/assets/e14616d1-0b5c-4833-930d-1973d6f5a5a4)
![image](https://github.com/user-attachments/assets/feafa9c0-234b-4fe6-9c2b-eb4f3fef5196)
![image](https://github.com/user-attachments/assets/1f6857e3-8300-4092-9a53-9a3324040bb2)
![image](https://github.com/user-attachments/assets/d48ac3fd-61a0-4860-95ad-85d9d990e181)
![image](https://github.com/user-attachments/assets/92bee2d8-f77c-4955-8818-84620c40f144)
![image](https://github.com/user-attachments/assets/d946fcd9-872d-4ced-9600-b3cba030ed6b)
![image](https://github.com/user-attachments/assets/71005827-29fa-43fd-93c0-520b0458c629)
![image](https://github.com/user-attachments/assets/8d92096c-8409-49d5-bb29-f7f686c9cb51)

**Lecture 10: Prompting, Reinforcement Learning from Human Feedback**

![image](https://github.com/user-attachments/assets/8b8d8d85-22df-4a0c-ae87-14646089600e)
![image](https://github.com/user-attachments/assets/0fe70cba-37b1-4cd9-960b-467d1d81ff57)
![image](https://github.com/user-attachments/assets/62c0af45-d635-4557-ab11-38aeabc4f3ef)
![image](https://github.com/user-attachments/assets/923799b1-a458-40ec-aa24-2455db773d64)
![image](https://github.com/user-attachments/assets/10ff414b-20ef-458c-993a-8c4a3eaf8375)
![image](https://github.com/user-attachments/assets/f3d07a34-5062-43ca-8cb2-5ae635e347b6)
![image](https://github.com/user-attachments/assets/c24f4db1-7588-4f57-a06d-dd75385ae0f5)
![image](https://github.com/user-attachments/assets/4c8a24ea-9235-4873-89f9-300074469b3c)



Tomorrow we continue diving deeper into the world of NLP. 



That is all for today!

See you tomorrow :)
