---
layout: post
title: (Day 120) Starting Stanford's CS246 - Mining Massive Datasets + MIT's Intro DL
categories: [theory,deep-learning]
---

## Hello :) Today is Day 120!
A quick summary of today:
* started [MIT's Intro to deep learning](https://youtu.be/ErnWZxJovaM) 2024 course
* started [Stanford's CS246: Mining massive datasets](https://web.stanford.edu/class/cs246/)

Last night I saw a notification that MIT is going to livestream their first intro to DL lecture on youtube (from their 2024) course, and this morning I decided to check what it's about. [This](http://introtodeeplearning.com/) is the official website, and it will provide lectures + homeworks. The course lectures will go from 29th April to 24th June. And the schedule is:

![image](https://github.com/user-attachments/assets/376b985f-4322-4ffc-b3a5-8cfba332f683)

As a 'basics lover' I am excited to go over old and hopefully new material. I am excited to see MIT's take on teaching intro to DL. 

### The summary of the 1st lecture is bellow

It started it where DL fits with AI and ML.

![image](https://github.com/user-attachments/assets/2137b92e-7523-4f26-b708-a39921b04ee1)
![image](https://github.com/user-attachments/assets/065743eb-c99b-4da2-9730-33c4820d9323)

Gave a general and very well put (as expected for MIT) explanation of a neuron/perceptron

![image](https://github.com/user-attachments/assets/b4eabeac-ed6f-4205-81ac-a7cb8d418736)

Provided an example of a simple neural network

![image](https://github.com/user-attachments/assets/76d6302a-9206-40af-b551-4c29f83cb031)

Also, how does the network know how wrong it is? Thanks to the loss

![image](https://github.com/user-attachments/assets/f42468b5-6279-45f2-a1d5-c964aed351ec)
![image](https://github.com/user-attachments/assets/0870a194-a1d1-46f0-b655-ee58a6cc6eda)

However, using the whole dataset to update the weights W (Gradient Descent) might not be the best idea, especially with the amount of data available these days. That is where Stochastic Gradient Descent comes in.

![image](https://github.com/user-attachments/assets/72823b4f-f75d-4cd7-9529-3115599c89a6)

In the end, we got introduced to the 2 of the most common regularization techniques: dropout, early stopping. 

### As for the CS246: Mining massive datasets

It was on my list for a while, the reason I did not start it earlier was because I was not sure if there were videos on it (even if there were not, I would still end up doing it, but a video lectures just adds a little more to just reading slides/book chapters). I saw today that there are [lecture videos from winter 2019](https://youtu.be/jofiaetm5bY?list=PLoCMsyE1cvdVnCgHk43vRy7PVTVWJ6WVR). Yes it was 4 years ago, but I compared slides from the most recent release (winter 2024) with the slides from winter 2019 and there is little deviation towards the end, when I can just watch the lecture and then by myself read the slides. 

Today I covered lecture 1

Covered topics:

MapReduce, Spark, Spark's RDD, cost measures

![image](https://github.com/user-attachments/assets/f60d448d-7566-4e49-aacd-f6cdee5ae104)
![image](https://github.com/user-attachments/assets/4f1c1690-7018-4b98-becd-b8fdecbdb7fb)
![image](https://github.com/user-attachments/assets/b4e99c18-0436-4732-8a6e-ebd937214877)
![image](https://github.com/user-attachments/assets/f42f0f1e-28ab-4789-b34d-fb46ede8dd55)
![image](https://github.com/user-attachments/assets/9bf36803-5586-4720-b991-4c155c573b04)
![image](https://github.com/user-attachments/assets/45412246-e95e-4527-8ac3-22ff5e94afe8)

I also registered for a new event hosted by Global AI for good that's in collaboration with Oxford University - [MLx Generatie AI (Theory, Agents, Products)](https://www.oxfordml.school/genai) on 22-24th August 2024. ^^

Topics to be covered:

Advanced theoretical topics in representation learning (e.g., vision, language, multi-modal, …)
Agentic AI (e.g., agentic reasoning and design patterns)
Human+AI alignment
Building Gen. AI products — from model.fit() to market.fit()
Using SOTA foundation models (e.g., fine tuning, RAG, RLHF, prompt engineering, …)
Application of large frontier models in applied domains (e.g., medicine, finance, education, …)


That is all for today!

See you tomorrow :)
