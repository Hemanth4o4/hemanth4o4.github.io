---
layout: post
title: (Day 209) Using Mage for pipeline orchestration in the KB project
categories: [mlops,gnn,applying-knowledge]
---

## Hello :) Today is Day 209!
A quick summary of today:
* creating Mage pipelines for the KB AI competition project

The repo after today

![image](https://github.com/user-attachments/assets/d4c78ff5-e86c-4a13-b2df-af6c49c0fdd7)

Today morning/afternoon I went on a bit of a roll ~ 

![image](https://github.com/user-attachments/assets/f521a12c-c8bb-455e-b323-a1d5cacd41f7)

I set up all the above pipelines in Mage. Below I will go over each one

get_kaggle_data

![image](https://github.com/user-attachments/assets/ba0f52b1-91a3-4f56-a552-f08484eeccb6)

it is just one block that downloads the data for the project from [Kaggle](https://www.kaggle.com/datasets/kartik2112/fraud-detection) using the kaggle python package

load_batch_into_neo4j

![image](https://github.com/user-attachments/assets/a80d9c66-59af-4ddd-be7e-4b40d813f9a6)

Gets the loaded data from the get_kaggle_data pipeline and inserts it into neo4j. This is the fraudTrain.csv from the Kaggle website (because the fraudTest.csv will be used for the pseudo-streaming pipeline).

train_gcn

![image](https://github.com/user-attachments/assets/8ac1e2a6-9861-4288-93a9-31952a94df45)

I tried to split this into more blocks, but at the moment the way I structured the code, the most optimal solution was to do it all at once. That is - create a torch-geometric dataset, create node and edge index, train the model, test it, and save summary info. At the moment, because everything is local, I am using mlflow just for easy comparison but later (after the project submission) I will add a proper mlflow server with a cloud db and artifact store where I can save models, and from where I can load models and other data. 

When this pipe is run, locally and in mlflow such folder is created:

![image](https://github.com/user-attachments/assets/f1feb992-3787-41bd-a91f-b96a5975b1b2)

Here are the images/text from this model run:

Confusion matrix:

![image](https://github.com/user-attachments/assets/0ae1f7b6-17e5-40f7-bc02-800515e3d864)

Feature importance (I need to modify this because besides 5 of the vars, the other are categorical dummies and I need to add a suffix to make it clear):

![image](https://github.com/user-attachments/assets/ba2a3141-c126-47b4-a5d9-8cdabe3a251d)

Accuracy: 0.8809, Precision: 0.8133, Recall: 0.9883, F1: 0.8923

Model summary:

![image](https://github.com/user-attachments/assets/d3d66a04-0642-45a4-9927-ea8fa9749272)

stream_predictions

For this pipeline to work I needed to set up a kafka producer to send data to the kafka stream. I created this and shared it in yesterday's post. 

![image](https://github.com/user-attachments/assets/ca2c5726-d45a-46c7-a8f6-c96d21c6be55)

Mage has a nice setup for streaming pipelines. The 1st block establishes a connect to read data from the kafka stream (to which I am sending transactions). In the 2nd block, using the data from the stream, I create a Data object that can be used as input to the trained GCN, and add it to the message:

![image](https://github.com/user-attachments/assets/88bcff5d-b987-4619-8e43-ca3c1193d6c4)

And in the final block - insert the new data to neo4j. 


Another thing - creating a simple model dictionary using streamlit

![image](https://github.com/user-attachments/assets/89c1593d-3eb4-4757-b636-676bfedf7741)

To create it I added a new option to the Makefile. Yesterday I thought I would use docker to create a service, but because I am not using mlflow with a proper db and artifact store, if I want to load images/text (as the above confusion matrix, model summary, etc.) I need to upload those files to the streamlit docker image as well, and this seems unnecessary. Once mlflow is set up properly then using streamlit from docker might be a viable choice. 

As for how the streamlit model dictionary looks:

![image](https://github.com/user-attachments/assets/03dfb635-9678-4ce3-be1a-34a09080e0b4)

it includes all the model info that is produced from the train_gcn pipeline. 
And once we have other models, their info can easily be added. 




That is all for today!

See you tomorrow :)
