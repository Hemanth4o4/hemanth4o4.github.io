---
layout: post
title: (Day 44) Batch vs Layer vs Group Normalization and GANs (+ found a free KAIST AI course)
categories: [applying-knowledge.theory]
---

## Hello :) Today is Day 44!
A quick summary of today:
* Found KAIST Professor Choi's Programming for AI lectures [link](https://youtu.be/TG3OJQjN-l4?list=PLLENHvsRRLjAmAjc8mV0f9C6i8Gh308SS)
* Discovered that there is Layer and Group norm (not only Batch norm)
* Learned about GANs

1) Batch vs Layer vs Group normalization methods

[image](https://blogger.googleusercontent.com/img/a/AVvXsEj9R6IO0Pwu-ZtrRo8lOASNx4F0YYViutpHal9cC0rPX_kZy_Et49ToBKmI7LqllwB9bFILbor9X2pRlWlP7R7qQfT0x_Kyr6-QJhiY0HeFOT01dxrDbcrEjr1sNi6oxxI2yeeg2X3qMZxDT7BVrLtV6rBlXap2dvvE5kszBLtXNAbsmQ7tq7f4bi0F_Wrw)

In one of Professor Choi's lectures, he explained about the above three norm layers (I have not heard of the 2nd 3rd 4th), plus some searches online, I found the difference.

Firstly, batch norm: given a batch of activations for a specific layer, the mean and std for the batch is calculated. Then, it subtracts the mean and divides by the std to normalize the values. (+ an epsilon is added to the standard deviation for numerical stability) following that, a scale factor "gamma" and shift factor "beta" which are learnable parameters are applied.

Secondly, layer norm: proposed in 2016, layer norm operates over the feature dimension (i.e., it calculates the mean and variance for each instance separately, over all the features) but unlike batch norm, it doesn't depend on the batch size so it's often used in recurrent models where batch norm performs poorly. Layer norm computes the mean and std across each individual observation instead (over all channels in case of images) rather than across the batch. This makes it batch-size independent and can therefore be used in models like RNNs or in transformer models.

Thirdly, instance norm: here, mean and variance are calculated for each individual channel for each individual sample across both spatial dimensions.


2) Generative adversarial network (GAN)
I finally got to meet GANs. 
What are GANs? 
GANs are networks that aim to understand the given data and recreate it. It's 'learning' process is interesting in that, it 'fights with itself'. There are 2 networks - a generator and a discriminator. The generator tries to create images as if they came from the training set and trick the discriminator, while the discriminator tries to distinguish between the actual training images and the ones coming from the generator.

[image](https://blogger.googleusercontent.com/img/a/AVvXsEjiBia7mos9_778ICi05in9JwSoIJNQlNakEJWNt7Z8XypDbTQZ4WPO3rdDSMjXAh6eW4xRYxjfU5-fck1J9vJvuungV2YfpLTUzkE6z7pG-KCMGSiFLmM7STrToWUeoGNQF9Sjum0OC4lQ0T_8r4vP_I5AEAdtTCt4tX4qlqOBcjfYMhGmridsWwD5YOWq)

A simple Generator:

[image](https://blogger.googleusercontent.com/img/a/AVvXsEgPeUySyF6kWg0XBVfilka7uFQydVHNgXD_0mpHCxdOSl6SQ7KRC7IWtFCj1lA-YYf2u1UtOb3mOEvrAlW9Cpf2WEI8VaZW_Opv94Pyv0l8aepFo-JfvVM-od7nCCJ__1FgjDOHufFTpqnz3MNGG_bEF3GgY6gk1z8OeEB5Ip4JF4zXpNmFTR5X-IoCfexm)

And a discriminator:

[image](https://blogger.googleusercontent.com/img/a/AVvXsEiiF_vm5TXe3RUa4_0uon5lr0rrwiNcyKUNOyEvZ3OVamAqL1TR8ZxgHNImVLMz38rCnxhBanbxeDGIV6DT8oHnn6WrauN6bxC5wr7N6OwbESqYfVGrYGJ99tJ_vOA7NfieVoPTwaikGchVXZnczmp0Jgt5f6PT2zDPJZT3myD3yi-qkT60tUG5gryz9n04)

They both use their own optimizer:

[image](https://blogger.googleusercontent.com/img/a/AVvXsEigbctX8Ms6nUVmqKeRRhcpfp9Wyc-NudRFrYSbQRBrX2Z_0AKoJqGW0utWID-4UbAFaFNdHRzdENzOkCf8_gae2E-gFNrUEAm9K7FIvdPy2zJMEes8Ho9geRmymOkh3q6XU-zIITFQAWTRjWHkG9_FyXLRRa0doYVXO4bfhUhbMAAQETiJ3OcB2lBijmWT)

First you train with real images and then with fake.

[image](https://blogger.googleusercontent.com/img/a/AVvXsEjbUIgtTlXZ8LazoEmgipC1y9VZKbVNBqnHcjEu3BR2qZKbhQB3qklpHtFEUXGnWwhQgNi0bDmfkKG4hZ4CrSRJefUpAkDjBoWWPYlna4hKyhMhkgwAVW01hQ4QvGDv3wXLPFT7VSJfoWQROcbsHvFZWM0-ERVAHb5P3yfPaLFX0XSM0Uct1H8g1-Uq8Wlx)

And then you update the generator

[image](https://blogger.googleusercontent.com/img/a/AVvXsEjIk8Mn0zzUZAAnfQdwCoEZznLg4M1Eb-2OAj66BVL2_MhOhpU6_k_S-y-nvge0P2wP7c59igv6Giv71XnSe1h1hvQjI0uf3MqDH5J7FGdPBnrKPZbAGakNlf-25zj2YQFwUvyyNLZV78gq5St5ldiUqeFQyRrSOwJA8zzzj5B8-kFlHCER2zyxBQAdSyU4)

To get a different perspective, I went to PyTorch's [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) page for GAN (more specifically - DCGAN)

A DCGAN is a more complex version of GAN that uses more techniques like batch norm, leakyReLU, and conv layers. 

Their GAN makes use of people faces dataset

[image](https://blogger.googleusercontent.com/img/a/AVvXsEiGYNwbB7pwHMAqvRnc3Cg0aaRXbzMoQuO_GrDU6aqG2nrsfMljr1Q8pzUF2lJQTNUtLffz3hFm_GG36c1QiO8I7wr5J-dHsz2BxcNmUn8BiV_VSc7LS-G5b-6Ft8EjNvvsLJzqW6395gqimzx7pNXof0E_nWrzEYBwf0jUzK8RQcnmI_qahKJxlYtjUpbf)

The generator and discriminator work against each other

[image](https://blogger.googleusercontent.com/img/a/AVvXsEiRdmnW_iiSi5_6p5LjdssTOjKaS3VAuMBEXT3_a7FsSnh0uY0gLjC_zn3EiTb1p8eSmaNlYolpxcU4KnafQQceQK7Vel-KTf_dEIuB1Y9QDVDfEH_Ruma0UXZ7UjyEZAXRtkFyJs-y9gCAtbXeXaM9baMwS0InC7hjXe4dVOpJpYb10qCSVXPY5PIGLG3Q)

The generator takes noise initially and turns it into an image to trick the discriminator:

[image](https://blogger.googleusercontent.com/img/a/AVvXsEgb34vsDkekipOJrAHvulAOl9HcdUsb063E_u29NJMLJ_NfbC_uhLchBcgCI-6qiKteFGM26w0hSwXYt42mG4yHgrDyZBRjrR5jB5hJslPr3xP8MizEaSRuxjoaToxbCmOHY39VaT-FnVaEErRGSopsVM1o5maHjUki6bKu-n8-4oq6W2CbyhnmbX7JEz1u)

While the discriminator takes an image and outputs whether it is fake or real

[image](https://blogger.googleusercontent.com/img/a/AVvXsEhgMh6G3pGcT9sGeLI1UDJpdsuG6N5vf4nfc9chKqVAuxQcWh3yyKSf-33edrpkMHzNaInFntg-nGe5a0CT8aYAXlunt2YQ8Zi_6Ux6PwFT7m4PpytJVcufh7WGD_3WhHIWCq3PQJwpecLJaztD932VMsYPaA5IgB7js_ue5o-PPiBDbLpnffsaiHG75XuG)

I ran their code on my laptop. Initially 5 epochs ran for 4 hours, and then I realised the device is 'cuda' but that does not work on mac, as it should be 'mps', so I was running it on cpu, wasting time :/ thinking "wow this must be computationally expensive" hahaha (model complexity is something to learn on my list)

After the 5 long epochs: the resulting faces are:

[image](https://blogger.googleusercontent.com/img/a/AVvXsEjtcg0SLOh-7m4PW9_z19tcVklqqceM4wsQDBqWUgEcthg-O2fKMZSJuiuH8b8mScN05v511n6JSYDBoBn6QZh9plLp9C7RQQ9oVkbsGIQUlkVLH1rOxDnnjygwYgHwximLQJr1C8YTKEMy8dETpxG9clWhGflTL5rVazFShsxp1L1-LfaRmq3kshMzpowA)

And this is after 19 epochs (I am going to bed now so I stopped it)

[image](https://blogger.googleusercontent.com/img/a/AVvXsEhXd4uVWRcEwTJdj8mijRuK4fEZhi-ZcXEL9EWgL06X7OGV5knJrBZtQOXg0sAcBfryw0pyFg1vIePVNeoak7sp4gJW_gfd7ELzAbSQJezSLby1XhT_k-evcOFH7q0c2aex0SPfk6kVNnFDpQoxKJOlIBs3TL-zNhe-PBxqUTDWa4GiDoznBJUKjomhFWwT=w382-h179)

It is kinda cool. But ... 

[image](https://blogger.googleusercontent.com/img/a/AVvXsEjyKtoLWHIxFcwqKItkxJtcYnIB8aHannzE-7n03gWyd3KfiWirmdddIW_vvi7w1drvr7V1VpsDylXiGrTlL53W0t3Wj2MGWiFQAczezsDmh95zkyuQmVoZpnA-0uNeffIk7X8X96RcYBHvPUAnjJIbnMe8Vy8L1CKpwsyGIOiICFaUGEvYF70pjjcsf3M1)

Obviously, DALLE-2 and midjourney can create way better fake images than my laptop :D but knowing the concept is key here. 

Thank you to Professor Choi from KAIST for sharing his lectures to the world.

Side note!
While I was looking for GAN info, I found a tweet from 2016 from Andrej Karpathy

[image](https://blogger.googleusercontent.com/img/a/AVvXsEifa15q1p4J26Dk7yLVHZ8Strc_o_hjVgTiNaNLl7nbhiiZ8wNvuc7gyMvYxJomfsk1RKh6SGwR95nWl7wmwLBA2dFgr6noxdxbdFq3LLGIssy7PTUpvCZqk652AA73e5ZAwdN3IFu3IgJQx8U-T9zlhL3BKb_pjqeZVw8AMtzTNni8n4aeXKCJrSznv3h1)

A joke or not, in 2021 it was the recommended by google haha

[image](https://blogger.googleusercontent.com/img/a/AVvXsEjOfR79TbWDcKugV19WX-cchpHgqALKRzqEsnIP78fRJw69Bg3w2Z5Z1vyRx9yFkW2gcjf58GVOB1nPmATucv4MzI_hphH3CpBd0birs3_zKuYhIK-7ur4j13hln8kpCIJzKapSsZCmCxUy2Hz_7rAVCVCJ-le_mrl0ytzMZLL6NK1jrt9qnj4BYxabUEuY)

And under those tweets, I saw that he left OpenAI and wants to focus on his personal projects

[image](https://blogger.googleusercontent.com/img/a/AVvXsEh7dhn_gO-ZkoZUu5VWpqGPS3lXUwl-ATfWKvZsuPBjHsPG8AyfSLameeM7ZDoZKg8Ups3jbNrFbJGV_eH6-1Pf7baB4zdFvqDszN4GQiAb9EMXI4EbXpJB9Gfdeta5DhKTgDMDuMPK2cthb-hH2GYNvsyi6VKmqwtUmTWmrxyUsQjLrQUhDfwj8cetYKgX)

Such as youtube videos

[image](https://blogger.googleusercontent.com/img/a/AVvXsEhDSyaZx1FVfOq5e7p5SpnHtaxiemHsZyi3FfB2ibfI40bw5kMMlq_t6UbAEq3DyqMMkCZJphfq9ofmJQTDzOdPlIluTBW4SCePU9jxCfuFsPl7IF1NB6Jz3_umTRaylAI6nxUlqBSzLKAnXUTWRlKjp2APJ7Gfci_cxgiiW2DxOvmSN3h_pE5Xi3xL26wY)

More Andrej Karpathy youtube videos - looking forward to them!

That is all for today!

See you tomorrow :)
