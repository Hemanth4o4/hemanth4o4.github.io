---
layout: post
title: (Day 43) Coding up LeNet, VGG, InceptionNet, UNet from scratch
categories: [applying-knowledge]
---

## Hello :) Today is Day 43!
A quick summary of today:
* write LeNet from scratch
* write VGG from scratch
* write InceptioNet from scratch
* write UNet from scratch (again)

Wanting to understand the popular models a bit more, I decided to do the above. 

1) Let's begin with LeNet. 

A basic framework developed in the 1990s, basic but set the groundwork for networks like AlexNet, VGG and ResNet.

![image](https://github.com/user-attachments/assets/b0e2ac81-a149-4fbb-bbcd-9169c2bc49bd)
![image](https://github.com/user-attachments/assets/3cc6d578-aa70-4513-8794-9e96618e0ef5)

consists of 2 conv layers, each followed by a maxpool, and then ending with 2 fully connected (linear) layer. 

2) Next is VGG

![image](https://github.com/user-attachments/assets/fe475b09-8f45-4334-967f-f487220279b6)

The paper proposes numerous versions, VGG11, VGG13, VGG16, VGG19 but from a google search VGG16 seems most popular (version D in the pic). It is deeper than the earlier LeNet, consisting of multiple conv+maxpool layers, each increasing the amount of filters, and decreasing the size of the image. 

![image](https://github.com/user-attachments/assets/4f7a6039-f755-40aa-9b3f-e494cee18456)

instead of 1 version, a general model was created so that it can adapt to the desired VGG architecture

below is the implementation. I think this is a nice set-up for testing the 4 versions on 1 dataset of my choosing. 

![image](https://github.com/user-attachments/assets/bfa1a582-4180-45eb-aa91-c1eb0a99b678)
![image](https://github.com/user-attachments/assets/e20d6ee4-b678-4631-9c68-0d3f77d4d34c)

3) GoogLeNet / InceptionNet

This is a long one... haha. Funny that a research paper references a meme (and it got the name inception from it)

![image](https://github.com/user-attachments/assets/a5c2884b-5449-4a9d-b61d-01a4abcc7eb7)

GoogleNet features inception modules - which consist of parallel conv branches of different receptive field sizes.Also 1x1 convolutions reduce dimensionality and improve efficiency.Towards the end, global avgpool replaces fully connected layers for fixed-length feature vectors.

![image](https://github.com/user-attachments/assets/14f358c7-a65a-4b9e-8986-ab6a83b60ff8)

and the implementation:

individual convolution block

![image](https://github.com/user-attachments/assets/8d4f7d79-5020-41b2-9c93-3034f073db39)

individual inception block

![image](https://github.com/user-attachments/assets/8736a06e-fc05-4fdc-86d4-1e01e7ad0dc6)

And the final GoogLeNet

![image](https://github.com/user-attachments/assets/62fb3ed7-df77-42ea-9238-e28f0d871e50)
![image](https://github.com/user-attachments/assets/24254935-cee1-48fd-91b7-3eea3fb10795)

4) UNet

Yesterday I attemped it and I think I got a good kind-of working model, but I was not sure if it is correct or not because I translated it from tensorflow to pytorch. Today I found another version of UNet online and pasted it into my records (thank you to [this youtuber](https://www.youtube.com/watch?v=IHq1t7NxS8k&t=122s&pp=ygURVW5ldCBmcm9tIHNjcmF0Y2g%3D))

![image](https://github.com/user-attachments/assets/92852315-97ff-48a6-89d3-cb69aa7bdb7a)

Pretty similar to yesterday. Here is the double conv part, and the init of the Unet itself.








