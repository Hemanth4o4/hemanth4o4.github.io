---
layout: post
title: (Day 259) ML monitoring pipelines + Going deeper into neo4j
categories: [mlops,applying-knowledge,theory]
---

# Hello :) Today is Day 259!
A quick summary of today:
* [Module 5: ML pipelines validation and testing](https://learn.evidentlyai.com/ml-observability-course/module-5-ml-pipelines-validation-and-testing)
* starting Neo4j's free courses

## 5.1. Introduction to data and ML pipeline testing

### When to perform testing

ML lifecycle involves various steps that require testing to ensure that our ML models function properly. Critical areas to test include the following steps of the ML lifecycle:

* During the feature engineering stage: testing the input data quality as it affects the whole pipeline.

* During model training (or retraining): model quality checks.

* During model serving: validating incoming data and model outputs.

* During performance monitoring: continuously testing the model quality to detect and resolve potential issues.

### How to perform testing

There are different types of checks you can use to test data and ML pipelines:

**Individual tests** A test is a metric with a condition. You can perform a certain evaluation or measurement on top of a data batch and compare it against a threshold or expectation. You can formulate almost anything as a test: assertions on feature values, expectations about model quality on a specific segment, etc. Whatever you can measure, you can design as a test.

Tests can be column-level (when metrics are calculated for a specific feature or column) or dataset-level (in this case, you calculate metrics for the whole dataset).

**Test suites** Individual tests can be grouped into test suites. For each test in a test suite, you can define test criticality and set alerting conditions: for example, based on the number of failed critical tests.

When you create a test, you must define the test conditions. There are two main strategies you can use for establishing test conditions:

**Reference-based conditions** You can use a reference dataset to derive conditions automatically rather than set conditions manually for each individual test. This is a great option for certain types of checks, such as testing column types (which are easy to derive from a reference example) and for ad hoc testing such as when you import a new batch of data and can immediately visually explore the test results. However, be careful when designing alerting, as auto-generated test conditions are not perfect and may be prone to false alerts or missed issues.

**Manually defined conditions** With this approach, you specify conditions for each test manually. This method does not require additional data and can be great for encoding specific conditions based on domain expertise.

### Test automation

![image](https://github.com/user-attachments/assets/5eaa0b50-5cc9-4031-931f-426c2390fa72)

### Recording test results

![image](https://github.com/user-attachments/assets/b918e75f-ba79-48d1-a3bc-beca0eabd431)

### Example use case

![image](https://github.com/user-attachments/assets/55a5709f-a393-4c53-8728-71237afe235a)

![image](https://github.com/user-attachments/assets/93de6ba2-c690-49ae-8295-ad469f24e6ed)

## 5.2. Train and evaluate an ML model (code practice)

Using a bank marketing dataset from sklearn:

#### Bank client data:
* **Age** (numeric)
* **Job :** type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown')
* **Marital :** marital status (categorical: 'divorced', 'married', 'single', 'unknown' ; note: 'divorced' means divorced or widowed)
* **Education** (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')
* **Default:** has credit in default? (categorical: 'no', 'yes', 'unknown')
* **Balance:** average yearly balance, in euros (numeric)
* **Housing:** has housing loan? (categorical: 'no', 'yes', 'unknown')
* **Loan:** has personal loan? (categorical: 'no', 'yes', 'unknown')

#### Related with the last contact of the current campaign:
* **Contact:** contact communication type (categorical:
'cellular','telephone')
* **Day:** ast contact day of the month (numeric)
* **Month:** last contact month of year (categorical: 'jan', 'feb', 'mar', …, 'nov', 'dec')
* **Duration:** last contact duration, in seconds (numeric). Important
note: this attribute highly affects the output target (e.g., if
duration=0 then y='no'). Yet, the duration is not known before a call
is performed. Also, after the end of the call y is obviously known.
Thus, this input should only be included for benchmark purposes and
should be discarded if the intention is to have a realistic
predictive model.

#### Other attributes:
* **Campaign:** number of contacts performed during this campaign and for
this client (numeric, includes last contact)
* **Pdays:** number of days that passed by after the client was last
contacted from a previous campaign (numeric; 999 means client was not
previously contacted)
* **Previous:** number of contacts performed before this campaign and for
this client (numeric)
* **Poutcome:** outcome of the previous marketing campaign (categorical:
'failure','nonexistent','success')


Doing some feature engineering:

```python
def feature_engineering(raw_data: pd.DataFrame) -> pd.DataFrame:
    preprocessed_data = raw_data.copy(deep = True)

    preprocessed_data.columns = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 
                              'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'class']

    #client data preprocessing
    preprocessed_data['has_default'] = preprocessed_data.default.apply(
        lambda x : 0 if x == 'no' else 1 if x == 'yes' else -1
    )

    preprocessed_data['has_housing_loan'] = preprocessed_data.housing.apply(
        lambda x : 0 if x == 'no' else 1 if x == 'yes' else -1
    )

    preprocessed_data['has_personal_loan'] = preprocessed_data.loan.apply(
        lambda x : 0 if x == 'no' else 1 if x == 'yes' else -1
    )

    marital_dummies = pd.get_dummies(preprocessed_data.marital, prefix = 'marital')
    preprocessed_data = pd.concat([preprocessed_data, marital_dummies], axis = 1)

    job_dummies = pd.get_dummies(preprocessed_data.job, prefix = 'job')
    preprocessed_data = pd.concat([preprocessed_data, job_dummies], axis = 1)

    edu_dummies = pd.get_dummies(preprocessed_data.education, prefix = 'edu')
    preprocessed_data = pd.concat([preprocessed_data, edu_dummies], axis = 1)

    preprocessed_data.drop(columns = ['default', 'housing', 'loan', 'marital', 'job', 'education'], inplace=True)

    # last contact data preprocessing
    contact_dummies = pd.get_dummies(preprocessed_data.contact, prefix = 'contact_type')
    preprocessed_data = pd.concat([preprocessed_data, contact_dummies], axis = 1)

    month_dummies = pd.get_dummies(preprocessed_data.month, prefix = 'month')
    preprocessed_data = pd.concat([preprocessed_data, month_dummies], axis = 1)   

    preprocessed_data.drop(columns = ['contact', 'month'], inplace=True)
    
    # other attributes preprocessing
    poutcome_dummies = pd.get_dummies(preprocessed_data.poutcome, prefix = 'prev_camp_outcome')
    preprocessed_data = pd.concat([preprocessed_data, poutcome_dummies], axis = 1)
    preprocessed_data.drop(columns = ['poutcome'], inplace=True)

    #target preprocessing
    preprocessed_data['target'] = preprocessed_data['class'].apply(lambda x : 0 if x == '1' else 1)
    preprocessed_data.drop(columns = ['class'], inplace=True)
    
    return preprocessed_data
```

Then, splitting into train, reference, and prod simulation

```python
train_data = bank_marketing_data[:5000]

reference_data = bank_marketing_data[5000:7000]

prod_simulation_data = bank_marketing_data[7000:]
```

The model used is a simple RF classifier

```python
model = ensemble.RandomForestClassifier(random_state=42, n_estimators=100)
```

To create a simple report:

```python
model_quality_report = Report(metrics=[ClassificationPreset()])
model_quality_report.run(reference_data=processed_train, current_data=processed_reference)
model_quality_report.show(mode='inline')
```

The output report is:

<img width="1408" alt="image" src="https://github.com/user-attachments/assets/9dd78617-2ad0-44ab-b464-99200dd697a4">
<img width="1392" alt="image" src="https://github.com/user-attachments/assets/ee56b259-fb56-424b-b2f9-48da5e97c029">
<img width="1389" alt="image" src="https://github.com/user-attachments/assets/2d7889d4-7a59-40ed-9370-5adb91221eca">
<img width="1389" alt="image" src="https://github.com/user-attachments/assets/f27b5ee1-bffe-4b02-9a75-74f4888222eb">

## 5.3. Test input data quality, stability and drift (code practice)

### Raw data checks:

```python
data_stability_suite = TestSuite(tests=[DataStabilityTestPreset()])
data_stability_suite.run(reference_data=reference_data, current_data=prod_simulation_data[:batch_size])
data_stability_suite.show(mode='inline')
```

<img width="1386" alt="image" src="https://github.com/user-attachments/assets/858f3408-7a1f-421c-8ac3-393d5d593121">

For any test we can see details:

<img width="1371" alt="image" src="https://github.com/user-attachments/assets/acfc7fe5-b889-4bab-9da3-991d0e09245e">

```python
data_quality_suite = TestSuite(tests=[DataQualityTestPreset()])
data_quality_suite.run(reference_data=reference_data, current_data=prod_simulation_data[:batch_size])
data_quality_suite.show(mode='inline')
```

<img width="1391" alt="image" src="https://github.com/user-attachments/assets/c373f0e1-0efa-42af-9244-412654c7df0b">

```python
data_drift_suite = TestSuite(tests=[DataDriftTestPreset()])
data_drift_suite.run(reference_data=reference_data, current_data=prod_simulation_data[:batch_size])
data_drift_suite.show(mode='inline')
```

<img width="1378" alt="image" src="https://github.com/user-attachments/assets/96a807e0-cda7-4b57-9941-614ca71801ce">

### Preprocessed data checks

We preprocess the data using our feature_engineering function

```python
processed_reference_data = feature_engineering(reference_data)
processed_prod_simulation_data = feature_engineering(prod_simulation_data)
```

Run the data drift preset report code

```python
data_drift_suite = TestSuite(tests=[DataDriftTestPreset()])
data_drift_suite.run(reference_data=processed_reference_data, current_data=processed_prod_simulation_data[:batch_size])
data_drift_suite.show(mode='inline')
```

<img width="1407" alt="image" src="https://github.com/user-attachments/assets/de5844d9-2d29-450b-8cc8-49e973fe068c">

We can run some quick checks for warnings/alerts to check tests pass rate like below

```python
data_drift_suite.as_dict()['summary']['all_passed'] == True # False
data_drift_suite.as_dict()['summary']['by_status']['SUCCESS'] > 40 # True
```

## 5.4. Test ML model outputs and quality (code practice)

Load the trained model and add predictions for reference and the prod simulation data that was created earlier:

```python
processed_data_reference['prediction'] = model.predict(processed_data_reference.iloc[:, :-1])
processed_data_prod_simulation['prediction'] = model.predict(processed_data_prod_simulation.iloc[:, :-1])
```

### Model output checks

```python
no_target_performance_suite = TestSuite(tests=[NoTargetPerformanceTestPreset()])
no_target_performance_suite.run(reference_data=processed_data_reference, current_data=processed_data_prod_simulation[2*batch_size:3*batch_size])
no_target_performance_suite.show(mode='inline')
```

<img width="1388" alt="image" src="https://github.com/user-attachments/assets/2922c20c-d70f-40fc-99c1-5e2b2be9fe91">


### Model quality checks

```python
model_performance_suite = TestSuite(tests=[BinaryClassificationTestPreset()])
model_performance_suite.run(reference_data=processed_data_reference, current_data=processed_data_prod_simulation[0*batch_size:1*batch_size])
model_performance_suite.show(mode='inline')
```

<img width="1390" alt="image" src="https://github.com/user-attachments/assets/3e9c0724-8033-4d80-92ad-213ef0eef0a2">

## 5.5. Design a custom test suite with Evidently (code practice)

Here is an example custome suite

```python
custom_performance_suite = TestSuite(tests=[
    #TestColumnsType(),
    #TestShareOfDriftedColumns(ls=0.5),
    TestShareOfMissingValues(eq=0),
    TestPrecisionScore(gt=0.5),
    TestRecallScore(gt=0.3),
    TestAccuracyScore(gte=0.75),
])

custom_performance_suite.run(reference_data=processed_reference, current_data=processed_prod_simulation[:batch_size])
custom_performance_suite.show(mode='inline')
```

<img width="1373" alt="image" src="https://github.com/user-attachments/assets/76394b63-a928-4d5b-a639-f2f6ba5f6b49">

## Running and logging reports using Airflow and mlflow

### Adding monitoring using Airlow

```python
import os
import pandas as pd 

from datetime import datetime, timedelta
from sklearn import datasets

from airflow import DAG
from airflow.operators.python_operator import PythonOperator 

from evidently.test_suite import TestSuite 
from evidently.test_preset import DataDriftTestPreset

default_args = {
	"start_date" : datetime(2023, 1, 1),
	"owner" : "emeli",
	"retries" : 2,
	"retry_delay" : timedelta(minutes = 5),
}

dir_path = "reports"
file_path = "data_drift_suite.html"

def load_data_execute(**context):
	bank_marketing = datasets.fetch_openml(name='bank-marketing', as_frame='auto')
	bank_marketing_data = bank_marketing.frame 
	context["ti"].xcom_push(key="data_frame", value=bank_marketing_data)

def drift_analysis_execute(**context):
	data = context["ti"].xcom_pull(key="data_frame")

	reference_data = data[5000:7000]
	prod_simulation_data = data[7000:]
	batch_size = 2000

	data_drift_suite = TestSuite(tests=[DataDriftTestPreset()])
	data_drift_suite.run(reference_data=reference_data, current_data=prod_simulation_data[:batch_size])

	if not data_drift_suite.as_dict()['summary']['all_passed']:
		return "create_report"

def create_report_execute(**context):
	data = context["ti"].xcom_pull(key="data_frame")

	reference_data = data[5000:7000]
	prod_simulation_data = data[7000:]
	batch_size = 2000

	data_drift_suite = TestSuite(tests=[DataDriftTestPreset()])
	data_drift_suite.run(reference_data=reference_data, current_data=prod_simulation_data[:batch_size])

	try:
		os.mkdir(dir_path)
	except OSError:
		print("Creation of the directory {} failed".format(dir_path))
	data_drift_suite.save_html(os.path.join(dir_path, file_path))


with DAG(
	dag_id = "evidently_conditional_drift",
	schedule_interval = "@daily",
	default_args = default_args,
	catchup = False,
	) as dag:

	load_data = PythonOperator(
		task_id = "load_data",
		python_callable = load_data_execute,
		provide_context = True
		)

	drift_analysis = PythonOperator(
		task_id = "drift_analysis",
		python_callable = drift_analysis_execute,
		provide_context = True
		)

	create_report = PythonOperator(
		task_id = "create_report",
		python_callable = create_report_execute,
		provide_context = True
		)

load_data >> drift_analysis >> [create_report]
```

<img width="1332" alt="image" src="https://github.com/user-attachments/assets/aa4b3d97-acea-4e47-bdd5-e1e5417db2b2">

### Log artifacts using mlflow

```python
import os
import pandas as pd 

from datetime import datetime, timedelta
from sklearn import datasets
import mlflow

from evidently.test_suite import TestSuite 
from evidently.test_preset import DataDriftTestPreset

bank_marketing = datasets.fetch_openml(name='bank-marketing', as_frame='auto')
bank_marketing_data = bank_marketing.frame
reference_data = bank_marketing_data[5000:7000]
prod_simulation_data = bank_marketing_data[7000:]
batch_size = 2000 

#set experiment
mlflow.set_tracking_uri("sqlite:///mlflow.db")
mlflow.set_experiment("Drift Tests")

for batch_id in range(10):
	with mlflow.start_run() as run:
		data_drift_suite = TestSuite(tests=[DataDriftTestPreset()])
		data_drift_suite.run(reference_data=reference_data,
			current_data=prod_simulation_data[batch_id * batch_size : (batch_id + 1) * batch_size])

		data_drift_suite.save_html("data_drift_suite.html")

		#Log parameters
		mlflow.log_param("batch_id", f"batch_{batch_id}")
		mlflow.log_param("success_tests", data_drift_suite.as_dict()['summary']['success_tests'])
		mlflow.log_param("failed_tests", data_drift_suite.as_dict()['summary']['failed_tests'])

		#Log report
		mlflow.log_artifact("data_drift_suite.html")

		print(run.info)
```

<img width="1343" alt="image" src="https://github.com/user-attachments/assets/0aeeacf0-ee0e-4212-a04a-b0a1bdab1773">

---

Below is [module 6](https://learn.evidentlyai.com/ml-observability-course/module-6-deploying-an-ml-monitoring-dashboard).

## 6.1. How to deploy a live ML monitoring dashboard

### Why build a monitoring dashboard?

Tests and reports are great for running structured checks and exploring and debugging your data and ML models. However, when you run tests or build an ad-hoc report, you evaluate only a specific batch of data. This makes your monitoring “static”: you can get alerts but have no visibility into metric evolution and trends.

That is where the ML monitoring dashboard comes into play, as it:

* Tracks metrics over time,

* Aids in trend analysis and provides insights,

* Provides a shared UI to enhance visibility for stakeholders, e.g., data scientists, model users, product managers, business stakeholders, etc.

![image](https://github.com/user-attachments/assets/c29e7aa1-76e4-4aa3-bd9d-4188fc63eba0)

### ML monitoring architecture recap

![image](https://github.com/user-attachments/assets/3dac8b3e-8cfa-46e4-8d11-3f6dc07d093b)

## 6.2. ML model monitoring dashboard with Evidently. Batch architecture (code practice)


```python
import datetime

from sklearn import datasets

from evidently.report import Report
from evidently.metrics import ColumnDriftMetric, DatasetDriftMetric

from evidently.test_suite import TestSuite
from evidently.test_preset import DataDriftTestPreset

from evidently.ui.dashboards import CounterAgg
from evidently.ui.dashboards import DashboardPanelCounter
from evidently.ui.dashboards import DashboardPanelPlot
from evidently.ui.dashboards import PanelValue
from evidently.ui.dashboards import PlotType
from evidently.ui.dashboards import ReportFilter
from evidently.ui.dashboards import DashboardPanelTestSuite
from evidently.ui.dashboards import TestFilter
from evidently.ui.dashboards import TestSuitePanelType
from evidently.renderers.html_widgets import WidgetSize

from evidently.ui.workspace import Workspace
from evidently.ui.workspace import WorkspaceBase

bank_marketing = datasets.fetch_openml(name='bank-marketing', as_frame='auto')
bank_marketing_data = bank_marketing.frame

reference_data = bank_marketing_data[5000:7000]
prod_simulation_data = bank_marketing_data[7000:]
batch_size = 2000

WORKSPACE = "bank_data"

YOUR_PROJECT_NAME = "Bank Marketing Classification"
YOUR_PROJECT_DESCRIPTION = "Test project using Bank Marketing dataset"


def create_data_quality_report(i: int):
    report = Report(
        metrics=[
            DatasetDriftMetric(),
            ColumnDriftMetric(column_name="Class"),
        ],
        timestamp=datetime.datetime.now() + datetime.timedelta(days=i),
    )

    report.run(reference_data=reference_data, current_data=prod_simulation_data[i * batch_size : (i + 1) * batch_size])
    return report

def create_data_drift_test_suite(i: int):
    suite = TestSuite(
        tests=[
            DataDriftTestPreset()
        ],
        timestamp=datetime.datetime.now() + datetime.timedelta(days=i),
        tags = []
    )

    suite.run(reference_data=reference_data, current_data=prod_simulation_data[i * batch_size : (i + 1) * batch_size])
    return suite

def create_project(workspace: WorkspaceBase):
    project = workspace.create_project(YOUR_PROJECT_NAME)
    project.description = YOUR_PROJECT_DESCRIPTION
    project.dashboard.add_panel(
        DashboardPanelCounter(
            filter=ReportFilter(metadata_values={}, tag_values=[]),
            agg=CounterAgg.NONE,
            title="Bank Marketing Dataset",
        )
    )
    
    project.dashboard.add_panel(
        DashboardPanelPlot(
            title="Target Drift",
            filter=ReportFilter(metadata_values={}, tag_values=[]),
            values=[
                PanelValue(
                    metric_id="ColumnDriftMetric",
                    metric_args={"column_name.name": "Class"},
                    field_path=ColumnDriftMetric.fields.drift_score,
                    legend="target: Class",
                ),
            ],
            plot_type=PlotType.LINE,
            size=WidgetSize.HALF
        )
    )

    project.dashboard.add_panel(
        DashboardPanelPlot(
            title="Dataset Drift",
            filter=ReportFilter(metadata_values={}, tag_values=[]),
            values=[
                PanelValue(metric_id="DatasetDriftMetric", field_path="share_of_drifted_columns", legend="Drift Share"),
            ],
            plot_type=PlotType.BAR,
            size=WidgetSize.HALF
        )
    )

    project.dashboard.add_panel(
        DashboardPanelTestSuite(
            title="Data Drift tests",
            filter=ReportFilter(metadata_values={}, tag_values=[], include_test_suites=True),
            size=WidgetSize.HALF
        )
    )

    project.dashboard.add_panel(
        DashboardPanelTestSuite(
            title="Data Drift tests: detailed",
            filter=ReportFilter(metadata_values={}, tag_values=[], include_test_suites=True),
            size=WidgetSize.HALF,
            panel_type=TestSuitePanelType.DETAILED

        )
    )

    project.save()
    return project


def create_demo_project(workspace: str):
    ws = Workspace.create(workspace)
    project = create_project(ws)

    for i in range(0, 10):
        report = create_data_quality_report(i=i)
        ws.add_report(project.id, report)

        suite = create_data_drift_test_suite(i=i)
        ws.add_report(project.id, suite)


if __name__ == "__main__":
    create_demo_project(WORKSPACE)
```

First, execute the above script, and then we can run `evidently ui --workspace bank_data`

<img width="667" alt="image" src="https://github.com/user-attachments/assets/d0e64ac2-b250-4021-b56e-7cfb0d8e2d84">

And when we go to the browser:

<img width="1508" alt="image" src="https://github.com/user-attachments/assets/4bf9d743-5131-476a-9141-02948e74f8b8">

<img width="1511" alt="image" src="https://github.com/user-attachments/assets/e600edc2-bbce-4cc4-9c62-8cad239f04a5">

We can view indiviaul reports and tests:

<img width="488" alt="image" src="https://github.com/user-attachments/assets/fc7afad2-5112-4575-a0d1-1ab99d57232d">

## 6.3. ML model monitoring dashboard with Evidently. Online architecture (code practice)

```python
import datetime
import os.path
import time
import pandas as pd

from requests.exceptions import RequestException
from sklearn import datasets

from evidently.collector.client import CollectorClient
from evidently.collector.config import CollectorConfig, IntervalTrigger, ReportConfig

from evidently.test_suite import TestSuite
from evidently.test_preset import DataDriftTestPreset

from evidently.ui.dashboards import DashboardPanelTestSuite 
from evidently.ui.dashboards import ReportFilter
from evidently.ui.dashboards import TestFilter
from evidently.ui.dashboards import TestSuitePanelType
from evidently.renderers.html_widgets import WidgetSize
from evidently.ui.workspace import Workspace

COLLECTOR_ID = "default"
COLLECTOR_TEST_ID = "default_test"

PROJECT_NAME = "Bank Marketing: online service "
WORKSACE_PATH = "bank_data"

client = CollectorClient("http://localhost:8001")

bank_marketing = datasets.fetch_openml(name="bank-marketing", as_frame="auto")
bank_marketing_data = bank_marketing.frame
reference_data = bank_marketing_data[5000:5500]
prod_simulation_data = bank_marketing_data[7000:]
mini_batch_size = 50

def setup_test_suite():
	suite = TestSuite(tests=[DataDriftTestPreset()], tags=[])
	suite.run(reference_data=reference_data, current_data=prod_simulation_data[:mini_batch_size])
	return ReportConfig.from_test_suite(suite)

def workspace_setup():
	ws = Workspace.create(WORKSACE_PATH)
	project = ws.create_project(PROJECT_NAME)
	project.dashboard.add_panel(
		DashboardPanelTestSuite(
			title="Data Drift Tests",
			filter=ReportFilter(metadata_values={}, tag_values=[], include_test_suites=True),
			size=WidgetSize.HALF
		)
	)
	project.dashboard.add_panel(
		DashboardPanelTestSuite(
			title="Data Drift Tests",
			filter=ReportFilter(metadata_values={}, tag_values=[], include_test_suites=True),
			size=WidgetSize.HALF,
			panel_type=TestSuitePanelType.DETAILED
		)
	)
	project.save()

def setup_config():
	ws = Workspace.create(WORKSACE_PATH)
	project = ws.search_project(PROJECT_NAME)[0]

	test_conf = CollectorConfig(trigger=IntervalTrigger(interval=5),
		report_config=setup_test_suite(), project_id=str(project.id))

	client.create_collector(COLLECTOR_TEST_ID, test_conf)
	client.set_reference(COLLECTOR_TEST_ID, reference_data)

def send_data():
	print("Start sending data")
	for i in range(50):
		try:
			data = prod_simulation_data[i * mini_batch_size : (i + 1) * mini_batch_size]
			client.send_data(COLLECTOR_TEST_ID, data)
			print("sent")
		except RequestException as e:
			print(f"collector service is not available: {e.__class__.__name__}")
		time.sleep(1)

def main():
	if not os.path.exists(WORKSACE_PATH) or len(Workspace.create(WORKSACE_PATH).search_project(PROJECT_NAME)) == 0:
		workspace_setup()

	setup_config()
	send_data()

if __name__ == '__main__':
	main()
```

This is the same as before, but now we update the evidently UI as new data is coming in. It is called 'near real-time' monitoring.

---

Overall, what a great course. Definitely going in my 'courses to recommend' list.

## Neo4j courses

Today I found neo4j offers free courses and tests. 

* [neo4j certified professional](https://graphacademy.neo4j.com/certifications/neo4j-certification/) (its free and I get a free t-shirt if I succeed)
* [neo4j graph data science certification](https://graphacademy.neo4j.com/courses/gds-certification/)

## [Neo4j Fundamentals](https://graphacademy.neo4j.com/courses/neo4j-fundamentals/)

<img width="285" alt="image" src="https://github.com/user-attachments/assets/70620086-bdbe-49c2-aed8-a8f4391f4b55">

Graph thinking:

- where graph theory originated (Russia)
- what are nodes and edges
- directed vs undirected graphs
- shortest path algorithms
- graph use cases (i.e. recommendations, network dependency analysis, supply chain management)

Property graphs:

<img width="813" alt="image" src="https://github.com/user-attachments/assets/73c84766-d6c3-414f-909b-93f4970f3db2">

RDBMS vs Native Graph db

<img width="537" alt="image" src="https://github.com/user-attachments/assets/64c2029d-24d7-476f-bff9-2e503a2de7fc">

<img width="555" alt="image" src="https://github.com/user-attachments/assets/b32a369c-66a1-41ec-9f0b-a28c7b62f3ac">


1. Neo4j offers faster queries due to index-free adjacency, as relationships are stored during writing, unlike relational databases where joins are computed during reading.

2. It simplifies relationship modeling, making it more intuitive than using pivot tables for many-to-many relationships in relational databases.

The last bit was using Cypher to do basic node and relationship querying on a movie dataset.

<img width="1041" alt="image" src="https://github.com/user-attachments/assets/8c2ca07d-0275-45c7-be3f-737d75ab73b0">

## [Cyper Fundamentals](https://graphacademy.neo4j.com/courses/cypher-fundamentals/)

<img width="284" alt="image" src="https://github.com/user-attachments/assets/87b47e9f-11d5-4d42-86d8-a5addde9f886">
<img width="269" alt="image" src="https://github.com/user-attachments/assets/5d49ce0f-ae00-4559-9149-f1ce00e29c36">


Sample graph structure

<img width="553" alt="image" src="https://github.com/user-attachments/assets/b8823d8b-1ce1-4e93-8326-5257488689b5">

Retrieving exercise for Kevin Bacon's birht year:

```cypher
MATCH (p:Person)
where p.name = "Kevin Bacon"
RETURN p.born
```

-> 1958

Another exercise: How many people directed the movie Cloud Atlas?

```cypher
MATCH (m:Movie {title: 'Cloud Atlas'})<-[:DIRECTED]-(p) 
RETURN count(p)
```

-> 3

Another exercise: How many actors in the movie The Matrix were born after 1960?

```cypher
MATCH (a:Person)-[:ACTED_IN]->(m:Movie)
WHERE a.born > 1960 AND m.title = 'The Matrix'
RETURN count(a)
```

-> 4

Create a node exercise

```cypher
MERGE (p:Person {name: "Daniel Kaluuya"})
```

Added 1 label, created 1 node, set 1 property, completed after 98 ms. MERGE and CREATE can be used but MERGE first checks if a node with that property exists, and if it does, it does nothing (using MERGE is best practice)

Another exercise:

1. Find the Person node for Daniel Kaluuya.

2. Create the Movie node, Get Out.

3. Add the ACTED_IN relationship between Daniel Kaluuya and the movie, Get Out.

```cypher
MATCH (p:Person {name: "Daniel Kaluuya"})
MERGE (m:Movie {title: "Get Out"})
MERGE (p)-[:ACTED_IN]->(m)
```

-> Added 1 label, created 1 node, set 1 property, created 1 relationship, completed after 22 ms.

Setting properties exercise: Write the Cypher code to find this Movie node and add the tagline and released properties for the node with the values tagline: Gripping, scary, witty and timely! and released: 2017

```cypher
MATCH (m:Movie {title: 'Get Out'})
SET m.tagline = 'Gripping, scary, witty and timely!'
SET m.released = 2017
RETURN m.title, m.tagline, m.released
```

In neo4j we can create nodes and relationships on individual lines with MERGE, or in 1 line:

```cypher
MERGE (p:Person {name: 'Michael Caine'})-[:ACTED_IN]->(m:Movie {title: 'The Cider House Rules'})
RETURN p, m
```

and this is what heppens under the hood:

1. Neo4j will attempt to find a Person node with the name Michael Caine.

2. If it does not exist, it creates the node.

3. Then, it will attempt to expand the ACTED_IN relationships in the graph for this node.

4. If there are any ACTED_IN relationships from this node, it looks for a Movie with the title 'The Cider House Rules'.

5. If there is no node for the Movie, it creates the node.

6. If there is no relationship between the two nodes, it then creates the ACTED_IN relationship between them.

However this can result in an error if one of the items does not exist. I.e. if our graph has a node for Michael, but there is not for the movie. It tried to create the entire pattern, but it failed because the node Person with Michael Caine already exists. That is why it is best practice to use MERGE on individual lines when adding nodes/relationships

```cypher
// Find or create a person with this name
MERGE (p:Person {name: 'Michael Caine'})

// Find or create a movie with this title
MERGE (m:Movie {title: 'The Cider House Rules'})

// Find or create a relationship between the two nodes
MERGE (p)-[:ACTED_IN]->(m)
```

To remove properties we can use REMOVE, to remove node/edge we use DELETE

If a node has a relationship, the operation will result in an error. To avoid this, we can use DETACH DELETE which 1st deletes any relationship and then the node

<img width="1043" alt="image" src="https://github.com/user-attachments/assets/49828498-3340-42af-b4bd-c89d34137d46">

## [Graph Data Modelling Fundamentals](https://graphacademy.neo4j.com/courses/modeling-fundamentals/)

Course structure:

1. Intro
2. Node modelling
3. Relationships modelling
4. Testing the model
5. Refactoring the graph
6. Eliminating duplicate data
7. Using specific relationships
8. Adding intermediate nodes



Data modeling process

Here are the steps to create a graph data model:

* Understand the domain and define specific use cases (questions) for the application.

* Develop the initial graph data model:

  * Model the nodes (entities).

  * Model the relationships between nodes.

* Test the use cases against the initial data model.

* Create the graph (instance model) with test data using Cypher.

* Test the use cases, including performance against the graph.

* Refactor (improve) the graph data model due to a change in the key use cases or for performance reasons.

* Implement the refactoring on the graph and retest using Cypher.

Graph data modeling is an iterative process. Your initial graph data model is a starting point, but as you learn more about the use cases or if the use cases change, the initial graph data model will need to change. In addition, you may find that especially when the graph scales, you will need to modify the graph (refactor) to achieve the best performance for your key use cases.

Refactoring is very common in the development process. A Neo4j graph has an optional schema which is quite flexible, unlike the schema in an RDBMS. A Cypher developer can easily modify the graph to represent an improved data model.

### Types of models

When performing the graph data modeling process for an application, you will need at least two types of models:

* Data model

* Instance model

Data model

The data model describes the labels, relationships, and properties for the graph. It does not have specific data that will be created in the graph.

Here is an example of a data model:

<img width="463" alt="image" src="https://github.com/user-attachments/assets/ab1ba46c-528d-4287-bf6f-68d4a2826b6b">

There is nothing that uniquely identifies a node with a given label. A graph data model, however is important because it defines the names that will be used for labels, relationship types, and properties when the graph is created and used by the application.

Instance model

An important part of the graph data modeling process is to test the model against the use cases. To do this, you need to have a set of sample data that you can use to see if the use cases can be answered with the model.

Here is an example of an instance model:

<img width="449" alt="image" src="https://github.com/user-attachments/assets/44b3edba-1196-403d-8b13-20d79e424487">

In this instance model, we have created some instances of Person and Movie nodes, as well as their relationships. Having this type of instance model will help us to test our use cases.


### Modelling nodes

#### Defining labels

Entities are the dominant nouns in your application use cases:

* What ingredients are used in a recipe?

* Who is married to this person?

The entities of your use cases will be the labeled nodes in the graph data model.

In the Movie domain, we use the nouns in our use cases to define the labels, for example:

* What people acted in a movie?

* What person directed a movie?

* What movies did a person act in?

Here are some of the labeled nodes that we will start with.

<img width="444" alt="image" src="https://github.com/user-attachments/assets/692a5f3d-3cdd-4c36-8482-05b14c6686a9">

#### Properties for nodes

Depending on our use cases we need to decide on node properties

| Use Case                                                        | Steps Required                                             |
|------------------------------------------------------------------|------------------------------------------------------------|
| 1: What people acted in a movie?                                 | Retrieve a movie by its title.                             |
|                                                                  | Return the names of the actors.                            |
| 2: What person directed a movie?                                 | Retrieve a movie by its title.                             |
|                                                                  | Return the name of the director.                           |
| 3: What movies did a person act in?                              | Retrieve a person by their name.                           |
|                                                                  | Return the titles of the movies.                           |
| 5: Who was the youngest person to act in a movie?                | Retrieve a movie by its title.                             |
|                                                                  | Evaluate the ages of the actors.                           |
|                                                                  | Return the name of the youngest actor.                     |
| 7: What is the highest rated movie in a particular year?         | Retrieve all movies released in a particular year.         |
|                                                                  | Evaluate the IMDb ratings.                                 |
|                                                                  | Return the movie title.                                    |
| 8: What drama movies did an actor act in?                        | Retrieve the actor by name.                                |
|                                                                  | Evaluate the genres for the movies the actor acted in.      |
|                                                                  | Return the movie titles.                                   |

Here is the initial data model

<img width="431" alt="image" src="https://github.com/user-attachments/assets/61db3d35-f992-471d-a762-491cb02e22a7">

And an initial instance model

<img width="458" alt="image" src="https://github.com/user-attachments/assets/83edc108-1bd9-4d44-a1b2-aabbcb172be9">


### Modelling relationships

Will continue tomorrow ^^


## Pen and paper exercises in ML: Optimisation

![Optimisation P1](https://github.com/user-attachments/assets/623ad2eb-70dc-4408-a757-72df5b9b4c7b)
![Optimisation P2](https://github.com/user-attachments/assets/f34d04e0-e768-44e6-9483-b487a2d33fd6)

---

That is all for today!

See you tomorrow :)
