---
layout: post
title: (Day 230) Watching more educational videos from probabl
categories: [traditional-machine-learning,applying-knowledge,theory]
---

# Hello :) Today is Day 230!
A quick summary of today:
* Protecting against temporal data leaks and skrub's TableVectorizer
* Embeddings in scikit-learn pipelines
* Little about timeseries
* Sparse data pipelines
* GMM for outlier detection

## Protecting against temporal data leaks and skrub's TableVectorizer

* [protecting against data leaks](https://www.youtube.com/watch?v=uevp7zJTM_c) - when creating time series models, it's crucial to ensure that the model learns patterns based only on past data and doesn't 'cheat' by accessing future information. However, it's easy to accidentally introduce data leakage, where the model unintentionally uses future data during training. To help data scientists avoid this mistake, Vincent (the probable instructor) has developed a library that safeguards against such errors by ensuring that the model only learns from past data

* [TableVectorizer](https://www.youtube.com/watch?v=wn0g1wBzDfQ) for quick feature preprocessing - this comes from skrub (a library that seems to be maintained by some sklearn devs) and we can include it in a pipeline with some model, and then get a working pipeline. The TV automatically determines data types and what kind of processing it will do to them - ordinal encoding, OHE, scaling, etc. It is a good tool to get us started. As Vincent said - some of his sklearn colleagues get a new dataset, put it through a TableVectorizer and a HistGradientBoostingClassifier (or Regressor) and get a pretty good model that can serve as an initial checkpoint

## Embeddings in scikit-learn pipelines

Vincent has been maintaining an embedding library that is built upon scikit-learn classes so it directly integrates with other scikit-learn components - [embetter](https://github.com/koaning/embetter/)

Taken from the github repo, with just a few lines of code we can get started with a text sentiment classifier:

```python
import pandas as pd
from sklearn.pipeline import make_pipeline 
from sklearn.linear_model import LogisticRegression

from embetter.grab import ColumnGrabber
from embetter.text import SentenceEncoder

# This pipeline grabs the `text` column from a dataframe
# which then get fed into Sentence-Transformers' all-MiniLM-L6-v2.
text_emb_pipeline = make_pipeline(
  ColumnGrabber("text"),
  SentenceEncoder('all-MiniLM-L6-v2')
)

# This pipeline can also be trained to make predictions, using
# the embedded features. 
text_clf_pipeline = make_pipeline(
  text_emb_pipeline,
  LogisticRegression()
)

dataf = pd.DataFrame({
  "text": ["positive sentiment", "super negative"],
  "label_col": ["pos", "neg"]
})
X = text_emb_pipeline.fit_transform(dataf, dataf['label_col'])
text_clf_pipeline.fit(dataf, dataf['label_col']).predict(dataf)
```

It is not here to replace pytorch or tensorflow as they are powerful libraries. Its purpose is for an easy setup and start up to get a prototype going.

## [Little about timeseries](https://youtu.be/93tV-1ckqto)

For periodic features in scikit-learn there is this SplineTransformer 

```python
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import FunctionTransformer, SplineTransformer

pipe = make_pipeline(
    FunctionTransformer(datetime_feats),
    SplineTransformer(n_knots=12, extrapolation="periodic")
)
```

With 12 knots we get the below. The data is daily weather data.

<img width="704" alt="image" src="https://github.com/user-attachments/assets/bcddc98d-3b82-467c-a74c-c78b060305cc">

Then using this as a feature pipeline, and adding a Ridge on top we get:

<img width="983" alt="image" src="https://github.com/user-attachments/assets/61f177ce-df7b-46cd-b52d-da0a8675017b">

To get something with quantiles, we can use the `QuantileRegressor`:

```python
from sklearn.linear_model import QuantileRegressor

feat_pipe = make_pipeline(
    FunctionTransformer(datetime_feats),
    SplineTransformer(n_knots=6, extrapolation="periodic")
)

mod_pipe_q_lower = make_pipeline(feat_pipe, QuantileRegressor(quantile=0.1, alpha=0.001))
mod_pipe_q_upper = make_pipeline(feat_pipe, QuantileRegressor(quantile=0.9, alpha=0.001))

mod_pipe_q_lower.fit(X, y)
mod_pipe_q_upper.fit(X, y)
```

<img width="982" alt="image" src="https://github.com/user-attachments/assets/b8591e76-c4c1-4419-85fd-af65ced82f47">

Finally, Vincent (the instructor) just talked about being careful when using lags in fear of data leakage. 

## [Sparse data pipelines](https://youtu.be/u_cFkXFGj0c)

#### Dense Matrices
- matrices where most elements are non-zero
- requires `O(m * n)` memory for an `m x n` matrix
- optimized for general operations, but inefficient for large matrices with many zeros

#### Sparse Matrices
- matrices with most elements being zero
- represented by `scipy.sparse` (e.g., CSR, CSC, COO formats)
- only stores non-zero elements and their positions, leading to significant memory savings
- efficient for operations involving non-zero elements; some operations may require conversion to dense format

#### Use Cases
- Dense: Use when the matrix is small or fully populated.
- Sparse: Use for large matrices with many zeros (e.g., term-document matrices, graph algorithms).

Example use case of sparse matrices is with text. And scikit-learn uses sparse matrices off the shelf

```python
from sklearn.feature_extraction.text import CountVectorizer
from joblib import dump
from pathlib import Path

out = CountVectorizer().fit_transform(texts) # .todense()
dump(out, 'tmp.pickle')
Path('tmp.pickle').stat().st_size
```

The size we would get is: ~2.2m bytes. If we use a dense matrix (by adding `.todense()` to the code), we would get 1.3b bytes. So we can clearly see the efficiency of sparse matrices when it comes to memory if we have to save something to disk. 

## [Expanding on GMMs](https://youtu.be/821aPPsFPsY)

Yesterday I learned about using PCA for outlier detection, today Guassian Mixture Models.

```python
import numpy as np
import matplotlib.pylab as plt

from sklearn.datasets import make_moons
from sklearn.preprocessing import StandardScaler

from sklego.mixture import GMMOutlierDetector

n = 1000
X = make_moons(n)[0] + np.random.normal(n, 0.12, (n, 2))
X = StandardScaler().fit_transform(X)
U = np.random.uniform(-2, 2, (10000, 2))

mod = GMMOutlierDetector(n_components=16, threshold=0.95).fit(X)

plt.figure(figsize=(14, 5))
plt.subplot(121)
plt.scatter(X[:, 0], X[:, 1], c=mod.score_samples(X), s=8)
plt.title("likelihood of points given mixture of 16 gaussians");

plt.subplot(122)
plt.scatter(U[:, 0], U[:, 1], c=mod.predict(U), s=8)
plt.title("outlier selection");
```

![image](https://github.com/user-attachments/assets/ebc79739-69ae-46c4-855c-a1146a2e79b3)

Depending on the threshold:

![image](https://github.com/user-attachments/assets/71f5f455-cf9b-4f38-8bf7-f2d83857cd8c)

In the scikit-lego library maintained by Vincent there is also a GMMClassifier 

---

The streams are 1h long each so it takes some time to watch. Nevertheless its quite interesting!

That is all for today!

See you tomorrow :)
