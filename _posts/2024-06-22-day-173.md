---
layout: post
title: (Day 173) Terraform, GCP, virtual machines, data pipelines
categories: [data-eng,cloud,applying-knowledge]
---

## Hello :) Today is Day 173!
A quick summary of today:
* learned more about terraform and how to set up a GCP VM and connect to it locally
* used mage for some data engineering pipelines with GCP

### Last videos from Module 1: terraform variables, GCP set up

Turns out there is a bit more of terraform from the data eng zoomcamp, and today I covered it.

After learning how to connect to gcp using terraform and create a storage bucket, the first thing today was creating a bigquery dataset

![image](https://github.com/user-attachments/assets/4d0d500d-5d9a-4e4b-85f0-04e6f716fa9e)

Adding the above to main.tf which now looks like:

![image](https://github.com/user-attachments/assets/48f8f05b-0e76-4947-b63d-a6b27cc8ca9f)

terraform apply, creates a demo_dataset as well

![image](https://github.com/user-attachments/assets/ccb04ba7-2e42-4e56-a8db-00bc7dc0b927)

Then I learned about variables in terraform

Create a variables.tf file and put a variable like:

![image](https://github.com/user-attachments/assets/685a7f1e-0952-4f8d-8a87-9decdf3ab9ff)

and in main.tf we can directly use the created variables like:

![image](https://github.com/user-attachments/assets/186c76eb-7c3e-43d1-b7bc-d08dbba34d0b)

Great intro to terraform - being able to define infrastructure code, create resources, and destroy resources.

The next part was an instruction on setting up GCP (cloud VM + SSH access)

First was creating an ssh key locally

![image](https://github.com/user-attachments/assets/fe82c565-756f-4707-9e20-426dcb5813a1)

And add it to the metadata in GCP's compute engine (hiding the username just in case)

![image](https://github.com/user-attachments/assets/b8fd77c4-50ac-4003-9d6d-8b2bcf74778f)

Then create a VM, and connect to it locally using that ssh key, using `ssh -i ~/.ssh/gcp username@gcp_vm_external_ip`

![image](https://github.com/user-attachments/assets/90f569c1-a451-4e67-a9e1-5f0afa575ebb)
![image](https://github.com/user-attachments/assets/87a10fd4-0beb-437c-99a9-3cbcc0ea12af)

For a quick connection to the VM, I set up a config which includes Host, HostName, User and IdentityFile, so now I can just run ssh `Host` and I am connected to the VM through my terminal. Nice.

Also set up vs code to connect to the created ssd

![image](https://github.com/user-attachments/assets/76b80425-a50c-4bac-9b79-c1cdc8597c25)

Then, I installed anaconda. 

![image](https://github.com/user-attachments/assets/1ccb1f56-34b5-48a6-92c8-5413279201b1)

And docker

![image](https://github.com/user-attachments/assets/e756126f-b6b8-49ea-a4a8-fe059f888c0f)

Then docker-compose

![image](https://github.com/user-attachments/assets/8c0f113b-db47-42c9-b45d-5ce5c1e4f9f1)

And make it executable from anywhere by adding the below to .bashrc
`export PATH="${HOME}/bin:${PATH}"`

And now we have it 

![image](https://github.com/user-attachments/assets/23d2d024-c9bf-43b1-9421-a740f7e028db)

Then installed pgcli with conda

![image](https://github.com/user-attachments/assets/a47938bc-503e-42f1-90ba-3cb8202ab2eb)

(random note - I am using a vm from my local terminal like this for the 1st time and its kind of cool)

And just like before(2 days ago, first part of the data eng zoomcamp), I can run docker-compose up -d and then pgcli to connect to mt db

![image](https://github.com/user-attachments/assets/689ab26c-3456-4874-a691-3ac16e8b1a46)
![image](https://github.com/user-attachments/assets/0eb1c2f6-1b08-4b88-bc6e-51380d626e8d)

In VS code that is connected to the VM, we can forward the port to the db

![image](https://github.com/user-attachments/assets/bc3ae7c0-59cf-433b-8355-e6917156373f)

And now when I run pgcli from my own PC's terminal, I can connect to it too.

![image](https://github.com/user-attachments/assets/4b99d843-c4f7-43ea-b24e-1e032d61dd87)

By adding port 8080 as well in VS code, I now can access pgadmin too from my browser (even tho it is all running on that GCP VM)

![image](https://github.com/user-attachments/assets/76cbfd68-4337-4654-b6c1-a6ca84f1f723)

Same for jupyter - added port 8888 in vs code, then I can run jupyter notebook in the VM, and access in my browser. 

Next, I installed terraform for linux

![image](https://github.com/user-attachments/assets/5bc1eea7-446d-4a12-8a28-0b8fa46bfab2)

When I was setting up terraform, I created a my-creds.json with the credentials from GCP. Now using sftp I transferred the json file from my local to the VM (sftp - another tool I am using for the 1st time)

![image](https://github.com/user-attachments/assets/2b158fb0-7717-4a0a-ba54-da8049fc2c7d)

And then I could run the same terraform apply and destroy to create and destroy resources.

And if I want to stop and restart the instance I can do it through the terminal (`sudo shutdown now`) or the GCP console. And when I start it again in order for the quick ssh connection command to work, I need to edit the config file's HostName that I created earlier. 
I found that if I restart the VM and want to use terraform, I need to set my credentials and gcloud auth using (and also just saving the commands for later):

`export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/my-creds.json`
`gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS`

### Next onto [Module 2](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/02-workflow-orchestration#222---%EF%B8%8F-intro-to-mage): workflow orchestration

My good old friend mage.ai. Let's hope for at least less errors than when I covered it in the MLOps zoomcamp. 

The first bit was to establish a connection with the postgres database which is ran alongside mage in docker-compose.yml

![image](https://github.com/user-attachments/assets/af92e004-10a8-4bbc-8a27-25df1f67e5a7)

Creating a block in a new pipeline to test the connection:

![image](https://github.com/user-attachments/assets/8cf45bc1-5fe6-4e2e-b5ef-b8fc69949172)

So far so good.

Next is writing a simple ETL pipeline - loading data from an API to postgres, where I just load taxi data in the first block using data type checking, do a little bit of preprocessing in the 2nd block and then make a connection to my db and load the data there in the 3rd block

![image](https://github.com/user-attachments/assets/37fcd9a8-e416-4dc0-9e9f-f24224ee69a4)

Next is connecting my gcp service account to mage (using the creds.json file), and a connection is made. 

Loading data to google cloud storage is very easy - just adding a google cloud storage (GCS) data exporter block, and putting my info down and its done

![image](https://github.com/user-attachments/assets/2ca381e8-49e4-4bbb-819d-8b655207c698)

Can be seen in gcs

![image](https://github.com/user-attachments/assets/d9f2cd8d-01bd-4ea5-b5f0-742070fe760e)

However, with larger files we should not load data into a single parquet file. We should partition it.

I learned how to use pyarrow (as it abstracts chunking logic) for that as in the below block:

![image](https://github.com/user-attachments/assets/2f3d928b-37e2-430a-98d1-5b48621c4b42)

And the partitioned data is in gcs now. Awesome

![image](https://github.com/user-attachments/assets/cf10fec2-2c90-4080-8bdd-938c7395d736)

And then load it into BigQuery

![image](https://github.com/user-attachments/assets/ba0df419-911d-4d90-a780-c4b0c658ed7f)

My experience using mage in this course compared to the MLOps zoomcamp is completely different haha. Now, the teacher Matt Palmer did a great job ^^


That is all for today!

See you tomorrow :)
