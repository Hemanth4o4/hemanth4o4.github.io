---
layout: post
title: (Day 48) KAIST's AI503 Mathematics for AI (Matrix Decompositions)
categories: [theory,math]
---

## Hello :) Today is Day 48!
A quick summary of today:
* Started Week 1 - Matrix decompositions of [AI503 Math for AI](https://alinlab.kaist.ac.kr/ai503_2021.html)

Following the book, I read the [chapter](https://mml-book.github.io/book/mml-book.pdf) and took notes on my tablet.

I hope the pics are legible. But the content is:
- Determinant and Trace
- Eigenvalues and eigenvectors
- Cholesky decomposition
- Eigendecomposition
- Singular Value Decomposition
- Exercises

Before the exerises it is mostly the theory. But the real deal was doing the exercises. In particular finding how to do SVD. Firstly I tried to learn how to calculate eigen values and eigenvectors, and then I moved onto SVD. 
Finding the proper way to solve SVDs... it took me a while. Firstly I followed some medium post, but then it turned out that the person made mistakes which were pointed out in the comments.

SVD formula is A = E*V*U. Some of the problem came from the fact that I was not sure the exact application of this, but I just wanted to learn how to do it now, because down the line I am sure it will lead to something bigger. They key to understanding it all was this [video](https://www.youtube.com/watch?v=cOUTpqlX-Xs) from an MIT professor.

To do SVD, the steps are:
1) multiply the matrix with its transposed version
2) Do the detrerminant and find the eigen values

![image](https://github.com/user-attachments/assets/8f73e709-af5f-4cbc-b38d-0ea4c022c13f)

3) From the eigenvalues you get the Epsilon

![image](https://github.com/user-attachments/assets/31974b8a-dfae-4382-a1ab-6a89df7c717d)

4) Find the eigenvectors by deducting lambda indetity matrix from the A transposed * A, then normalize and you get V

![image](https://github.com/user-attachments/assets/2f831353-b683-4f61-8ad6-63f8aae75d3e)

5) Find U by normalizing the product of A and V

![image](https://github.com/user-attachments/assets/b58048ba-cddc-46e0-b59c-809976fd0e9e)

At first I was not sure how to get the Epsilon, another problem was when doing the eigenvectors, how do I get v1 and v2 (i.e. equalizing the matrix to 0), but I found out that the right result is, by what column vector do I have to multiply the matrix in order to get 0. Another issue was finding U, just how to do it. Many websites that do 'step-by-step' tutorial actually skip some steps because they assume the reader knows, but thanks to the MIT video most of these 'holes' were fixed. The above is my final exercise, but below are all my notes :)

![image](https://github.com/user-attachments/assets/dcc6d23f-bb04-4b0f-a9d9-958b0bbbf121)
![image](https://github.com/user-attachments/assets/2a25a6d9-860a-487a-9bb6-02247ff303f5)
![image](https://github.com/user-attachments/assets/40328283-bd52-47eb-848f-92df546f9cb1)
![image](https://github.com/user-attachments/assets/fb921a32-07f6-467d-a003-f595adbc0b86)
![image](https://github.com/user-attachments/assets/8f4d9e74-4634-4216-b6d7-6fcbf69560ff)
![image](https://github.com/user-attachments/assets/f4e6d6a5-8187-438d-ba23-c29c068e3467)
![image](https://github.com/user-attachments/assets/803d7e0e-165f-4a4b-92ed-cd941a338a46)
![image](https://github.com/user-attachments/assets/c419ec08-a070-42e7-bd6b-578f339d59e4)
![image](https://github.com/user-attachments/assets/ab759274-3ce8-4cb5-9aa4-ec2b05149044)
![image](https://github.com/user-attachments/assets/5a8ad570-dcc4-4dcc-8aa5-9d1bfc4c9eec)
![image](https://github.com/user-attachments/assets/37985cb7-de7d-4fd7-8024-c552d5a98f34)
![image](https://github.com/user-attachments/assets/75a7f003-5853-403a-9888-9d5b7ebf9ed6)
![image](https://github.com/user-attachments/assets/4da50c28-9656-4ab6-9533-2d70d49961e8)
![image](https://github.com/user-attachments/assets/8d87714a-ec3b-4517-81f4-12ad3edaa761)

I am actually still not 100% sure the results I got for SVDs are correct only because I saw comments noting that minuses and the order of values in some matricis matters. I asked in a ML community but I am waiting for a response.


That is all for today!

See you tomorrow :) 
