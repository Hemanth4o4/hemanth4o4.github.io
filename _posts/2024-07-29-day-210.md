---
layout: post
title: (Day 210) 118 minutes of Glaswegian accent audio clips
categories: [glaswegian,applying-knowledge]
---

## Hello :) Today is Day 210!
A quick summary of today:
* final audio clips preprocessing to reach our audio dataset mark

![image](https://github.com/user-attachments/assets/5b167a4c-0a29-4fca-97f9-ba01d5121171)

Final dataset for the glaswegian voice assistant AI ([link to HuggingFace](https://huggingface.co/datasets/divakaivan/glaswegian_audio)). Today I preprocessed the final audios from 2 of Limmy's youtube videos (Limmy accidentaly kills the city and The writer of Saw called Limmy a ...). 
Just an update on how the process goes now ~ 

Since our transcription AI is pretty good (according to my Glaswegian speaking project partner), we pass the full raw audio to our [fine-tuned whisper](https://huggingface.co/spaces/divakaivan/glaswegian-whisper) model hosten on HuggingFace spaces. Then the transcript is put into a docs file (where first I check over it for obious mistakes and flag if I see something odd and cannot understand it from re-listening to the audio) and split into sensible (small) bits while listening to the audio, like:

![image](https://github.com/user-attachments/assets/4dc85181-d691-4b04-af57-6aeb3244e094)

(this is the start from Limmy accidentaly kills the city)

Then using an audio tool, I cut the full audio length into clips according to the cut text, then I match clip name and transcript in excel, then using python I get the clip length and sampling rate. Finally I add static info like gender, age, class, location, speaker id to the data, and finally I get a csv which I upload onto the hugging face dataset along with the cut audio clips.

Next steps are to train a final whisper model, and then a Text-To-Speech model using this final dataset. Maybe tomorrow when we go to breakfast I will leave whisper to fine-tune. 


That is all for today!

See you tomorrow :)
