---
layout: post
title: (Day 117) Some linear algebra + eigenvector/values and transferring more posts to the new blog
categories: [math,theory]
---

## Hello :) Today is Day 117!
A quick summary of today:
* watched a couple of [3brown1blue](https://www.youtube.com/@3blue1brown)'s vides
* transferred more posts to the new blog

### Firstly, I watched this one about [abstract vector spaces](https://youtu.be/TgKwz5Ikpc8)

In abstract vector spaces we are dealing with a space that exists independently from the coordinates we are given. and the coordinates can be arbitrary depending what we choose as our basis vector.

![](https://blogger.googleusercontent.com/img/a/AVvXsEgKkFWDHRpkZbZDKaO3uIo8k-JhZwaIZNApfF3TV3ajFYFfdHRBKWpB9_vwvnzSKFf0-178VBJSbOvqUjZan6BwdPnBMVXXM-ej9Ya3jwMFfTOEeFU1_xBDNWu7UKkR96OS89kNisBIr2eZXNgbKsg13DZ_Xi7j6r4NGU-w0gMxLrKx4sn0cFQov2qIEOIj)
![](https://blogger.googleusercontent.com/img/a/AVvXsEhPSZSJFr1S352SWuMo0hdYvGUUGYEkhx1bKG2NhfilDBgrVRLj52imnXlJRQ70FOFJ_IVRuEKF66GuVxrSsfLLOXh-nWe4EMI82GyQscfhHo6a-xNhz4dt_IHHU7fNAWjisA1v90l8VElUCDzWXdNVTdJwLLJ72DKddQHiSFyra3GFbHzEj5kovSggw2dj)

Determinants and eigenvectors dont care about the coordinate system - the determinant tells us how a transformation sclaes areas, and eigenvectors are the ones that stay on their own span during a transformation. Se can change the coordinate system, and it wont affect the values of the above 2.

Functions also have vector-ish qualities. in the same we can add 2 vectors together, we can add 2 functions to get a third, resulting function.

![](https://blogger.googleusercontent.com/img/a/AVvXsEjuIeiFMwe04l-IWdg5K_c_MQ_ZmclZw2eK94GrfUOB-ZwteGRgbEnDsWXAQp5u0JvrjSBtnhzThWwgJXuqkSVQXYvcu3WFwzK-9FUFjei5805NE_7b_NWAH8S2kZjAUwTn6m9eD57WbnabsIQbtaPMY4DrSaGA2QJi93oVOtMdUPOEAj5zx7eEGsE29cHC)

It is similar to adding vectors coordinate by coordinate.

![](https://blogger.googleusercontent.com/img/a/AVvXsEgHnfLbdkfZI-pRtF-GlMyhCHiYltOgYSw5y4lebYWo20wCkygesaCIEDRVY3Y0ST95zTm0gX41nLRsZSRYHwDB8uPPQxZ7WpG0SKCJYxClIV5Z7TXUdSHuorLdUMzCqyiRQma818VLvPmJd95bEFm6T2eH1im0azbZ14NdLfX4tKbMEFAi_vILYIAmWh_S)

Similarly, we can scale a function.

![](https://blogger.googleusercontent.com/img/a/AVvXsEg5COfyg8k4ZtyL7ndYiX0ZXx5vi7ukMMdjtmMcS3lVjvwTiePAsc2TadjpPb5wee5t2VDHWKl73CHUWXFHh5hfsFnyfi4eIOjT2MZ9SSL7Wrj7W0bhVBNIPQOqCWPf5-VsCoseXytLQfaTn4s7_oGt0bN6_Vs_t076vzPRVdpdG3uemivnmOjQiVp6IkAj)

and it is the same as scaling a vector by a number

![](https://blogger.googleusercontent.com/img/a/AVvXsEjKAnae7CdsAR0wC_29UX-DFkEWCTgim7rgxZ3ZnDwZr6WeG2F74-OYCC18RgPVTeas0VeB1UP8dX21mn-3-golltrrWFuOsdibqEivndVbJJ3THkkNPTJQDuihIm61W6eAKNwNMqjIQ963zslMLRkNnVM0OyaMbzPk0gzxN_PK0fIlmEzUq08xTbH6kZTl)

What does it mean for a transformation to be linear?
it needs to satisfy 2 properties:

![](https://blogger.googleusercontent.com/img/a/AVvXsEj3Qvs13KJKZG86HCHqvlsPXFUp26unXw21vHv-F_NrvfNAB7g6uhcrw8UJ23ytlfHsPiOOIj5-1qE1cSSVXETpPTjuiztY_8y8oFFkYEdG5UsMfC-USdFyamh8_iqGNFJ149bPpkwzuRKWl9rQWP5OTAlMau8VvOddACd9c3tR8fa4Ofmtx5VG0-h9SMQ_)
![](https://blogger.googleusercontent.com/img/a/AVvXsEjGyrO02atB45cakj0TWDYzwzyJSDIR3SOHsS2x4EBmw059OI1XPs5lB9uV_88OMBE2hlYdpX2MTN22wtlGuYAM4ytEfHcVfa85FuKeZzdpdf7n7ViSlzT7pv_7Rbfh5TABCiReaWx05lLKcrZz0ioDqRJZ3l2HZh6QdVSJADdD8zWnf3CY7nFVD_LXA5nc)
![](https://blogger.googleusercontent.com/img/a/AVvXsEhKsKAceYR9l9JUV-Wbiq06owE5uuPCHuld3yTK6rAA0V6mmhwR-mC-6dXaQdIRdQEBYRu6OwndwUEF3jrhduU5TQtsMsWVKYAeurIh7Y2BnsTT6vJcl-JFuuTVhcNHBTcxnWtC3wR89K-zVcY8qXs2KGwur1knXw9asM-aDcJPadJ-QddjggHdTOkqrFay)

Every concept in linear algebra has its twin brother with a different name when it comes to functions

![](https://blogger.googleusercontent.com/img/a/AVvXsEgwl-chbsUOX21jQQcceFpE5Q6aO5fakV56fwSdJvwEXQ7E-48FOFABO6sDaqW4jAHCeBexN9l-qyAJC5kTUM6665HrdabR5waCWBht1WZ2mC838LY0OJQbrzBJ1XuGIMT0Waw8uEhhSQV-ikEb_Z--9Ki12FH4M4gKzEsE1pO9ihVj5a6qUPQGdfin5AoT)
![](https://blogger.googleusercontent.com/img/a/AVvXsEinLIoKpy_6-bhjIbTcdWsMY348i5AjCgTIxkGfNY4Sni8gIdd2_r6yHKztIR7dV_WSBQEDDOCgOwYY7gfMg7-qqvydYV6xm2d0uajwLw4ltoED6C3TSWa0x-c_veEybsXO6br9NTh0za6KX-ceGemKN6YsAMPFJppI1Qw5FF5mtTj-BY2CiWCtoZGxLsAD)

### Secondly, [eigenvectors and eigenvalues](https://youtu.be/PFDu9oVAE-g)

A while ago, during Day 47-52, when I covered KAIST's AI503 Math for AI, I got introduced to these two concepts, and I kept seeing them around, knowing how to calculate them, but never 100% sure what they meant. Today I learned ^^

Imagine a linear transformation in 2 dimensions.

![](https://blogger.googleusercontent.com/img/a/AVvXsEhi6-Ifj2aDq8gxwmoKRUd91yRPstCqzrrlKSHtSgK1RFDR4YzngLf-_tkcW2MLsfLmYcT4-ircsa4ZA0LB6GlJHrnrx_7btD-Tnb_PY8c433cJLhVXjYIBPX8KHs_lnfmBK8gzydm5D-5kiiO0R3rvOrjqQ4waz6R18ACKfxYtbVM0raa0XEA-Dr-ycQ3E)

becomes (we apply a linear transformation)

![](https://blogger.googleusercontent.com/img/a/AVvXsEhEJgjk_Fw5-tYLy-EQlVpFBDIbMiBnXglhSOlwVN1ovICuaeyyRDOzZmwTBm9KlQmZ028iuuWT_4ktH90qtprDDFnSUBOKhKPyOIfDHEpZg7eBKkhC3IGGTSVspNMqmiIhs20X89mjSH9JXZtn-cPO7glEt6wjNNggTTgk1TFiXvQyMdNRE08BRy2Ytwfp)

Most vectors will get knocked off their original span during the transformation. However there are some that just get stretched/squashed, but still remain on their span.

For example, below, the black and green vectors remain unchanged. 

![](https://blogger.googleusercontent.com/img/a/AVvXsEijlgF-FvfeXheFvuecB6Q3wxnX3VxsXrh7AjvovvoDGw3WO6r-pVCYNZjkdYsNh4XvoFVz-XkvhtVXWYJ0AwAUAQ38LjcuTtj7zmwDLzMSfMATGbyhUYh6RYxBmyOTMNvXLejvJxPvtdbKJJ-jBET-zdMUIm3DEyhb5qYGNplVLOiZyAk9GYo3Ce2g_Si_)

becomes

![](https://blogger.googleusercontent.com/img/a/AVvXsEh_9MRB2RM02Tvd9AN8N7hy2dLnChrT65agiGuernPkzJsYGUZIZkgUSN2sqVlhsyo6BCYRZh-KcNShZD_jCRakTittUTTw3J_Y5j5yGQqynKqA_MmUx-vTD3PLhMJ7NJ2K-rQoAmI3ELNATpuwGiIrkRgVQxccjTD-_OtGyGvl5NZXUMClHL-GA6SXUhDr)

These vectors are called eigenvectors of the transformation.

And the eigenvalue is the emount the vector is stretched/squashed during the transformation.

![](https://blogger.googleusercontent.com/img/a/AVvXsEiByBzUepgsiOTV5fZMpSJy-96TJTssqYvX1UHdnyIxa0l6DLTwB8-tiEh1jDkI15xRxL5HUq_lZNpa6XCPc0jqIXdfNKtlVG9cSLRm140vL0IuaOb-ngx2zFDyJz_b8LAuNnzORQVnUF5UpaxCm_GbdYvC3ZPz5m1fYoJjwstyOmUvgiHI8jDmUu2QsYeN)

More formally, the result from the multiplication of an eigenvector with the transformation matrix is the same as multiplying that eigenvector by the eigenvalue. 

Also, I found a quick trick on how to compute eigenvalues (for 2d cases). 

![](https://blogger.googleusercontent.com/img/a/AVvXsEhvXWQFXsKZXy6xKaPIzs9N519fJ-EODWjUx1rz-i8hMhO4YkL8t2nLfTosWQ4W8mtnQ8gH4Tt1mqbdCBxMSsdNexwcPq8DdDjiZ-F58GM68Yj228upg-la50dBxA4wv1eYnYVUFS2PXm5xqwxmpkFxBKU8et72hpUxuj9IclKlC3aRIJoTOknHMKK_NG2Q)

using the formula on the left, we can find the eigenvalues are: 2 +- sqrt(2^2 - -1) => 4.23606798 and -0.236067977

Finally, I managed to transfer more blogs to the new blog

It is indeed a bit tidious, but I have only a few Korean ones left, and the rest should be just copy-pastes. 



Tomorrow, I am going to Seoul for a career event, so on the bus ride there, I will probably read 1-2 research papers that I can talk about on tomorrow's post.


That is all for today!

See you tomorrow :)
