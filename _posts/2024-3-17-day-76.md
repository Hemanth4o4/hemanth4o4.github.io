---
layout: post
title: (Day 76) Learning about tokenization in LLMs
categories: [theory,nlp]
---

## Hello :) Today is Day 76!
A quick summary of today:
* Finished section 6 and 7 about multilingual retrieval-based LMs and retrieval-based LMs' challenges and opportunities ([ACL 2023](https://acl2023-retrieval-lm.github.io/))
* Covered [lecture 11](https://youtu.be/s9yyH3RPhdM?list=PL8PYTP1V4I8DZprnWryM4nR8IZl1ZXDjg) of CMU 11-711 Advanced NLP - Distillation, Quantization, and Pruning

**Section 6:**

![image](https://github.com/user-attachments/assets/2304d26b-2cf1-44ba-bfe6-0ed5aefe0d36)
![image](https://github.com/user-attachments/assets/7985c483-220c-4824-990a-b051077a2358)
![image](https://github.com/user-attachments/assets/fed31c2a-0e6f-4602-bb3d-29e874b7697e)

**Section 7:**

![image](https://github.com/user-attachments/assets/8f4677b5-df5a-40dd-b387-502160c00aba)
![image](https://github.com/user-attachments/assets/5b8f5a7b-6d9e-4ecd-859f-2b2e405e4736)
![image](https://github.com/user-attachments/assets/d6bfd1cb-b5d3-4e24-8b8c-1596283e8479)

**Lecture 11: Distillation, Quantization, and Pruning**

Problem: The best models for NLP tasks are massive. So how can we cheaply, efficiently and equitably deploy NLP systems at the expense of performance?

Answer: Model compression

Quantization - keep the model the same but reduce the number of bits
Pruning - remove parts of the model while retaining performance
Distillation - train a smaller model to imitate the larger model

Quantization - no parameters are changed, up to k bits of precision

Amongst other methods, we can use post-training quantization

![image](https://github.com/user-attachments/assets/c763ce69-2fe2-42f2-b158-ac81eaadc5f4)

We can binarize the parameters and activations 

![image](https://github.com/user-attachments/assets/5a198921-ff47-4510-8ffa-f09f619be02f)

Pruning - a number of parameters are set to zero, the rest are unchanged

There is Magnitute pruning (Han et al., 2015; See et al., 2016)

set to 0, a % of parameters with the least magnitute

![image](https://github.com/user-attachments/assets/71e3352b-6e07-486d-8ff4-c41079f4777b)

With machine translation, researchers found that we can remove almost half the params with almost none effect on downstream tasks. This is a type of unstructured pruning, because we remove params throughout the model, anywhere we see good.

Problems with unstructured pruning:

![image](https://github.com/user-attachments/assets/2770c2d5-7b04-4133-807f-60cb29ba7ae6)

we can make our model(vectors) and make it sparse, but if our hardware cannot take advantage of this sparsity, we don't gain anything - and this is currently an active research area.

A more currently useful idea is structured pruning (Xia et al, 2022).

Instead of picking params from the model, we remove entire components/layers from the model, example

![image](https://github.com/user-attachments/assets/9b1f177d-cae4-44bd-81ef-1585f04f31b5)

We can remove almost half the heads of a transformers, and we get a negligent impact on performance.

![image](https://github.com/user-attachments/assets/6f8cd012-5161-447c-b254-2cd64cb0ade2)

Distillation - train one model (the 'student') to replicate the behaviour of another large(r) model (the 'teacher'); ~all params are changed

Train the teacher

Generate soft targets - instead of using the actual(hard) labels for training, the teacher outputs soft targets which are probability distributions over the classes

Train the student using the soft targets, and it learns to mimic the behaviour of the teacher by minimizing the difference between its predictions and the soft targets

Example is DistilBERT (Sanh et al., 2019)

![image](https://github.com/user-attachments/assets/ef1a3655-5c66-49bb-bfc5-8fd8ea2d7d1e)


That is all for today!

See you tomorrow :)
