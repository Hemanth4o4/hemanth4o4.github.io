---
layout: post
title: (Day 176) Testing, Documentation, Deployment with dbt and visualisations with Looker
categories: [cloud,data-eng,applying-knowledge]
---

## Hello :) Today is Day 176!
A quick summary of today:
* finished [Module 4](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/04-analytics-engineering): analysis engineering and using dbt from the data engineering zoomcamp

A preview of what I created in the end

![](https://blogger.googleusercontent.com/img/a/AVvXsEg-EklHUAERxDrqjoIea4GZHIjuzO15JEHxT2-obQe62p3UJoN1n1A2q_Jb78-WqvRbscXU2SdJAEgGrc7aqEaHZb4b6O5tFTr4hHxf4bAeXRaT5_AVjxOqQ_lYaDpp6-kudeDP4bhzvSNuz6QEI-aVm3yl_rycN79absognjQl4Vz-kAeBARV2JIRf_PKt)

Continuing from yesterday with dbt ~

First I learned about testing and documenting dbt models

We need to make sure the data we deliver to the end user is correct, and how do we make sure that we are not building our dbt models on top of incorrect data?

dbt tests

* assumptions that we make about our data
* tests in dbt are essentially a select sql query
* these assumptions get compliled to sql that returns the amount of failing records
* tests are defined on a column in the .yml file
* dbt provides basic tests to check if the column values are:
  * unique, not null, accepted values, a foreign key to another table
* we can create custom tests as queries

Before writing tests, to ensure our data's schema is correct we can autogenerate it using a package

First, include the package in packages.yml (and run dbt deps to install it)

![](https://blogger.googleusercontent.com/img/a/AVvXsEje0mDWA2F4gCMosBHYl3YKrwMRkWwtxTZkYZuBLRvAh6g3QwDRQYy7cA9CVPVwQfNy1pCBTYlO2z51LG5o2J_1xMFAwPjju5XDVc_PmoPdJnGHHUCiAvnHNiE3ZUEkMF5quEb7M3TIONl1QoEIU0BHhnbeuD93G7vfpcDU7TIRlfeoorHkxpZLXMI0W5Yb)

Then, we can use a macro the auto-generate schemas for our data

![](https://blogger.googleusercontent.com/img/a/AVvXsEisqg1LUYxJhgfT4FZCX0xf3w1ylV-E0cvXXyl0QD9VVzZB_4WgW4Kv_r1hPIJ3PaBaUcPeozYsYlDnhMzPNUQHnxKk6nMMONGzj863EvMNzikm-Q8AvygL9VLKsO8dFh47LlWK10jxLeNCIGyToqoSs4KHu_JMjRyMG6hTnS5eZViypcZfHBXH3jvPUYiU)

And when we compile the above code, we get (image is truncated)

![](https://blogger.googleusercontent.com/img/a/AVvXsEgkgoaUelmz6xSuBOy3Dzg8azztMAlpMnqKnHQZRU3MTkirOwzrXZ0HTwBbyMWb2DZVzknEn4Dz-PQZ2ubSwmsdjfWmR-vgrk4BrLI_48ZIgPgnbFk1IgbNmIaX-LacajYAveS01bQay0N7Vnk60FuRr0BPpxpGtVZG4F0MCOW0UuvUfQ-DC9U5f_aDmqYm)

Which we add to the schema.yml which keeps info about our data.
These schema.yml files are not needed to run the models, but are recommended because it serves as documentation.

![](https://blogger.googleusercontent.com/img/a/AVvXsEjZqifB36qoXIr-TTQIhNES2g1nkoUcEOSv7vL2rFQEn4cjjto1HHilWvg1CeKskN1lwlD_XHnwCxAjn0t4-Q2Vw8A3KhYzSs6OfYHAeo9U4AECabc32p1ol5R3-V76CgSCte8Hr9JGaoT33P71PBH0GD7eLl-Nd1ff6EPIAGUq-TcMVb9UBIwtkfGpsgsI)

Now, adding tests is fairly straighforward

For example, we can add tests to check that the column tripid is unique and not null

![](https://blogger.googleusercontent.com/img/a/AVvXsEgXHl6C8gZa0DkfZ0gXk7-8xIOrjAe4d1hdkncer0y1CTvbz5u34-DOgDq2YHKqG01BbEKmTCfm3Gp5N-PihJ0Az2iS7Aw8sBeNJHlskNHlE_nJHE3ptFbQWjsqQtupFW_jkBAkmYFsOtFtbiMf73oBqbYoadqZGgkKkuePp6_qlMZaqR-b6FffLx9j1U96)

In addition to severity, we can add thresholds as well. 
Another test to check that pickup_locationid and dropoff_locationid are within the existing locationids in a reference table

![](https://blogger.googleusercontent.com/img/a/AVvXsEgL2yGNYl8y1ggZG-D2cwYTPh_pM4e97A5ptnOikmiEpHhjqCY15Gg-elhJX_-23xzZirwZ_X4LUTfiwwDr9SNhfAFG0r3dU3NKsSeVxRHDA8WB8XLCjlNWrQjbPCoFlnb1RB9cvrrQOUovkz2C-s5gepb9Tzbj77drdNYfT1svb4rIgf0tqlmugAcXkFb_)

Also added a test to check that payment_type is within certain values

![](https://blogger.googleusercontent.com/img/a/AVvXsEj5juKrIlf_-ySyGNaq0gJtLsbby2hqsRHr86fDnESCgmiuutaLBDLQSzYdJmqRzj1jJgxjeB0ftrVTUyZ9BcAfYMKdAPYAaXJOn1V7q9FAEWH9ZsIiLZXbzhspjy8v67pnjZ_CipVH_XdPykJ-uPWWwkFALuiz0_PM6sajU8u7iA9Hw92nRwISSdYszfwP)

We use a variable here, which is defined in the dbt_project.yml and is a list of [1,2,3,4,5,6]. It is just an example of how we can use variables, because the same check needs to be done for green_tripdata (the above tests are for columns in the yellow_tripdata table). 

And when we run dbt build now

### Next, learning about documentation

If we check the pics from above, there is a description: `""` field, which is part of dbt documentation.

dbt provides a way to generate documentation for our dbt project and render it as a website. The documentation includes:

- Information about the project
- Model code
- Model dependencies
- Sources
- Auto-generated DAG from the `ref` and `source` macros
- Descriptions (from `.yml` file) and tests
- Information about our data warehouse (`information_schema`)
- Column names and data types
- Table statistics like size and rows

dbt docs can also be hosted in dbt Cloud.

Once we are happy with the descriptions in our schema.yml files

for example:

![](https://blogger.googleusercontent.com/img/a/AVvXsEg2q9CogcsW6ON6BwJbokX1ryLOJL72hB5Orn3gSsEWDLUZmZmAFtN6nso4mO5K18rt47TngyHNPRr1YIaKpYWGkV3vzLdvVybcPFmTqba8OpQwEiwn3gaAgw_uIH5ouFDcEW4fZl3LK-fniVmijSKYEkenHv9uNpYZD_cC57AjiPdC2CCPxcm9IliTd7o-)

We can run `dbt docs generate`

And we get a docs page

![](https://blogger.googleusercontent.com/img/a/AVvXsEg56MG2P7NCazuq5-ASGj_huGSPjjzRcveY3UPeg5ENrCkg_2OlSnPWukRyrxX4Gz3UE-RGsG6_xZ3pOxSZ9oYETgvZCq5y-8Jlkp_bnssx2lfkQlTppvam6UqRce95Lyxm8J8LyQlwUgsT16lyJ46yT251S-Cux32ywzqg33SZOEbh9dEPD268qmDzbJxB)

The structure of folders is the same

![](https://blogger.googleusercontent.com/img/a/AVvXsEhk18NR1BWmJKP5PMh_y2tXp4MKGkgjLzMdklsxQmj57hNhK56s2yjn92PUtpd0As3WTOt5nl1L_frEFsFRiDp5YjC0rts7Zd6_YWMHf4ajfQwaEvL9ZV6PoHV4sutJJmXDnfRF-cbS71txA3GrjLdmCvJ3mTAoTrn-e0hXRcL3hhL1YCfKBcQSiUG4dbsr)

And for example in dim_zones we can see

![](https://blogger.googleusercontent.com/img/a/AVvXsEhcPCfqAWAzbhLZyirUJtzcbBnltc7AivxH4F9F9Uxzd7EOgKxFI3JkB64FIuioaIGO-Nv_rcTD1y_m3xbenK97bymuR1L2VvBGeJf43Ni-rTbli3_yVpFmP9XY3UH7bedYZ14Pcj-ih_wa93mt-Ci00iEvO0DvayCiQhNG7MrrLcPkYr9zgs9Z3tOJEJzv)

There is a database tab where we can look at documentation related to data in BigQuery

![](https://blogger.googleusercontent.com/img/a/AVvXsEiTHUIFgRdslVTs8mvQ6OJ8w0hnqRGUQVl3rYF7sbDr8TebmSMhfOHso1JOJr2pYsbZ84_zE1y60aMxRsx3Izdzk2YwMfeFWBdzot3Q3U5x_4II6RVy3M_zdhyaJB6lmZqqiKgOf3hS-uxHZW7aaV2gkrRx8lke4S06bar3fwQdDK_YTGHmXfb5MZaVEPNo)

Also in the bottom right corner there is a button that shows us the lineage of our project

![](https://blogger.googleusercontent.com/img/a/AVvXsEioEFWANFc2M00nPvZM8N32UEDeHWw8VsF4xY9niU-s6ieOIISLRHJsFa5lQSSJt6Y-xGtAvj-3rV9UkdZ78iVwlC2at2MDVRn-mrq7YqdOW10qNmqjDVCfHfZ5GpZa0YzNKEUtN-L1RvTjuf0P4ezd6IHnHLaqF6paJD-qz2Sm4j69idOQvCAvtJLLEdeJ)

### Next, learning about deployment using dbt cloud

To deploy we need to create a PROD environment

![](https://blogger.googleusercontent.com/img/a/AVvXsEgVIuMvERJHSi-YxdGaKn2-aTBqScUA0a0slCjPXPwmC28lWrLxD7pnQxmtWLjEhJKXqlPwp08eRlUEUiZCL-1Ymaiz1CeTaKCejZly9HeNdwLJT5qxkGCMDpz1MooBRhc0B13f4mlvZ901YJ8BEUdxT6gTi3FpHtbKUgPhUP92ramzLaHaEHYP9xDmQYM2)

In there, we can create jobs

![](https://blogger.googleusercontent.com/img/a/AVvXsEggtQg0rkauraPxRPhTjwqGf-VEMtzNEraQriVSbiVm3WWgMC1i6V-8wfDWdJiE9zHCp-TzZxRuOziaAYV68ZYnqg7wqKUB9GLfO20TtSxSJJov38sPMEZFegoP1UlIzXEPrlsj6nTqGoX3dxE_3aZ0a9982dbk71FuRl0QdZ0c59vD84QqmCx6Sg8Y36x9)

![](https://blogger.googleusercontent.com/img/a/AVvXsEjS3jDvtqjBRlyOT-5MM8ziI2S_9_zND9j4G0HfrZhV_Vh3GvyL2-nMr-1kQseXGyr_I3ttqCKooMfiisF3vozZ4iJvq7QRPhwrWPM6IzeNyesbTcssejXy3sFBI9cZCvXz7BbzGaV5etWEHlELhi9yKFMg9vZh9ZLH--Ic3roIUeybvSgi4Q7bp8ihLnPr)

Where we have a Job settings section

![](https://blogger.googleusercontent.com/img/a/AVvXsEgiHzBNYUMz-6nAHv6O1Wq6oaCmYomvfNT_ftGfrvfoNNX8fef0S6zqJfIuWC_C9fzC3zHUJPJqaT0dAD_1ibtfPFV3aEUE6Wv9E6gPlXDdtkr1a-mMp8mJes8f1AXVaFuLMMNcC55ojDmxAzdSfK6MFiDeB5YP17HVM-aY8F2CqlLPXxvvXs_SjrRrfPfX)

Execution settings section

![](https://blogger.googleusercontent.com/img/a/AVvXsEhqDlhrzkErqnJTWI-A5Mk8VVmw_yORKVUB6OJOMuyPOLRR7yYbL3ng2vV3taSotQtmdceGambU9avu4l2py42thEOzimuVA7ozjVF3p_m_jv6oK0BNBNf305yaDCdQp7rDIpL4TuU_ay6TLldowAqifJlG7YtsEeA55fkuEY3Y2J0rxowiAuPktUNRhzQv)

Triggers

![](https://blogger.googleusercontent.com/img/a/AVvXsEhJZD2jr4LX846-e79F9w25L-VWvWc4nz4y0ow-gTD2OlDaOfHN3woDGU2Unwq7xU8N6LXx2TKnPllVXAgdvEwimksFicA0AYHFVAmqTMJxtkP6Z7YBR44ymukr__H30vm_4XOU7RNf3-lhKQJ25qMCbsBAUrUA7I1nvL9AsJ1sQ-Gqv93cjafWHgWHJ8bM)

And advanced settings section

![](https://blogger.googleusercontent.com/img/a/AVvXsEgZy-zJaSmBxWBUkJiFglWsprGwM5UYL9ABYV4h1hmr0imfXzP8nbWs4jawZiQD_hyr-ITNLM4qkCc50K82ShWPB9BtZ4Wwqd3rBZbCFe71s0Xm8goZGKn6ns6l_mhfED_7JUtX4OoaFRNyvP8syEsltUD3ICIZEIOi7cfuqBKJvncBbKB_AduzE7pA_Rh9)

Once created, we see: (there is a next run because I set it to run automatically every friday midday UTC)

![](https://blogger.googleusercontent.com/img/a/AVvXsEhM6SKmDvRvELdAW-ldL_74kVoFt6L_CW597w9WNOKhzz3uxnAg2MyT8qKK49kCoYlF3NK57ElminwXpNnA1uon-v73QDkeSFi5j666leSSbnrLx8etSvAFnGvtCfhOTrWenmp4yzEsdBgdiOCxaeYslRJOp5tq_1lbMNKTAwOaispZ-ARIVPe02d8yM0hm)

And we can trigger a run via API as well.
I pressed Run now and I see a new run:

![](https://blogger.googleusercontent.com/img/a/AVvXsEgTuwQEr83WgDOxt4N7QsDqkyLuy56BPtK4N7VAYONq9D37XNsIyed3RK3A3IxmVI7_B1_mnQASBuDkNrcqkFRMM1an81vqqwMPdI-DrayDrDRxKTzDoDn_utCWNZb96zqidQxb5FTss_ux2BJQenwfXdQXfWtIN1rhabI7VKzVMiCLv3uyR3TChXqS1fjN)

We can see details by clicking on it (the warning is about a deprecated functionality)

![](https://blogger.googleusercontent.com/img/a/AVvXsEjmfhK7a2TQgJWxewnnjNKE-tBT6StlCAcTgkYvHctwmU1aIF_s4YkcfhPsWTFN7-jU0Ts2yCGGnFPZs_7U7V1vIGvRJjnQpcCgqSiO99Hwo79lp1c8692sWpzqr2Zr_83C9u3VeW1XxRLtEd54VKfoijusi9AQpiyrJHcKhzjxkJjTy0OdRNDl57yCiSlp)

This also generates a hosted documentation.

Going back to creating jobs ~ we can also create CI jobs
There is a git trigger on pull request

![](https://blogger.googleusercontent.com/img/a/AVvXsEhfwSXIkZTzhRxCl6B8MKilAY881e7fj2Lj6a6_nuHPskmnZjozNJkGrNxVg9FUDT3jHpcQ7c_PUpEQPXKTsVJ6X_m2uATBR52wz3_Ua8OjLtJdbrdcGtDNMU6YGEp8L7ybWbzIQ-x_ck2CUKHNih5U92R5rJkH5yvZhAqTccD8bnlMkstoJLKxxU8agR2m)

There is an option to list commands where we can run whatever we want. And also a field to select with which env to compare the run

![](https://blogger.googleusercontent.com/img/a/AVvXsEg1VgRJ1yCR_XxJpoSNAIwwn6BllaXc72_qIJiX9_HBrrIT2YlLXxClR6sp0r8VyYrTVQJaoyoTOAnR4dyI2bkI7yOU1l_q_A9pMjlzcaa6vUEfqW4mFtTA52uMzF-9ya8ho9xsfrudX55bEnVqvUAIcRjAVYhYfKjWEx6F7FKnww512Wd7TG6ArapfQ4mu)

### A final task - visualising data with Google Data Studio

Create a new report, make a connection to BigQuery and we can see the fields of the selected table

![](https://blogger.googleusercontent.com/img/a/AVvXsEg9gBjrnmwvE9b6nU3zLowOjbfV-DVY6oTb98BK8Dp3pWQOJeAhbb2bPi0810wDB1JBHrPLWjhV48xRj85b3QAMSzzns_do61BK-2jboatik2HIK4S0XAAroI3u06odJm5RQ5W0ZbyVAyveaKQ0UaSf3b8Wx0iXM7pzb3KrTPhvxl6kkgvYISbUC3qaQIl1)

After some playing around on Looker ~

![](https://blogger.googleusercontent.com/img/a/AVvXsEhsq1dI6CF7xj5HX7XKX2KU9Ls3RwSLkEfUglg1M98NabPAne1cUZIkVEULE6NV7v2P-m1HHVnMxnE1GDF2mcnLlPJ2kJuOG0vaO70dFZlIrx2cZC-4Ve1jPu0z5HFfHusYe3qN59aY3cJ7QNX75N3v6i2ruW-vdHJSvsh1DgE-W58E14u9HtNceCXjm_Yo)

All the code from today as in dbt + a pdf of the visualisation is on my [repo](https://github.com/divakaivan/data-eng-camp/tree/main/Module_4).


That is all for today!

See you tomorrow :)
