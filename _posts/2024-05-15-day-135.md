---
layout: post
title: (Day 135) Going deeper into MLOps
categories: [mlops,applying-knowledge,theory]
---

## Hello :) Today is Day 135!
A quick summary of today:
* covered [module 2](https://github.com/DataTalksClub/mlops-zoomcamp) of the mlops-zoomcamp by DataTalks club about experiment tracking and model management
* cut 2 more videos for the Scottish dataset project

### Firstly, about using mlflow for MLOps

Maybe this is because I am starting to learn about MLOps for the 1st time and I don't know other tools, but WOW mlflow is amazing. Below are my notes from the module 2 lectures. Full code on my [github](https://github.com/divakaivan/MLOps-camp/tree/main/Module%202) repo.

First, some important concepts
* ML experiment: the process of building an ML model
* experiment run: each trial in an ML experiment
* run artifact: any file that is associated with an ML run
* experiment metadata

What's experiment tracking?

* the process of keeping track of all the relevant info from an ML experiment (could include source code, environment, data, model, hyperparams, metrics, other - these can vary)

Why is experiment tracking important?

* reproducability
* organization
* optimization

Why can't we just use an excel spreadsheet?

* error prone
* no standard format
* visibility and collaboration

_MLflow enters the stage_

![image](https://github.com/user-attachments/assets/09d776ec-9a73-4a19-9c0d-91fb1abd832b)
![image](https://github.com/user-attachments/assets/590e6179-c361-41c4-9c8d-09a9fa4d957c)

We can spin up mlflow with: mlflow ui --backend-store-uri sqlite:///mlflow.db

And the sqlite db will keep artifacts and metadata inside of it. 
To create tracking:

![image](https://github.com/user-attachments/assets/03206a8a-6b0f-4f5c-af14-0cf0186fa8f2)
![image](https://github.com/user-attachments/assets/a84360c8-323a-438d-9f5d-8cf352bbd974)

With the above we can log a run, its params, metric, more info. 
In the UI, we see

![image](https://github.com/user-attachments/assets/163b2de3-5760-4ac2-9f2f-ec47666adc30)
![image](https://github.com/user-attachments/assets/c44482d7-cf6d-4aeb-b192-2f214ce57230)

Run another model with alpha=0.1, and we can compare the two

![image](https://github.com/user-attachments/assets/f1b09289-f217-4d15-8f12-e4dbf55d5fab)

Next, doing hyperparam optimization using xgboost and pyperopt

![image](https://github.com/user-attachments/assets/3384c32b-3d90-423e-b7c6-a1da9b6fd3db)

we need to define an objective

![image](https://github.com/user-attachments/assets/b239e87a-0c97-4e3e-8474-a3c937e884e8)

Search space and then run

![image](https://github.com/user-attachments/assets/e3f6e4e4-be84-4426-a4bd-6a7c823e1046)

After this ran (for around 30 mins), in mlflowsâ€™ ui we can visualise all the runs that use the xgboost model

![image](https://github.com/user-attachments/assets/c306e7ed-dcb4-426b-808f-5b84fd1b79c6)

it also shows which hyperparam combinations lead to which results

![image](https://github.com/user-attachments/assets/f9ef6908-4302-4e60-a6b6-23ac9dee7ef2)

For example if we see a scatter between rmse and min_child_weight we ca see somewhat of a pattern where smaller min_child_weight results in smaller rmse

![image](https://github.com/user-attachments/assets/117e35c2-a7f4-480e-affb-6ad7f79aa048)

we can also see a countour plot where we can see 2 hyperparams and their effect on the rmse

How do we go about model selection?

A naive way would be to choose the model with lowest rmse

![image](https://github.com/user-attachments/assets/b0620d28-96e8-4db7-a9c8-f83f7068f7f3)

and after clicking on it we can check more info about it.

![image](https://github.com/user-attachments/assets/316d21f4-0749-4134-a0ed-08a9412394cb)

And we can make further judgements like run duration, data used, etc.
Instead of writing lines of what to log, we can use autolog, and when we run the model with the best params from the hyperparam search

![image](https://github.com/user-attachments/assets/443eb924-2d72-41f2-99d1-bcf90634a611)

we get a bit info:

![image](https://github.com/user-attachments/assets/b0f4d7fc-3256-4bce-92b8-82dec8de3875)
![image](https://github.com/user-attachments/assets/0f3e7b79-e7c7-43f5-b9af-5f807093002b)
![image](https://github.com/user-attachments/assets/3294b909-9e57-4c2a-a823-db52450234de)

In model metrics:

![image](https://github.com/user-attachments/assets/995c6c2a-cfb1-4e2a-b990-f3ad5b64d333)

We have more content in the â€˜Artifactsâ€™ tab as well

![image](https://github.com/user-attachments/assets/0b5ff4a8-8e96-499e-9996-d3d98a26b16b)

Like model info, dependencies, how to run the model

![image](https://github.com/user-attachments/assets/99cf93ec-a38f-402b-9402-ddaec087260e)

Next, I learned about model management

![image](https://github.com/user-attachments/assets/4847b322-ba76-4c49-a3ab-1b38b1074c34)

What's wrong with using a folder system?

* error prone
* no versioning
* no model lineage

To begin with, we can use to save the model in the artifacts tab

![image](https://github.com/user-attachments/assets/3817e9d4-94f0-4e21-b495-17db80dba674)

And when we run the code again, we get

![image](https://github.com/user-attachments/assets/38d481e2-845c-46a7-9ccc-8369f5ef717d)

Instead of just getting the bin file, we can get better logging using

![image](https://github.com/user-attachments/assets/5392f373-8bd2-4deb-b5c4-4603ca635780)

Which results in

![image](https://github.com/user-attachments/assets/16895f8c-2eed-42da-9938-1810eca6827a)

We can also log the DictVectorizer used in the data preprocessing step, and get it from mlflow

![image](https://github.com/user-attachments/assets/34ea8953-f5fc-4294-82dd-4e6042d65116)
![image](https://github.com/user-attachments/assets/536c929b-e4ca-431d-98b5-a4be96060cae)

Using the saved model for predictions mlflow provides this sample code when a model is logged in the artifact tab

![image](https://github.com/user-attachments/assets/3255932f-d6ee-432f-85ed-33e972f80a91)

If we use the sample pandas code to load the model, we can check out the model

![image](https://github.com/user-attachments/assets/73d56c87-aba0-4cad-892d-81f2ca22c2eb)

Since it is a xgboost model, we can load it with mlflowâ€™s xgboost and then use it as normal

![image](https://github.com/user-attachments/assets/00bc7504-8c19-4b63-a35e-b91509bdc821)

### Next, model registry

Imagine a scenario where a DS sends you a developed model and asks for me to deploy it. My task is to take it and run it in prod. But before I do it, I should ask what is different, and what is the purpose. What are the needed dependencies and their versions which are needed for the modelâ€™s environment 

![image](https://github.com/user-attachments/assets/b624fb6f-fde5-45e5-aa3b-8f60b0a901e3)

This email is not very informative, so I might start to look through my emails for the previous prod version. If I am lucky enough someone that was in my place before me or a DS, has put the info of the current prod version in a tool for experiment tracking (like mlflow).

![image](https://github.com/user-attachments/assets/ef66c815-4529-4068-a8b3-aff822421fce)

The pictures above with lots of model runs is the tracking server. A model registry lists models that are in different stages, production and archive. To do actualy deployment, we need some CI/CD. When analysing which model is best for production, we can consider model metrics, size, run time (complexity), data. To promote a model to the model registry, we can just click on the â€˜register modelâ€™ button in a modelâ€™s artifact tab.

![image](https://github.com/user-attachments/assets/ee377027-601d-4b4f-ada2-323ed4f87b9b)

We can go to the models tab now in mlflow

![image](https://github.com/user-attachments/assets/a3f22094-3bb7-437f-8b1f-cdc4d41daaa3)

and see the registered models (I ran and registered a 2nd model)

![image](https://github.com/user-attachments/assets/39fd6a98-115d-47fd-b069-8f42b737c2d1)

In the newest version of mlflow, they have removed the staging/deployed/archived flags, and it is more flexible, so I assigned champion and challenger (custom) aliases

![image](https://github.com/user-attachments/assets/12062937-add7-42fc-a05e-e24d1d7be64a)

Now the deployment engineer (assuming we understand the meaning of the aliases) can decide what to do. All of the above actions can be done through code as well, no need to use the UI (I learned some of the functions and used them in the github notebook and using the mlflow client docs)

Next, I learned about mlflow in practice

![image](https://github.com/user-attachments/assets/3c04d71d-08aa-4aff-8585-ac96f92db5f4)

1 - no need to use mlflow, saving locally is fine

2 - running mlflow locally would be enough. Using model registry would be a good idea to manage the lifecycle of the models. But it is not clear if we can just run it locally or need a host.

3 - MLflow needed. We need a remote tracking server. Also model registry is important because there might be different ppl with different tasks.

![image](https://github.com/user-attachments/assets/6853f9f1-2c16-4b62-94ee-c812aacfd1e8)

For the 3rd scenario I wanted to follow along and create an AWS instance, but I donâ€™t have an account and for some reason my card keeps getting rejected. So I guess I will have to do it some other time ðŸ˜•

![image](https://github.com/user-attachments/assets/87ec114b-a520-4584-82ae-0de84e2e3158)
![image](https://github.com/user-attachments/assets/5adb4202-7b5b-461e-bfec-265a76170507)
![image](https://github.com/user-attachments/assets/6cc435f5-a2fa-4f5d-a3b4-8e4bd3a172e0)

[This website](https://neptune.ai/blog/mlops-tools-platforms-landscape) shows comparison between mlflow and paid/free alternatives.


Secondly, about the Scottish dataset

Today I finished the last 2 videos from my collaborator

![image](https://github.com/user-attachments/assets/3c23565c-5c93-47e6-ae9f-f6b735016b8f)

And now the total audio length in our dataset from all 10 videos is 770 seconds. I need to find a better way to store our audio files and the dataset itself. It all lives on my google drive at the moment. 

I also added some other metadata for each clip

![image](https://github.com/user-attachments/assets/88643658-d1f6-450b-9bad-2a805e6125aa)

And this is a describe of the length_seconds column

![image](https://github.com/user-attachments/assets/8f03ffa7-9a8c-486c-96d2-3c9992887fc0)

Tomorrow and Friday is [AWS Summit Seoul 2024](https://aws.amazon.com/ko/events/summits/seoul/), and there are a lot of panels each day (2 days in total). So I will share my experience and attended panels. 



That is all for today!

See you tomorrow :)
