---
layout: post
title: (Day 175) Learning about and using dbt cloud
categories: [data-eng,cloud,applying-knowledge]
---

## Hello :) Today is Day 175!
A quick summary of today:
* basic SQL in BigQuery
* continued [Module 4](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/04-analytics-engineering): analytics engineering and dbt from the data eng camp and learned about dbt

There were some bits from Module 3 that I did not finish. About using BigQuery to create a partitioned and clustered dataset and see the impact it has on processed data.

Using the infamous nyc taxi dataset, I executed some simple queries on creating external datasets in BigQuery and could see the effect of partitioning and clustering

First create a non-partitioned and partitioned table in BigQuery

![image](https://github.com/user-attachments/assets/22f2f4a9-4b3b-4eee-92aa-6d87eb61cf69)

The dataset is by dates so we are using one of the datetime columns to partition by it. Below we can see a significant drop in processed data when doing a simple WHERE between dates on the non-partitioned vs partitioned tables.

I faced an issue here because when creating the partitioned data, in BigQuery there is a table details section where it says 0 partitions. And I was really confused because the data I was partitioning is 10GB and I saw that for other people it says there are partitions there. But for me it says 0. I kept rerunning and changing/uploading more data.

![image](https://github.com/user-attachments/assets/58f428e7-d861-4849-b8a0-20629b0a43ed)

But it turns out that even though it says 0 partitions in the Number of partitions part, my dataset was indeed partitioned and the results were clear (as in the pic)

![image](https://github.com/user-attachments/assets/4f4b4e4e-ea53-424f-b176-008cc519aaca)

And finally a table that is both partitoned and clustered, and there is a decreased in processed MB of data in the two same queries as well.

![image](https://github.com/user-attachments/assets/28d215a0-f780-478d-963d-d44d236e81e7)

### Now onto Module 4

First I learned about ETL vs ELT

![image](https://github.com/user-attachments/assets/66773554-e3f8-4964-8de1-7aad450f1ca2)

Extract-Transform-Load:
* slightly more stable and compliant data analysis
* higher storage and compute costs

Extract-Load-Transform
* faster and more flexible data analysis
* lower cost and lower maintenance

Then - Kimball's dimensional modelling

Objective

* deliver data understandable to the business users
* deliver fast query performance

Approach
* prioritise user understandability and query performance over non redundant data (3NF)

Other approaches:
* Bill Inmon
* Data vault

Elements of Dimensional Modelling

Facts tables
* measurements, metrics or facts
* corresponds to a business process
* 'verbs'

Dimensions tables
* corresponds to a business entity
* provides context to a business process
* 'nouns'

Architecture of Dimensional Modelling

Stage area
* contains raw data
* not meant to be exposed to everyone

Processing area
* from raw data to data models
* focuses on efficiency
* ensuring standards

Presentation area
* final presentation of the data
* exposure to business stakeholders

**What is dbt?**

![image](https://github.com/user-attachments/assets/408e3001-48bc-449d-8c32-b46770fe1202)

It is a transformation workflow that allows anyone that knows SQL to deploy analytics code following software engineering best practices like modularity, portability, CI/CD, and documentation.

Workflow we will use:

![image](https://github.com/user-attachments/assets/b42e9ebd-ead2-4d20-8ddf-ec65c31fc30d)

Creating a new dbt project

This took some time to figure out creating right folders and access to my repo, but in the end:

![image](https://github.com/user-attachments/assets/a41361f8-2a4b-410a-9502-a8b211e07ab6)

When creating a new project there are sample dbt data models (with its lineage)

![image](https://github.com/user-attachments/assets/2b4c24c9-a249-4c9f-8fb7-f85c254853dd)

Anatomy of a dbt model

![image](https://github.com/user-attachments/assets/5908db88-13af-4bd9-8bee-f559b4d3803c)

Materialisations in dbt cloud

* Ephermal - temporary and exist only for a duration of a single dbt run
* View - virtual tables created by dbt that can be queried like regular tables
* Table - physical representations of data that are created and stored in the database
* Incremental - powerful feature of dbt that allow for efficient updates to existing tables, reducing the need for full data refreshes

In dbt we will select FROM two types

Sources
* the data loaded to our datawarehouse that we use as sources for our models
* configuration defined in the yml files in the models folder
* used with the source macro that will resolve the name to the right schema, plus build the dependencies automatically
source freshness can be defined and tested

Seeds
* CSV files stored in our repo under the seed folder
* benefits of version controlling
* equivalent to a copy command
* recommended for data that does not change frequently (and is not huge)
* runs with `dbt seed -s file_name`

Ref
* macro to reference the underlying tables and views that were building the data warehouse
* run the same code in any environment, it will resolve the correct schema for us
* dependencies are built automatically

Now onto creating my 1st model

In the models folder, create a new folder and in it a .yml file

![image](https://github.com/user-attachments/assets/166efaf6-6be9-4ed8-b454-967ccb3d2b9b)

The names in this yml file come from my BigQuery dataset

![image](https://github.com/user-attachments/assets/b7021926-469c-4f77-83d8-1dcaf3fa36c9)

By clicking on 'Generate model' above the -name: green_tripdata, we get

![image](https://github.com/user-attachments/assets/a8c0fce7-9294-4c4d-b4d0-01390c43acbc)

To run this, we use `dbt build` and dbt executes the models in order of the folders. Then, the below is seen: (ignoring the failed test)

![image](https://github.com/user-attachments/assets/41e6131c-d278-476b-845c-fa4536281749)

Next, I learned about Macros
* use control structures (i.e. if statements and for loops) in SQL
* use env vars in our dbt project for production deployments
* operate on the results of one query to generate another query
* abstract snippets of SQL into reusable macros - these are analogous to functions in most programming languages

Creating a macro (reminds me of excel VBA in high school with the macro syntax)

![image](https://github.com/user-attachments/assets/e9ddcd8f-2986-4613-8191-7339405ba3cd)

Then we can use it as a normal function in our models

![](https://blogger.googleusercontent.com/img/a/AVvXsEiMjQ7fRvGC8ca8dWx2FfIKJ4RnjWVJAzS5FRwJIsn-nzCOyzyc2p-KSINnc2jyyUYJP0weJQ4edmBFpXqDB9dl_Qk-gsf7MIXy6xwdhseasfZPq7Dg__gEMo5AuV8zd6_KZGUBZ8DcFfHQe180B3WqJUzLLZJZkkoeDGr5Ql0VI158QsG3vnw9pVhkBL_E)

Then, I learned about packages
* like libraries in other programming languages
* standalone dbt projects, with models and macros that tackle a specific problem area
* by adding a package to a project, the package's models and macros will become part of that project
* imported in the packages.yml file and imported by running `dbt deps`

We can add packages to packages.yml file which lives on the same level as our dbt_project.yml

![](https://blogger.googleusercontent.com/img/a/AVvXsEhu0gGvXpM6AseZN0TGhGEu4fWh7dlrb_HQ5Wy0oHJ4Gnzm8e15mb0X1Ai86bm8FOFp7u-xG9QZ6Afi__r_-PjW3GFH5I9WGvMawOne9ZVEmIrX8b6SjS9OVXtUZMcsbHbrTrG3iJwcqFA5Quvq6nHLZjLLZdqQLZHB4uFqwF53gTT7aZAilDFRzHvzYljx)

Then run with `dbt deps`

![](https://blogger.googleusercontent.com/img/a/AVvXsEjOhwh-xo0ZWMb5kwAyedo9hnidbRIiiEWAK61ZC18h8EM9Um_mOf5RMQ6vQlyUdmg7h9XWkTrhk_o7oCXy5VTzJfAQBW5KL9A1bwsczO4ZV4Gsbkp7ajADvefPyM9ZQ45rJDTFJnCwPLjkPI4nSezH6oE-bR9tDQYRIs0Kg2ZWvJ4WdR74UqDa5uqiPhL1)

Then we can use functions from these packages in our data models.

Once we run models, we can also see their results in BigQuery as well:

![](https://blogger.googleusercontent.com/img/a/AVvXsEg3JAG-9WEupOQ9vVChbksXoVSu5PcCnZ_OQaAEZsB7sy0H8kVPJHER9wdHWQYNMSCmj4QdG8mzbmbYoo0zeoqrDfc32lAyocw6hqbAVtTWFOjrRsvgzMAEqe1GJvhKMwNDFwx0BQf4x5HShyKiqlViDjEhgyGlWhHwKfuUZ5J7IbpVNv2EAzm4dnSiOuYt)

Then, I learned about variables
* useful for defining values that should be used across the project
* with a macro, dbt allows us to provide data to models for compilation
* to use a variable, we use the {{ var('...') }} function
* variables can be defined in two ways:
  * in the dbt_project.yml, or
  * in the command line

Example:

![](https://blogger.googleusercontent.com/img/a/AVvXsEje8YkZujzWgPsKv5WrmEm6xvOch8c1xY-5TAeOy0zxkx0guSlLxkm7t5aVkOJch4NsGEc8HqP-m_7mlrPGIHt5ZyQ_fbMClRsvJ_8MYZxOE5wtqIivY4zo7ET3yLWakPEdcKHtDtEI869PuSgyAxPjKytff6_wUgGm9IsPces6hQbmqs742uhuHhgMiGAJ)

When we do dbt build this_model and we provide the var is_test_run, there will be a limit 100 added. 

Next, after adding a model for the yellow_tripdata, the project's data lineage looks like:

![](https://blogger.googleusercontent.com/img/a/AVvXsEhsARm9rnaJrmm2FUNK36tGkbWxML65KZDZycFo2udoMbc3Emndo0kQMe2C_Pa2pXOkLwGM5_5uWmssOre9sK_4ZhCeKgtP4Lf1ELkkjpdtXHESS-cXADkklvXzf0j0bSBxkhhfXtuydkdyuCZObgsbcXwKGy2d6A727il0YkiwqtybxpdlbBe9NLkTbKZe)

After adding a seed data, and creating a model for it as well, we can combine all and get something that looks a bit more modular

![](https://blogger.googleusercontent.com/img/a/AVvXsEjjUD83UOvNF3C7yotQZhkYmkxcDp5X9NMjwPxOc57xTO93xUrur6KKT4Ef9OCKTZx80twB5t8oXluFC5Bvw694ilkjZejBTMvIEHlspFBLge6n-Mfe3ZmpY3KDWmp7017E_8mo8PxInqap7gPo9iA3xQdLz0LhJomzBQoxwMOMF1xBQFydsqa9vyLNU8e8)

Here is an example error that shows some details

![](https://blogger.googleusercontent.com/img/a/AVvXsEhORLFCV5Qpww_WYaBHU0HDWmdaZxOLdL_sWmBCITnhhRl6b0JQ7FifYKHp3A4erF6729w_9B0nDWC4z29FDzMyfGR8g6hMUXB6Ykm48O7a51qKkdVtiOURiBBbUlCCNXrrrh_CSd82pviTdB2SPTE5wnRLyJiu7jizr7KWat1pyeVCFrCUv4Gk7Dgz8_FA)

After fixing ~ Nice

![](https://blogger.googleusercontent.com/img/a/AVvXsEiKGKdyiEQ5a6h0NceFGyr9_vdGOsUWmVF6Uo7m-PPhuVIgnYn2ncWJS6CkOB8UOcMLE1rYXhY19jgZD-tHim75xk4nK9Ccz7gu77ut_Hyh4Y4DRee06SGox4GFjDVR8MZspTuYPxB5awr9vAg3wDwK2EkNFxq1NaMmSJeCnj11VPGh_vpCl9b8EtsUCvHk)

I am finishing up now ~ and this is how the folder system looks

![](https://blogger.googleusercontent.com/img/a/AVvXsEjDv7-fCSKZHzIxxgAB_Wfk24ZKl8X0kmrzMMYcVcctj8VfxZ5L1jXgmUGyzU8QW8dJSAfpA-W07fqE58sW8ulIlUnMIdcDio5eZ2leHQqfjq5roUwYBUjuf4KBWDncOvCPXNM_OAlxhRn8abEqf9rcanF_6rr6iO4uT0x1KL50TARyvYDlwrXpYbFG_ptn)

and I can push directly to my github, which is awesome. 
Everything is on my [repo](https://github.com/divakaivan/data-eng-camp/tree/main/Module_4).

This is not all for Module 4, tomorrow hopefully I will cover the rest ^^



That is all for today!

See you tomorrow :)
