---
layout: post
title: (Day 42) Creating a UNet with PyTorch
categories: [applying-knowledge,cnn]
---

## Hello :) Today is Day 42!
A quick summary of today:
* Road segmentation with UNet
* Forest area segmentation with UNet
* Human segmentation with UNet

But why? I can easily load an UNet (well-made UNet, even pretrained) and it will work great on the images I found. Well, I tried to follow the model from yesterday, that was written with TensorFlow, and I just wanted to translate it to PyTorch and use it on new data. 

Was my attempt successful? - Not exactly haha 
Why ? - Training takes a lot of time. I ran out of free GPU resources and started using CPU (super slow ㅜㅜ). Adjusting the learning rate helped, but adjusting other hyper params can help too, but training time is too long without free GPU hours ㅜㅜ

I had 3 attempts on 3 different datasets

![image](https://github.com/user-attachments/assets/5a212bfc-6782-44bc-92f4-b108352399f1)

This is the format that I tried to follow.

Model structure:

![image](https://github.com/user-attachments/assets/ed1bf54d-9a56-468f-be7c-edccb1c32af0)
![image](https://github.com/user-attachments/assets/6ab0c63a-5ac2-44c7-a95e-7279bc2d94fa)
![image](https://github.com/user-attachments/assets/1e1440a4-fb0b-44ba-b131-abdbead0a8b9)
![image](https://github.com/user-attachments/assets/83734375-2336-48e2-af94-b87f2d7cbdac)
![image](https://github.com/user-attachments/assets/3b4ab348-de26-4f54-a425-56afe2c1268f)
![image](https://github.com/user-attachments/assets/4e0b2d1a-8ed9-4f39-ae8d-66c3096bdce3)
![image](https://github.com/user-attachments/assets/4c90191a-76e5-4426-ab5a-a63551a63e46)

Actually, I followed a TF model and translated it, so a potential issue is that I might have missed or did something in the wrong order. But given that the results are not terrible, such possibility is low (but still exists). 

For the loss, I used DiceLoss (also learned about it yesterday) and also binary crossentropy.
As for data augmentations - used albumentations and did horizontal and vertical flip with 50% probability. And of course resizing to 128 (or 256).


Below are summaries and info about the 3 models.

1) Road segmentation with UNet

![image](https://github.com/user-attachments/assets/e53ba7d7-c77f-4183-8a7e-b9e36f897a47)

Actually this dataset was a bit problematic, because some of the pictures had these white spots, which ruined the models' performance, but if we look in the ground truth, there are actually roads there.
Some results that were not too bad ~ around 0.5 loss. 

![image](https://github.com/user-attachments/assets/ee0e0319-f7f5-4def-8e80-014e9aaf8c59)

And here is an example of a picture that is not very good and just hurts the model (probably can just delete those from the training to improve performance)

![image](https://github.com/user-attachments/assets/e0b4b765-43d8-4d1a-88f4-3bc55ff8e1c9)

2) Forest area segmentation with UNet

![image](https://github.com/user-attachments/assets/1df4a4fb-37ca-4a67-ad20-0587cbec209b)

Example of this particular dataset ^
The valid loss after 25 epochs was 0.65 and a sample result. Definitely not great. The model needs more adjustment for these images. 

![image](https://github.com/user-attachments/assets/07eb8fbb-6890-4163-8de4-9c57dca60b02)

3) Human segmentation with UNet

Sample image (flipped in this case haha)

![image](https://github.com/user-attachments/assets/fa93abd1-0e69-4708-8182-66be03f8954a)

After 100 epochs, with learning rate 0.0001 (last attempt) - 0.6 loss
And a sample result

![image](https://github.com/user-attachments/assets/b3661837-8227-4c3b-a3a9-4e0becdcbf11)

Not great. :(

Because the training was taking long, I ran notebooks at the same time on Kaggle, google colab and my laptop. Tried adjusting learning rate, image size, batch size. I tried to avoid adjusting the filters layers of the UNet to see if the same structure can work for different images, but unfortunately today I could not get good performance. I am thinking of concentrating on 1 and just let it run in the background while learning/trying something else tomorrow.

That is all for today!

See you tomorrow :) 
