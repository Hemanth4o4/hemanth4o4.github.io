---
layout: post
title: (Day 260) DeepLearning.AI Data Engineering Professional Certificate got realeased
categories: [data-eng]
---

# Hello :) Today is Day 260!
A quick summary of today:
* [the DE course](https://www.coursera.org/professional-certificates/data-engineering) by the writer of the DE holy book - Fundamentals of DE is live

I thought the course release date is the 19th, but I got an email saying I got access from today. I pre-registered so I got 14-day free trial and that means I have 14 days to complete the course

Yesterday I was planning to continue with the neo4j courses, but given that this DE course is for free for a limited time, this takes priority now.

Here is an overview of the course

<img width="1226" alt="image" src="https://github.com/user-attachments/assets/38f2d92c-5df1-4967-8480-5a15d6cfede9">

This program was designed by Joe Reis in partnership with DeepLearning.AI and AWS to cover the fundamentals of data engineering, both in terms of the underlying theory and frameworks for thinking like a data engineering, as well as practical skills for building data engineering solutions on the cloud. 

## Today I started the 1st course: **Introduction to Data Engineering**

----------------------------------------------------------------------------

## Week 1

<img width="824" alt="image" src="https://github.com/user-attachments/assets/69df0622-270b-42d2-9a8a-cfe7bf6ac4f0">

As a DE we will focus on the big square in the middle, but we will also look into the other 2 sides.

### A Brief History of Data Engineering

Data is foundational to information, existing in various forms such as numbers, words, or even phenomena like photons or wind. While data has always existed, digital data—recorded and processed through computers—gained prominence in the 1960s with the advent of computerized databases, followed by relational databases in the 1970s and the development of SQL. The 1980s introduced data warehouses for analytical decision-making, while the 1990s saw the rise of data systems for business intelligence. The dotcom boom in the mid-1990s led to rapid growth in web applications, supported by backend systems like servers and databases.

The 2000s saw the emergence of big data, driven by companies like Yahoo, Google, and Amazon. Big data refers to datasets characterized by velocity, variety, and volume, and its era began with innovations like Google’s MapReduce (2004) and Apache Hadoop (2006). Simultaneously, Amazon introduced cloud services like EC2 and S3, revolutionizing data storage and processing. AWS became a major player, followed by Google Cloud and Microsoft Azure.

By the late 2000s, the transition to real-time data streaming and cloud-first tools simplified big data processing. Today, the term “big data” has lost relevance, as scalable data tools are accessible to all companies, and data engineers focus on building scalable, business-driven data systems. The role of the modern data engineer is to leverage innovations and tools developed over the past decades to create data solutions that drive business value.

<img width="786" alt="image" src="https://github.com/user-attachments/assets/14d5d2b1-5559-4622-928e-ceaff626f08a">

### The Data Engineer Among Other Stakeholders

<img width="832" alt="image" src="https://github.com/user-attachments/assets/9e916c54-8f27-441f-a324-47e5a15ca81d">

### System Requirements

<img width="775" alt="image" src="https://github.com/user-attachments/assets/19301104-71b8-424b-aa1a-2b3d4c7e1161">

<img width="753" alt="image" src="https://github.com/user-attachments/assets/87061e05-1f37-4d9b-aeb7-728378f8dc7e">

The first step in any data engineering project is gathering system requirements, typically from downstream stakeholders who have business goals in mind. These goals often aren’t expressed as clear system requirements, so it's the data engineer's job to translate them. The process starts with conversations tailored to each stakeholder's technical background and role. In upcoming videos, experts Sol Rashidi and Jordan Morrow will provide advice on communicating with different stakeholders and effective requirements gathering. These videos are optional, and you can skip ahead to see a practical example involving an e-commerce company.

### Translate Stakeholder Needs into Specific Requirements

<img width="746" alt="image" src="https://github.com/user-attachments/assets/24f520a2-e9f0-47f2-9848-1c5f28502a53">

Example conversation with a DS

<img width="543" alt="image" src="https://github.com/user-attachments/assets/e4559273-bbe7-455d-a590-4a9719ac2ab4">

<img width="684" alt="image" src="https://github.com/user-attachments/assets/d878e2ca-a32c-4e4f-b1b6-9412b335fd63">

<img width="777" alt="image" src="https://github.com/user-attachments/assets/7311cf50-117a-48fb-b6c6-a6920a348c0d">

<img width="818" alt="image" src="https://github.com/user-attachments/assets/2e8d40e3-6c42-410c-b4b6-d4a67ac18a4a">

Key tactic: ask stakeholders what action they plan to take with the data (its not the same as asking what they need)

<img width="809" alt="image" src="https://github.com/user-attachments/assets/cb0fe99b-d3bb-4993-b565-19ab28214f50">

### Thinking like a DE

<img width="843" alt="image" src="https://github.com/user-attachments/assets/8c99451b-034e-4c9a-bf16-c74e51947a16">

<img width="737" alt="image" src="https://github.com/user-attachments/assets/df3350f9-5448-4fbe-b3c9-bb5b60283226">

### DE on the Cloud (aws)

The 2nd part of week 1 was an intro to the cloud and AWS' services.

5 core services: compute, network, storage, databases and security (there are more but these are good to start with)

<img width="835" alt="image" src="https://github.com/user-attachments/assets/3300b35b-395e-4851-8a49-64b9241cb6ac">

<img width="823" alt="image" src="https://github.com/user-attachments/assets/21d916ed-6ed9-4401-8e1d-1800668ef224">

<img width="771" alt="image" src="https://github.com/user-attachments/assets/811ba765-33d7-49c4-a96c-fcdb7504cc9a">

Databases:

<img width="776" alt="image" src="https://github.com/user-attachments/assets/1b82295f-a67d-40df-99ca-6844a0b3475e">

<img width="825" alt="image" src="https://github.com/user-attachments/assets/84defbd9-5477-44e7-abe0-d80441447c3b">

## Week 2

This part was very similar to his (Joe's) book (unsurprisingly). And it's nice seeing a video of the material I read in the book. 

<img width="834" alt="image" src="https://github.com/user-attachments/assets/bafefd0d-43dc-409c-8224-8d34834d546b">


### Data generation in Source systems

<img width="813" alt="image" src="https://github.com/user-attachments/assets/5bee449c-7698-4442-b84a-a8544fb34968">

The first stage of the data engineering lifecycle involves generating data from various source systems. As a data engineer, you’ll work with data from diverse sources like internal databases, APIs, and IoT devices. These systems, often maintained by other teams or external vendors, are out of your control, but understanding how they work is vital for building reliable pipelines. Common sources include relational databases, NoSQL systems, files, APIs, and data-sharing platforms. 

In reality, source systems are unpredictable—schemas may change, or systems may go down, causing downstream issues. Establishing good relationships with source system owners is crucial for navigating changes and ensuring smooth data workflows. Next, the process moves to data ingestion, which varies depending on the project.

### Ingestion

Ingestion is moving raw data from source systems into your data pipeline for further processing

<img width="690" alt="image" src="https://github.com/user-attachments/assets/f9082040-247a-464b-add7-25b7ff0906e7">


### Storage

<img width="629" alt="image" src="https://github.com/user-attachments/assets/567cf7f0-f915-4023-bfc4-e27cbc39d9cb">

As a data engineer, it's entirely possible that you will spend much of your time operating at or near the top of this hierarchy, meaning that you won't be required to think about the details of exactly how your data is moving between different storage components and systems. However, you will be most effective in your work. If you take the time to understand the inner workings capabilities and limitations of your entire storage solutions right down to the wrong ingredients. The truth is that many practicing data engineers today do not deeply understand the details of the storage systems they build. This leads to unfortunate consequences when it comes to things like performance and cost.

### Transformation

The transformation stage in the data engineering life cycle is where value is added by turning raw data into something useful for downstream users like business analysts and data scientists. Transformation involves three parts: queries, modeling, and transformation itself. Queries retrieve data from storage, often using SQL, and must be carefully written to avoid issues like performance problems or row explosion. Data modeling structures the data to reflect real-world relationships and make it easier for stakeholders to use, often requiring denormalization. Finally, transformation involves manipulating and enriching data at various stages, ensuring it's in the correct format for downstream use, whether for reporting, analytics, or machine learning. 

### Serving data

* analytics (BI, operational analytics, embedded analytics)
* ML
* reverse ETL

### Undercurrents

* security - use the principle of least privilage; adopt a defensive mindset
* data management - the development, execution, and supervision of plans, programs, and practices that deliver, control, protect, and enhance the value of data and information assets throughout their life cycles. Data governance is first and foremost a data management function to ensure the quality, integrity, security, and usability of the data collected from an organization

<img width="743" alt="image" src="https://github.com/user-attachments/assets/2c6888a9-61c1-4bce-a640-6f96b39cabc8">

* data architecture - a roadmap or blueprint for our data systems; the design of systems to support the evolving data needs of an enterprise, achieved by flexible and reversible decisions reached through a careful evaluation of trade offs

#### Principles of Good Data Architecture

1. Choose common components wisely
2. Plan for failure!
3. Architect for scalability
4. Architecture is leadership
5. Always be architecting
6. Build loosely coupled systems
7. Make reversible decisions
8. Prioritize security
9. Embrace FinOps

* dataOps - aims to improve the development process and quality of data products. DataOps is first and foremost a set of cultural habits and practices that you can adopt. These include things like prioritizing communication and collaboration with other business stakeholders, continuously learning from your successes and failures, and taking an approach of rapid iteration to work toward improvements to your systems and processes. These are also the cultural habits and practices of DevOps, and they're borrowed directly from the agile methodology

<img width="664" alt="image" src="https://github.com/user-attachments/assets/0fca7528-7c89-43dc-9ff3-469bec8acec7">

* orchestration

<img width="835" alt="image" src="https://github.com/user-attachments/assets/bc93face-21ea-44f1-b314-bf01bad4b3eb">

<img width="837" alt="image" src="https://github.com/user-attachments/assets/9b6a6b7b-4125-439a-a146-f854b27ed65d">

### Practical examples on AWS

Below are the AWS options for each part of the DE lifecycle

<img width="842" alt="image" src="https://github.com/user-attachments/assets/6742a5e5-8d36-4d7d-a6f5-6fbe8eeca09d">

<img width="842" alt="image" src="https://github.com/user-attachments/assets/d83ea176-eea9-452e-bc1e-68c89074ebeb">

<img width="834" alt="image" src="https://github.com/user-attachments/assets/e3ad0b12-d13c-4ff4-bdd1-bb3f5e047b64">

<img width="837" alt="image" src="https://github.com/user-attachments/assets/ef4aab60-dd66-4cfb-b8f0-ccfa2b7c97e8">

<img width="837" alt="image" src="https://github.com/user-attachments/assets/96a9ab43-eafd-4053-ab92-e24f59f8d5b1">

<img width="841" alt="image" src="https://github.com/user-attachments/assets/04083391-f7a0-40f9-afdd-f7947fbbd331">

Below are the undercurrents on AWS

<img width="832" alt="image" src="https://github.com/user-attachments/assets/d5cfbf63-6fc5-4fc8-ae7e-c6e5402e8e23">

<img width="783" alt="image" src="https://github.com/user-attachments/assets/4926743c-d47e-49aa-8243-33d0ce968567">

<img width="833" alt="image" src="https://github.com/user-attachments/assets/f07f57d7-cd95-4ca6-9c24-0eb58cdbee92">

Orchestrator - airflow (but any of the other popular ones works)

<img width="661" alt="image" src="https://github.com/user-attachments/assets/153b1daa-3a27-4e72-b2bd-ff84eca127ec">

<img width="746" alt="image" src="https://github.com/user-attachments/assets/02b807b5-6771-4333-94de-021a1984d7fd">

### Next is an assignment in a hosted AWS environment

Below is a summary of the assignment as I do not want to give too much of the actual thing.

<img width="474" alt="image" src="https://github.com/user-attachments/assets/1bc32c45-3287-4662-807b-030bc5e534a7">


#### 1. Setup Review

<img width="1491" alt="image" src="https://github.com/user-attachments/assets/11480a77-dc4c-4406-a8a9-d89cd83b7776">

- setup Cloud9 (an AWS IDE)

<img width="1509" alt="image" src="https://github.com/user-attachments/assets/fc458752-3061-455d-9364-676ef34719e4">


#### 2. Exploring the Source System
- Access the source system database (MySQL) using the AWS RDS service.
- Retrieve the database’s endpoint and connection details from the AWS console.
- Establish a MySQL connection using the endpoint, username, password, and port.
- Explore the database structure using SQL commands (`show tables`, etc.).

#### 3. Resource Creation
- Use Terraform files to create and configure AWS resources (AWS Glue, S3).
- Initialize the Terraform environment and run commands (`terraform init`, `terraform apply`) to deploy resources.
- Review `.tf` files for understanding Glue, S3, and networking configurations.

#### 4. Running the Glue Job
- Start the AWS Glue job using a command provided in the lab.
- Monitor the job progress in the AWS Glue console (ETL Jobs section).
- After the Glue job completes, verify the transformed data in the S3 bucket.

#### 5. Querying the Data
- Use a Jupyter notebook to run analytical queries on the transformed data stored in S3.
- Utilize AWS Wrangler and Amazon Athena to extract data from S3.
- Perform queries, such as finding total sales by country and other insights.

Tbh, this is amazing. The way this whole env is setup - really makes me feel like I am using the cloud and doing something real.

The end result is creating an interactive dashboard in jupyter:

<img width="869" alt="image" src="https://github.com/user-attachments/assets/4b9c4639-d6aa-4e30-a1f3-6c0e6c68aa47">

My score after submitting the lab:

<img width="441" alt="image" src="https://github.com/user-attachments/assets/067e6f91-4169-4e6a-a2fd-8b7bdfd4e209">

## Week 3

### Data Architecture

<img width="782" alt="image" src="https://github.com/user-attachments/assets/4ede4b17-c1e7-4009-b534-fb96da044fde">

Conway's law - any organization that designs a system will produce a design who structure is a copy of the organization's communication structure

<img width="814" alt="image" src="https://github.com/user-attachments/assets/3fa0ee83-03cc-4258-b72b-eb61241fdd5f">

To Be Continued ...

---

Tbh, I was expecting high quality from DeepLearning.AI and Joe Reis, and the course so far absolutely delivers on those expectations. 

That is all for today!

See you tomorrow :)
