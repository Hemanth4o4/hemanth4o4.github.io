---
layout: post
title: (Day 5) Kaggle's 'Learn' courses
categories: [machine-learning]
---

## Hello :) Today is Day 5!
A quick summary of today:
* found extremely helpful courses for ML in Kaggle

![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/0ea13b70-d32f-4737-9707-f9a2c77139bd)

The content of the above courses is as below

**Intro to Machine Learning**
* model creating, training and predictions
* model validation
* model selection - under and overfitting
* beginner RandomForest model creation

**Intermediate Machine Learning**
* dealing with missing data
    * deleting the column, or imputation
* handling categorical data
    * deleting the column, ordinal encoding, one-hot encoding
* pipelines
    * define preprocessor
      ![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/1647ca59-f368-45e2-9bde-5cc0acf22fee)

    * define model
 ![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/6126bf51-35a3-41ae-b7ba-48742efb6c3c)

    * create pipeline
      ![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/1eb0821b-b2c7-4a01-9d0e-c17744a866e0)

* cross-validation
* XGBoost
* Data leakage (target leakage and train-test contamination)

**Data visualization**

Using seaborn
* line plot, bar graphs, heat maps, scatterplot

**Feature engineering**

* Mutual information - it is a lot like correlation in that it measures a relationship between two quantities. The advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships. Features should be integers

  ![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/f7315274-4f55-4d52-9989-b52e3625159c)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/e267b451-f1ff-479c-b08a-4d0b0ec61455)

* Clistering with K-means
* PCA - its primary goal is to reduce the number of features (or dimensions) in a dataset while preserving as much of the original variability as possible (need to look more into this)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/8a042c9c-f385-4bab-a5b7-24220a62325c)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/49d53303-4084-4b65-bc3e-517da14e73a9)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/b917ce5a-c377-4b9c-b7b7-4f715b79025c)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/c7fe6f85-86dc-4c16-92cc-53757b61de5c)

* Target encoding with MEstimateEncoder
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/732635cb-9f21-4122-9eae-1071e642d1ca)

Above, instead of mean encoding, smoothing would be better. The idea is to blend the in-category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average.

![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/33da4baf-0fff-4eb7-8d0f-da0edc0eb163)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/d6101a7f-1956-4e8c-928c-4cea85c7a95c)

When choosing a value for m, consider how noisy you expect the categories to be.
Use Cases for Target Encoding:
* High-cardinality features: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.
* Domain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness.

Today I learned about PCA, pipelines and target encoding, but I have a feeling I need to look into them a bit more. 

That is all for today!

See you tomorrow :)

[Original post in Korean](https://50daysml.blogspot.com/2024/01/day-5-kaggle-learn-courses.html)
