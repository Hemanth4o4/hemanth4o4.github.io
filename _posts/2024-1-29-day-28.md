---
layout: post
title: (Day 28) Diving deeper into Convolutional Neural Networks
categories: [deep-learning, cnn]
---

## Hello :) Today is Day 28!
A quick summary of today:
* covered [CNN by Andrew Ng](https://www.coursera.org/learn/convolutional-neural-networks/)

Today's material was a bit hard, but still fun ^^ 

I learned about widely used CNN architectures (LeNet, AlexNet, VGG-16, ResNet, MobileNets)

### 1. Basics

![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/693d7c8f-1e2f-49b1-b739-774a0ad3ceb7)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/cb20a0e4-f8ec-4e7c-938d-24eacec2ea62)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/2270e89c-acb9-4d0b-961c-bf9813415681)

depending on the filter hyperparam, we can learn different features

![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/941b82d2-4a9b-478b-a4bd-57225ad4498e)

During training, the w1....w9 parameters can be learned, allowing us to know the different features of the image

![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/f2c4b504-9377-4568-b132-5538cf3330b0)

If you look at this picture, the model is composed of a conv layer and a pool layer.
Among the pool layers, there are average and max layers, and the calculation is different.

If I want to learn more about architectures, I will need to read some research papers. 

### 2. Widely used CNN architectures

* LeNet
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/2692b5b9-8e96-4269-a3d7-7f2417d6626d)
7 layers: 3 conv, 2 pooling, 2 dense. Used for handwritten digit recognition

* AlexNet
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/855ded16-5456-4e70-9f9b-e9a0eb95ed48)
8 layers: 5 conv,  max-pooling layer, 3 dense

* VGG-16
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/0da8f37a-6ff1-4694-b438-020e0a42da5e)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/c09104d1-6cd7-429b-9f3b-0a69dcc74c6d)
conv + pool + dense layers

* ResNet
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/b87e8fa8-c0f2-4d0c-b908-468b3f7c2d39)
different architectures: ResNet-18, ResNet-50, ResNet-101, and ResNet-152

ResNet's 'skip connections' allowed us to train deeper learning neural network model. 

### 3. Other

* 1x1 convlutions
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/e64d5fe9-98b0-42fd-9e5b-453db5a52f0c)
I need to read the paper for this one - [https://arxiv.org/abs/1409.4842](.)

* MobileNets
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/8e71410a-a024-4359-b144-4de7e0dcae3f)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/62e652b2-79c1-4f74-98ce-23b4c4ee1dee)
![image](https://github.com/ivanstudyblog/ivanstudyblog.github.io/assets/167014511/3bfdb4fe-bc2d-4182-89a3-19924b36934d)

There is big cost difference.


I need to apply what I learned today in Kaggle some day.

<br/>

That is all for today!

See you tomorrow :)

[Original post in Korean](https://50daysml.blogspot.com/2024/01/day-28-convolutional-neural-networks.html)


