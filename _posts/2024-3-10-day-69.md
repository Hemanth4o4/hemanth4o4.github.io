---
layout: post
title: (Day 69) Training an LLM to generate Harry Potter text
categories: [applying-knowledge,theory,nlp]
---

## Hello :) Today is Day 69!
A quick summary of today:
* tried to build upon the built LLM from the book from the days before, and write a training loop in the hopes of generating some Harry Potter text ([kaggle notebook](https://www.kaggle.com/code/divakaivan12/llm-from-scratch-harry-potter-text-generator/notebook))

Firstly I will provide pictures of the implementation
(then share my journey today)

The built transformer is based on this configuration

![](https://blogger.googleusercontent.com/img/a/AVvXsEgk_XN6e1osZg5TujmB4brtdyElFbpnJXACHZt6E0WuMTEGVk64nVrC8ZARD_yFs8bqZPgrYOhdqXS2dNb9Aq6CwTNPuxawt7VKMppLr9GjqjrbqztdjDHcIzGgss8PTupuYQfyF1YhONvBPS28-vitVROexYOlE4_yAiIpopVYHq9_OULhQbqXiW_9znih)

Dataset is Harry Potter book 1 (Harry Potter and the Philosopher's Stone) text file from kaggle. Used batch size 16, and a train:valid ratio 9:1

Model architecture code:

Multi-head attention

![](https://blogger.googleusercontent.com/img/a/AVvXsEhdLmtUI7uV1zhCY7qlT7b4QxSzm9T8n-3Orrl7VDXbpOm9obWqMrnQcEKvse5TmDDtVTTq_kChK5s0ntgxnqc2yYfRGKpTVp76rbLQ1tkJk9XBhe6WkvbNjTjyI3GDJhKwLK8AczVlg0PB_EPKwGQ2wLKve37dYZsNKFQqM9qvdq8A3Ogw5X7Nhg14IQ2p)

GELU activation function

![](https://blogger.googleusercontent.com/img/a/AVvXsEhnuoqw50yT--JAwuPIaVdIhjUI8ho1JmBMoq8umvwCxMDy0gI39MSeACGs3JAa8TQxoStPzDkA8P8JiEgZRbRaRQDbZ9-rFtYtIn2zlmSgujMDkYEIYSYnrEqzE5-QF7ZvW06vyd_1w4O0BQxppyZ52dABcPprTthYx16_5hcy71yBw5YNsGZ_2-f3tOoh)

Feed forward network

![](https://blogger.googleusercontent.com/img/a/AVvXsEhyAqfHgDI_NbVLpKGHlOAYsuo3UDXOeeDnNu3IXiw4WhFNnQm4PxO6aMXqCwHrJX1zTihkOIeyq2xZYxeaHGecB3dW5r2fU2DVA9BXs_tYakk-wQAaAEGXIF5Qe7u_28SL5rnHc4R17fNsOdJ3lu7-mOPWlqL5um3ONA99lzAOAaLiC9VggmwMIWSBGe9m)

Layer norm

![](https://blogger.googleusercontent.com/img/a/AVvXsEj7nV91C4JZtK_FueMbrFLvbVxdWeRXqlekZYNxRqmcYF7y5Y9fgVpIevVpFI0o8_MsPS50RaamxXGiZ_4BB5Xmvsw5uFxYj7cGecd2DylNC6sk6KwwBJggiGazWeKhmmi9aS2Isq9v14USWwnqDoJCZ_gf14_oG2sJrJ2wADbOdQfIIjuwJfKnC0juvW89)

Transformer block

![](https://blogger.googleusercontent.com/img/a/AVvXsEijR5HpiruhBJRV2nZMh8iLoANTNBWL_Rl-rk_yhfZeeqKAV2VzvfRBlwx4frVNzrzCxCjt-iuz_C_MZ6IoZCovU-47E86aKmkb-Ges3gUPwtLwcX8wytlEPnfz317o8YyTENwYCsED5Oequc_ON5xgsK874_3azCA1VrXZtEi-5g8qOm63oGRs7bZQUz1X)

GPT model (+generate function)

![](https://blogger.googleusercontent.com/img/a/AVvXsEj6rhmhDEnejMsuvrgH6gupybxi4uXaZgxfjb2kyBQYFnkWMpMm5m9hSKevhfIf2sKU_0xDVQdKOrnPIrrhJicgpw9n4YuInWSTAYnAc_Ql0xp8iAjcEfPoZgrbmgjg_TweCpww86yKrEWgxo2x0xx8-Nk2o7KjexQRUS-1_rhV7meXxG-_SuawoiIllda-)

Optimizer: AdamW with learning rate 5e-4 and 0.01 weight decay. Ran for 1000 epochs. 

Final number of model parameters: 1,622,208 million

**Now, about my journey today ~**

The easy part of was the dataset choice - harry potter.

The hard part was choosing hyperparams that would end up in not perfect, but somewhat readable and some logic to it. 

The result is:

![](https://blogger.googleusercontent.com/img/a/AVvXsEicBIvZYKyRlIsafB5wkXy_cnYBQ3qPFQvHD85MwwuHyh8_mWFvg5Bjl58faMB0G6OC4v5eYnex9pIo5W6r4A7aeruIBFfUHP-c0xXmaBiZIfZooBX2dXipQA7rQfKVPGQBcolJvLW-HXjXb9Pl0JsCx3QcypTp8-dJ3nTRMXHqBgEvevQvCf7nbjhMqUp_)

Initially, I was doing it with just a train_dataloader (no validation) and I found a combination of hyperparams (manual search) that resulted in going to ~0.4 loss and the text was readable. Then I wanted to add validation to my process. That is when, almost every combination of hyperparams (higher learning rates, lower learning rates, embedding dims, context length, playing with the GPT_CONFIG) resulted in exploding valid loss. During my time today, I was running different versions on colab, my laptop (both slow) and on kaggle (fast), and I was getting great training, but lowering and at some point increasing valid loss (also at the points at which the valid loss started increaseing, the traing loss was still high, I remember in some cases still between 3.~6.). Lowering the learning rate was the biggest factor affecting the valid loss jumping after some epochs. The training was slow, so many params and understandable. Of course - a transformer might be an overkill for such a small (~600KB) dataset, but I just wanted to practice, to see the transformer in action. Still lots to learn about the inner workings, definitely using a hyperparam search would help, and also using weight clipping and lr scheduler - which I saw are used with transformers. I used them for 2 runs, but then I removed them, because at that point I was switching between train and train+valid training loop because I thought my code was wrong in how I calculated the valid loss. 

Below is a generated text from another run that is on kaggle too (version 1).

![](https://blogger.googleusercontent.com/img/a/AVvXsEhI1qlZjtC2lCag4zZ16WOm6Txy61gJBK8Vo5ffPPvefiowNLrbqS-4zdX2U_MqAX5IITgqkElNq-zd19UkcF0kgRgBNmqSv_eRhLaCCwNVIDJuPXMLr8suFuVYKrQD9vDRfc3K77TSULdPvjdC-XykqsHbTeggeUtp8O8LAtdQAXgRX_EvnIrUoQTibXKG)

It is ...something... haha.


That is all for today!

See you tomorrow :)
