---
layout: post
title: (Day 53) Getting closer to becoming a 'backprop ninja' (thanks to Stanford Uni's cs231n assignments)
categories: [applying-knowledge,theory,deep-learning]
---

## Hello :) Today is Day 53!
A quick summary of today:
* Started Stanford University's [cs231n](https://cs231n.github.io/)
  * [Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits](https://cs231n.github.io/classification/)
  * [Linear classification: Support Vector Machine, Softmax](https://cs231n.github.io/linear-classify/)
  * [Optimization: Stochastic Gradient Descent](https://cs231n.github.io/optimization-1/)
  * [Backpropagation, Intuitions](https://cs231n.github.io/optimization-2/)

First I will quickly share my notes from the 1st three, and then for the big one (backprop) - will do it last.

1st lecture - Image classification using KNN
Actually my impression is not so much from the theory in the lecture, but from the assignment exercises which the course has. 

The KNN assignment was about implementing the loss of KNN (L1/L2 distance) and how to do it efficiently. 

Firstly there was the most inefficient one - with two for loops

![a](https://blogger.googleusercontent.com/img/a/AVvXsEgBxdteU9X-a6cq-N-JIdWhwmAj7OLZXewPFBW5AZjEDvTh5_tYpQOlwQvYX0tXdJGY0TP2p5VjTHCTM01P2jAlm7fa2YkSwgsoota9cvj4O5as-mmE4zTaKVtmEJV4R61eVD8BmQfNJyIm0bxcW5gtqy_uQRIv4mBNN-JFE85uWZEdvN8_6-9Ac6WWpkP7)

Then it was with one for loop

![a](https://blogger.googleusercontent.com/img/a/AVvXsEgk0OCj_DkIyM69INv3vEFrnW_pFIhAoQEdF3KZUxnyB3vPDypPQeWfEWO3kaDkP00swmZ1GMe23yFAwwlxf-Uc-tSTqDT-eXbNt8izImDPbFCZ6gRnUDT0CVkzX3zIfvYv3ZuBYDnSc8lELtodGOg_S6exHBdYTzMaQz2-GzlIAKRTzbC3XOFao5jc1Gzz)

And then, the most efficient one which uses matrix multiplication magic

![a](https://blogger.googleusercontent.com/img/a/AVvXsEirDSwFbTeiRJB-l8Fx9sjngVLOyPcEtZZljrD5jZh1nmbFnOK2TYkSZtsm6Y1W4Nm3RExZylaT8Kg9QcApFC8UCwrEQR7cRlS27SPTHFRBMekcQT7WMiIgLNCJhh_1T01lmlFhMw1CxsyDntw8vwmaVaSKqBlc-h9BKqZbbl7e5p9RgBim0JZa_qtzOZm1)

























