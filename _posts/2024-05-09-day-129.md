---
layout: post
title: (Day 129) AI with a Scottish accent? + MLE lecture by Chris Piech (Stanford CS109)
categories: [applying-knowledge,theory,statistics]
---

## Hello :) Today is Day 129!
A quick summary of today:
* I thought to myself: is there an AI that can speak with Scottish accent and does not sound robotic
* covered Lecture 21 from Stanford's CS109 on Maximum Likelihood Estimation


### Firstly, about the Scottish AI

I was looking for videos to practice my Scottish accent (which is awful). After a few, I thought whether there has been an AI model that given text, can generate that text being said in Scottish accent. Started looking, and the best I could find was [this](https://www.narakeet.com/languages/scottish-accent-text-to-speech/), which tries to do it but it maybe sounds 5% (general) Scottish. 

Then, I started looking on huggingface and kaggle, for models developed by solo devs - nothing. I searched TTS models, most are trained on general south UK English, American, Indian, Australian. But none can speak in Scottish. 

Then, I started looking for a dataset. I could not find any either. That is when I tried something.

I found one video on youtube where there is only Scottish accent used - [The Glasgow Uni accent](https://youtu.be/j11YFdS7hwY). I downloaded the audio file of it, and transcribed it myself. 

The short version is: 'Hello? Hi Shanese. How are you? Who is this? It\'s me Kath from school, silly. Why you talking like that? Like what? Like you\'re changing. Don\'t be daft. Anyway, that\'s me done with uni...' (I did the full video but this is just the start). 

However, while transcribing, though able to understand, there were a few words that for the life of me I could not understand, and youtube CC could not help either. 

![image](https://github.com/user-attachments/assets/bb1a3a75-7029-4160-9515-16da0842b772)

I got this for 1. But what if I could do it for many, and actually transcribe it properly. At the time, I was thinking that this requires manual work, because we need to get the Scottish audio transcribed correctly. A general AI model that was not trained on Scottish accent might not be able to transcribe perfectly the Scottish accent (maybe this is something to test).

I need a Scot! - is what my first thought was. So I made a few posts on r/learnmachinelearning and r/scotland proposing my idea to create a dataset of transcribed audio files so that future generations of AI can also speak in Scottish accent. 

Comments came in and there is a lot of good info about organisations and institutions that aim to preserve Gaelic and promote Scottish, info about potential websites I can use for data. And after a few hours, someone interested replied! I will call them 'M'. M shared with me a youtube video where they do short phrases in a general Scottish accent, as an example of what we can use for our dataset. We connected on LinkedIn, and we will have a chat about it this Saturday. Exciting!

Before then, I will look at some research papers about audio file length when training TTS models. My initial thoughts are to get this dataset going. Audio files + transcription, but we will discuss it with M. 

### Secondly, about MLE by Professor Chris Piech

My thoughts while watching: it's kind of scary how great these explanations are

Jokes aside, wow! So far the best MLE explanation I believe was from the book Intro to Statistical Learning, but now I definitely think this lecture beats it. And it's not like if someone new to AI watches that one lecture they will know MLE, watching the previous lectures - it's like building up to MLE, but without knowing it. And *all* the explanations along the way are so clear, and now with MLE too. Anyway ~ below are some screenshots I took.

We begin with an image of how AI, ML and DL fit in together

![image](https://github.com/user-attachments/assets/5a2db663-2d17-4a5c-bfe7-be997a681df8)

Then, is our path to how to get to DL

![image](https://github.com/user-attachments/assets/ef79ede3-2014-437f-844d-36ce9f59e30a)

In that big box there are 3 mini boxes

![image](https://github.com/user-attachments/assets/35ad5532-6573-4396-9f69-8bb0ed86e91e)

The 1st one, from the lectures so far, we learned how to do

![image](https://github.com/user-attachments/assets/bdc1b7f0-c5a3-49eb-8fce-2540b0d63e79)
![image](https://github.com/user-attachments/assets/6d8e3d8c-a70b-46aa-9a51-b0a057a96173)

We already know how to get unbiased estimateros (the 1st part in the Path image)

![image](https://github.com/user-attachments/assets/689dde4a-231b-41c7-ab06-b48f952bf881)

Now, for the 2nd cube - MLE: the theory behind a general method for choosing numbers in a model

We begin with trying to guess the mu and sigma of some random data points, and try to find the best parameters that result in the data

![image](https://github.com/user-attachments/assets/b00f67f3-713f-42e5-afe0-d60928abb1a1)

We want to find the likelihood of the data resulting from the params.
We can find those params by using argmax (find the arguments of a function that maximise the likelihood)

![image](https://github.com/user-attachments/assets/393f928d-3f8a-4133-b9c4-06b8dc6bab99)

While max looks for the max value, argmax is looking for the inpuWe are not really interested in how likely is our data, what weâ€™re really interested in is which input of mean and std make the data look most likely.t that results in the max value.

It turns out that the argmax of a function is the same as the argmax of the log(function). And the log helps us with dealing with very very very small numbers and also cleans up the math (as seen later)

The Holy Grail for MLE

![image](https://github.com/user-attachments/assets/f4175303-1494-49e4-845e-c75c1c6e6cb1)
![image](https://github.com/user-attachments/assets/81bbd5d0-6d19-4391-8b5b-b6404b247bcd)

(we always assume iid, otherwise nothing works)

![image](https://github.com/user-attachments/assets/bcb5be5e-41b8-4cd6-93c8-231d93ce5dbc)

Then we derived the MLE with Poisson, which is just average the data points.

![image](https://github.com/user-attachments/assets/c7687821-cb36-41a0-a3d0-37f65edf8aab)

Then we tried for Bernoulli

![image](https://github.com/user-attachments/assets/c35fda3c-d4ed-425e-bc97-340db28d7b25)

However, this is not differentiable!!!

![image](https://github.com/user-attachments/assets/6c4a008a-0095-44ba-a0d0-0b63b18bed5b)

The right is a continuous version of the left expression

![image](https://github.com/user-attachments/assets/1ec276bd-7a37-4bae-aaeb-e1d71cc80482)

Result: choose your p to be the sample mean (same as Poisson)

Then for a Normal distribution

![image](https://github.com/user-attachments/assets/c468c49d-6a31-438d-9a24-689effc2e31d)
![image](https://github.com/user-attachments/assets/6037a4fd-861e-44a1-b720-72cf2fc92a1d)

However, the variance is a biased estimator (no n-1)

Finally, MLE with Uniform dist

![image](https://github.com/user-attachments/assets/58f222bf-78d4-4779-a960-d3681e69c9de)

In uniform, MLE chooses alpha to be the smallest in the data, and beta the largest number. But as seen the original params are 0 and 1. This leads to the problem that small samples cause to MLE (our good ol' friend - overfitting)

![image](https://github.com/user-attachments/assets/7b533e13-9104-4b00-bcdb-7fdf323e79ce)

Finally, summarised with the properties of MLE

![image](https://github.com/user-attachments/assets/ce110359-2e35-4741-9da5-338c6a5c207f)

Just amazing. What a great way to introduce MLE, a key concept.


That is all for today!

See you tomorrow :)
