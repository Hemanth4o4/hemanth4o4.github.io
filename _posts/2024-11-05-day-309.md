---
layout: post
title: (Day 309) Finishing scikit-learn's MOOC + AI fairness
categories: [theory,applying-knowledge,traditional-machine-learning]
---

# Hello :) Today is Day 309!
A quick summary of today:
* reading some of the fairness python lib docs
* stream studying the last 2 modules of sklearn's MOOC

## [Loan Approval Model fairness](https://fairlearn.org/v0.11/auto_examples/plot_credit_loan_decisions.html#)

I got this post somewhere from sklearn's MOOC and I kept it for a few days, and today I decided to read it.

The library `fairlearn` is about assessing and improving the fairness of models - like not to disciminate between genders/race/etc for loan approvals, and the above example link in their docs is exactly about credit loan decisions (they have other examples too)

In the US (Apple to be specific) there was a backlash in 2019 when Apple's Apple Card allegedly gave men higher credit limits than women, even for married couples with joint assets. It shows that while AI systems can comply with the law, they can still exhibit fairness issues. Some fairness interventions may even conflict with anti-discrimination laws

### Dataset

It includes demographic features, payment history, bill statements, and default information. The task is to develop a model to predict whether an applicant will default on a personal loan, using credit card payment history as a proxy for creditworthiness

The dataset is imbalanced when it comes to the `SEX` feature:

```
SEX
female    0.603733
male      0.396267
```

An additional synthetic feature is create - 'interest rate'. It adds realism by simulating financial factors in loan approvals, allowing to demonstrate how additional features can influence fairness assessments and bias in credit decision models.

### Fairness assessment

After an initial fairness-unaware model is trained, the next step is performing fairness assessment. The steps are:

* identify who will be harmed
* identify the types of harms we anticipage
* define fairness metrics based on the anticipated harms

**Who will be harmed?**

* based on the US Apple case, the model may incorrectly predict that women will default on their loans
* the system might unfairly allocate less loans to women and over-allocate loans to men

**Types of harms**

In this case:

* allocation harms: when AI extends or withholds resources, information, or opportunities, such as in hiring or lending

Other: 

* quality-of-service harms: when AI performs inconsistently across groups, like differing accuracy in face recognition or search results
* stereotyping harms: when AI reinforces stereotypes, often through autocomplete suggestions, shaping expectations based on characteristics instead of individuality
* erasure harms: when AI omits certain groups or histories, like ignoring notable figures or cultural sites, leading to underrepresentation or misrepresentation

A secondary harm, is the long-term impact on an individual's FICO credit score, getting approved/denied for a loan has an impact on your score and future applications.

One other type of harm we anticipate in this scenario is the long-term effects of denying loans to applicants who would have successfully paid back the loan

**Fairness metrics**

* balanced accuracy
* false positive rate
* false negative rate

**Results from the fairness-unaware model**

![image](https://github.com/user-attachments/assets/d1d50b65-7d97-49e3-a151-dcf5f3fc8470)

The model produces a significantly higher false_negative_rate and false_positive_rate for the applicants labeled female compared to those labeled male. In the context of credit decision scenario, this means the model under-allocates loans to women who would have paid the loan, but over-allocates loans to women who go on to default on their loan.

### Mitigating unfa

* postprocessing: in the postprocessing approach, the outputs of a trained classifier are transformed to satisfy some fairness criterion
* reductions: in the reductions approach, we take in a model class and iteratively create a sequence of models that optimize some fairness constraint. Compared to the postprocessing approach, the fairness constraint is satisfied during the model training time rather than afterwards

**Using the first approach, the model produces:**

![image](https://github.com/user-attachments/assets/4f3e202a-a1cb-47b2-a499-351b177126dd)

The model achieves a much lower disparity between the two groups compared to the unmitigated model. However, this does come with the trade-off of lower balanced_accuracy score for male applicants.

**Using the 2nd approach:**

![image](https://github.com/user-attachments/assets/22302248-6441-44d2-8c46-7dc82bcfe386)

In the reductions model, we see a decrease in the false positive rate for female applicants. This is accompanied by an increase in the false negative rate for male applicants

Overall, the equalized odds difference for the reductions models is lower than that of the original fairness-unaware model.

#### Fairness under unawareness

When proving credit models are compliant with fair lending laws, practitoners may run into the issue of not having access to the sensitive demographic features. As a result, financial institutions are often tasked with proving their models are compliant with fair lending laws by imputing these demographics features. However, some research has shown that these imputation methods often introduce new fairness-related issues.

After this I decided to check some of the other example notebooks in their docs.

## [CorrelationRemover visualization](https://fairlearn.org/v0.11/auto_examples/plot_correlationremover_before_after.html)

They have a `fairlearn.preprocessing.CorrelationRemover()` class which does what the name says I guess. A diabetes dataset hospital is used, and the goal is to reduce the correlation between a sensitive variable of interest (`race_AfricanAmerican` in this case) and the other variables without affecting the correlation between the other variables.

Before:

![image](https://github.com/user-attachments/assets/c49a21e2-68ff-4d63-8142-fb59aeafc1f3)

After:

![image](https://github.com/user-attachments/assets/b146c77d-b702-488c-9162-93458684942d)

After, with alpha=0.5:

![image](https://github.com/user-attachments/assets/cb6d634e-96d3-45ec-b33a-198c87bd554f)

Even though there was not a high amount of correlation to begin with, the CorrelationRemover successfully removed all correlation between 'race_AfricanAmerican' and the other columns while retaining the correlation between the other features. Using a lower value of alpha results in not all the correlation being projected away.

## [MetricFrame visualizations](https://fairlearn.org/v0.11/auto_examples/plot_quickstart.html#)

_This data structure stores and manipulates disaggregated values for any number of underlying metrics. At least one sensitive feature must be supplied, which is used to split the data into subgroups. The underlying metric(s) is(are) calculated across the entire dataset (made available by the overall property) and for each identified subgroup (made available by the by_group property)._

```python
# import
from fairlearn.metrics import (
    MetricFrame,
    count,
    false_negative_rate,
    false_positive_rate,
    selection_rate,
)

# prepare diabetes dataset ...
# use a DecisionTreeClassifier ...

# Analyze metrics using MetricFrame
metrics = {
    "accuracy": accuracy_score,
    "precision": zero_div_precision_score,
    "false positive rate": false_positive_rate,
    "false negative rate": false_negative_rate,
    "selection rate": selection_rate,
    "count": count,
}
metric_frame = MetricFrame(
    metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=A_test
)

# plot
metric_frame.by_group.plot.bar(
    subplots=True,
    layout=[3, 3],
    legend=False,
    figsize=[12, 8],
    title="Show all metrics",
)

# there are different options as well, using ylim, colormap, plot kind
```

Out:

![image](https://github.com/user-attachments/assets/fb4f621e-ba3b-45d1-b35f-4cd7c266c883)

Here are others that I read:

* [Passing pipelines to mitigation techniques](https://fairlearn.org/v0.11/auto_examples/plot_mitigation_pipeline.html)
* [Basics & Model Specification of AdversarialFairnessClassifier](https://fairlearn.org/v0.11/auto_examples/plot_adversarial_basics.html)
* [GridSearch with Census Data](https://fairlearn.org/v0.11/auto_examples/plot_grid_search_census.html)
* Showed this one on stream - [Plotting Metrics with Errors](https://fairlearn.org/v0.11/auto_examples/plot_plotting_metrics_with_error.html)

## [Stream](https://youtu.be/iLJa9dI71yQ)

Covered [Module 6 - Ensemble of models](https://inria.github.io/scikit-learn-mooc/ensemble/ensemble_module_intro.html)

Just like the previous modules, this was amazing as well. Main overview:

* understanding the principles behind bootstrapping and boosting
* get intuitions with specific models such as random forest and gradient boosting
* identify the important hyperparameters of random forest and gradient boosting decision trees as well as their typical values

## [The last module - Evaluating model performance](https://youtu.be/v8vejOmvNJ4)

The goals of this last module were:

* understand the necessity of using an appropriate cross-validation strategy depending on the data
* get the intuitions behind comparing a model with some basic models that can be used as baseline
* understand the principles behind using nested cross-validation when the model needs to be evaluated as well as optimized
* understand the differences between regression and classification metrics
* understand the differences between metrics


Overall ~ I don't have enough nice words to describe how great this MOOC was. It's amazing! And definitely in my 'to-recommend' list for people wanting to get into ML.

---

That is all for today!

I streamed for about 6 and something hours and it was a bit exhausting ðŸ˜†

See you tomorrow :)
