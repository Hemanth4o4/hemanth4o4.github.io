---
layout: post
title: (Day 245) 
categories: []
---

# Hello :) Today is Day 245!
A quick summary of today:


## Streaming Databases Chapter 4: Materialized Views

### Views, Materialized Views, and Incremental Updates

Materialized views separate the processing logic for generating query results from the main processing pipeline, leading to more modular and manageable code in stream processing systems.

#### Traditional Views vs. Materialized Views

- Traditional Views:
  - Defined by SQL statements executed upon client selection.
  - Results aren't stored, leading to higher query latency as data is not preprocessed.
  - Analogy: A smart chipmunk, Simon, counts nuts in a yard every time he's asked, resulting in delays.

- Materialized Views:
  - Store precomputed results, reducing latency.
  - Analogy: Simon counts the nuts and stores the total, while a less-smart chipmunk, Alvin, reads and returns the stored value
  - Incremental changes are monitored, avoiding full recomputation.

#### Incremental Changes

Incremental updates refer to small, targeted changes to data instead of recomputing everything from scratch. This approach keeps data up-to-date with less computational overhead. Simon continuously tracks changes, similar to streaming settings.

### Change Data Capture

Materialized views and Change Data Capture (CDC) are closely related. Materialized views handle precomputing by monitoring incremental changes and storing the results, while CDC provides these incremental changes by capturing data modifications from the Write-Ahead Log (WAL) in an OLTP database. This allows materialized views to preprocess the incremental changes captured by CDC.

#### Extended Chipmunk Analogy

- Attributes of Nuts:
  - Color
  - Location (latitude, longitude)
- Nuts can change color, move, or be removed by other animals. Simon updates the list of nuts in the box to reflect these changes, ensuring that any client querying the list sees the latest nut status.

- WAL Replication:
  - The WAL from the primary/OLTP database is replicated to create a database replica.
  - A CDC connector writes the WAL to a topic in a streaming platform, allowing other systems to subscribe.
- Consumption and Replication:
  - Sink connectors consume from the topic and create replicas in other database systems.
  - Stream processors can replicate the database in their cache.

#### Use Case Focus

This technique supports real-time processing by allowing the replication of the original OLTP database in downstream data stores or stream processing engines without relying on batch processing.

### Push Versus Pull Queries

#### Chipmunk Analogy Extension

- Push Query (Simon): Simon watches for changes in the nut count asynchronously and updates the box with the results. This represents a push query that runs without client intervention.
- Pull Query (Alvin): Alvin retrieves the results from the box synchronously when queried by a client, representing a pull query.

#### Trade-offs Between Push and Pull Queries

- Push Queries:
  - Lower latency, ideal for applications needing real-time results.
  - Limited flexibility; clients can't ask complex questions (e.g., averages, joins).

- Pull Queries:
  - More flexible, allowing for complex queries and deeper insights.
  - Higher latency due to the additional processing required.

#### Balancing Push and Pull Queries

- Materialized Views:
  - Serve as a balance between the heavy lifting of push queries and the flexibility of pull queries.
  - Adjusting the balance:
    - Moving towards push queries improves performance but reduces flexibility.
    - Moving towards pull queries increases flexibility but at the cost of higher latency.

![image](https://github.com/user-attachments/assets/33143fd4-99ce-48cc-8638-7293c64a9d5b)
![image](https://github.com/user-attachments/assets/d9a95a03-d850-4196-be11-7a024b43402c)

#### Combining Flexibility and Low Latency

- Materialized Views with WAL Emission:
  - A client submits a push query, which creates a materialized view.
  - The client subscribes to changes in the materialized view, receiving updates in real-time.
  - This approach allows for ad hoc queries with low latency, without needing separate push and pull queries.

#### Challenges and Solution

- Current Dilemma: Push and pull queries often run in separate systems and are authored by different teams (streaming engineers vs. analysts).
- Solution: Streaming databases that combine stream processing and databases, using the same SQL engine for both real-time and stored data.

![image](https://github.com/user-attachments/assets/c28456ff-5b1b-4e89-98cb-4f3c34bcf100)

The above shows the path by which data in an OLTP database travels to an RTOLAP system for serving to a client. Let’s look closer at this architecture:

1. The entities are represented as tables in the OLTP database following domain-driven design.
2. The application inserts, updates, or deletes records in the table. These changes are recorded in the database WAL.
3. A CDC connector reads the WAL and writes the changes to a topic in a streaming platform. The streaming platform externalizes the OLTP WAL by publishing the changes into topics/partitions that mimic the WAL construct. These can be read by consumers to build replicas of the tables from the original OLTP database.
4. The stream processor is one such system that reads the topic and builds internal replicas of tables by using materialized views. As the materialized view gets updated asynchronously, it outputs its changes into another topic.
5. The RTOLAP data store reads the topic that contains the output of the materialized view and optimizes the data for analytical queries.

### CDC and Upsert

The term upsert is a portmanteau of the words update and insert to describe the logic an application employs when inserting and/or updating a database table. Upsert describes a logic that involves an application checking to see if a record exists in a database table. If the record exists by searching for its primary key, the record then invokes an update statement. Otherwise, if the record does not exist, the application invokes an insert statement to add the record to the table.

Upsert operations can indirectly improve select query performance and accuracy in certain scenarios. While upserts themselves are primarily focused on data modification, they can have positive impacts on select query performance and accuracy by maintaining data integrity and optimizing data storage. Here’s how upserts can contribute to these improvements:

Data integrity and accuracy
* Upserts help maintain data integrity by preventing duplicate records and ensuring the data is accurate and consistent. When select queries retrieve data from a database with proper upsert operations, they are more likely to return accurate and reliable information.

Simplified pull queries
* Selecting from a table with proper upsert operations simplifies the queries upon lookup. Having to perform deduplication or filtering for the latest records complicates the SQL and adds latency to its execution.

Upsert operations, by definition, only support inserts and updates, deletes tend to be omitted. Some systems will implement upsert to also include delete logic. Others, like Apache Pinot, will only flag a deleted record so that its previous versions can be recovered. In these cases, it’s important to use the RTOLAP implementation of upsert, which requires the RTOLAP to read directly from the output topic. Some RTOLAPs may not expose the delete feature, and the work would have to be done in the stream processor.

In summary, there are two locations where the upsert logic can be implemented—in the RTOLAP system or the stream processor. The simpler and preferred approach is to have the RTOLAP read from the output topic and apply the upsert logic itself. The output topic also provides a buffer in cases where the stream processor produces data faster than the RTOLAP can consume.

Upsert highlights the pain of having two real-time systems grapple over or dodge ownership of such complex logic. These pains will create further contention between data engineers and analytical end users.

### Joining Streams

#### Stream Types and Constructs

- Append-Only Streams:
  - Streams where only inserts are allowed (e.g., click events).
  - Represented as append tables in streaming systems, not backed by a state store.
  - Not stored in materialized views due to potential storage issues from continuous growth.

- Change Streams:
  - Streams containing change events (e.g., inserts, updates, deletes).
  - Represented as change tables in streaming systems, backed by a state store (materialized view).

#### Streaming Platforms: Topic Types

- Append Topics:Contain append-only data.
- Change Topics: Contain change events or CDC events (sometimes referred to as "table topics" in Kafka).

#### Joining Streams and SQL

- SQL is used to define joins and transformations, making it easier to combine streams and databases.
- Using the same SQL engine for both data in motion (streams) and data at rest (databases) integrates stream processing and database functionalities, leading to the concept of a streaming database

#### Clickstream Use Case

![image](https://github.com/user-attachments/assets/39bff41d-620b-498b-8f6c-7901dcfee674)

1. A customer updates their information.

  * a: The information is saved in an OLTP database.

  * b: A CDC process runs on the OLTP database, capturing changes to the CUSTOMERS table and writing them into a CDC topic. This topic is a compacted topic that can be considered a replica of the CUSTOMERS table. This will allow for other systems to build their replicas of the CUSTOMERS table.

2. The same customer clicks on a product on an e-commerce application.

3. The click event is written into a topic. We don’t write click events into an OLTP database because click events are only inserts. Capturing them in an OLTP database might eventually cause the database to run out of storage.

4. The stream processor reads from the CDC and click topics.

  * a: These are the messages from the CUSTOMERS change table topic in the stream processor. They are stored in a state store whose size depends on the window size (or, in the case of, for example, Kafka Streams or ksqlDB, fully stored in a KTable).

  * b: These are the messages from the CLICK_EVENTS append table topic in the stream processor.

  * c: A left-join is executed between the CLICK_EVENTS append table messages and the CUSTOMERS change table messages. The result of the join is CLICK_EVENTS enriched with their corresponding CUSTOMER information (if it exists).

5. The stream processor writes its output to the topics below.

  * a: This is a change topic and contains the CDC CUSTOMER changes. This would be a redundant topic since the topic in 1b contains the same data. We keep it here to keep the diagram balanced.

  * b: This is an append topic that contains the original CLICK_EVENT data enriched with the CUSTOMER data.

6. Topics are pulled into the RTOLAP data store for real-time serving.

  * a: This is a replica of the original CUSTOMERS table in the OLTP database and built from the change topic.

  * b: This contains the enriched CLICK_EVENTS data.

7. The user invokes queries against the RTOLAP data store.

  * a: The user can query the CUSTOMERS table directly.

  * b: The user can query the enriched CLICK_EVENTS data without having to join the data themselves, as the join has already been done in the stream processor.

## Chapter 5: Introduction to Streaming Databases****

### Identifying the Streaming Database

![image](https://github.com/user-attachments/assets/69637b39-3299-47ef-a102-2df4b9e67a28)

* The database can be one of the three types of data stores that we talked about so far: OLTP, RTOLAP, and the internal state stores in a stream processor. The differences between them dictate how data is stored and queried.
* The topic is a construct that mimics the WAL in an OLTP database. Topics publish streams of data to other databases and stream processors.
* Stream processors are the applications that transform streams of data. They hold an internal state store.
* The materialized view is a process that precomputes a result and stores it in a database. Materialized views are created in a database or stream processor, both of which need to have a persistence layer.

### SQL Expressivity

SQL expressivity refers to how well SQL can succinctly and effectively represent complex data manipulations and queries with concise syntax. It measures SQL's ability to capture the intent of a query or operation in a way that is easy to understand and maintain.

#### Challenges in Merging SQL Engines

Merging SQL engines between a **stream processor** and an **OLAP/OLTP database** can introduce several challenges due to differences in design, use cases, and performance characteristics:

1. Performance Mismatch: Stream processors handle high-velocity, real-time data, while OLAP databases focus on complex analytical queries on historical data. A combined SQL engine might struggle to balance these conflicting requirements.

2. Latency: Stream processing demands low latency, whereas OLAP databases prioritize query optimization. Achieving both in a single engine is challenging.

3. Resource Allocation: Stream processors need resources for real-time data, potentially causing contention with OLAP queries, which require significant compute and memory resources.

4. Data Modeling Differences: Stream processors work with raw or semistructured data, while OLAP databases need structured, preprocessed data. Merging the engines may cause conflicts in data modeling.

5. Data Consistency: Stream processors operate on data in motion, and OLAP databases work with data at rest. Ensuring consistency across these states can be complex.

6. Complexity: Combining stream processing and OLAP capabilities can lead to increased system complexity, affecting maintainability, debugging, and stability.

7. Data Volume and Retention: Stream processors typically have shorter data retention due to high volume, while OLAP databases store data long-term. Managing retention and integration is challenging.

8. Query Optimizations: OLAP databases offer advanced query optimizations that stream processors may lack, leading to suboptimal performance for analytical queries.

9. Schema Evolution: Stream processors may handle schema evolution more flexibly than OLAP databases, which require well-defined schemas. This discrepancy can create issues when merging engines.

10. Maintenance and Updates: Managing a combined SQL engine that handles both streaming and OLAP workloads is more challenging, as updates must cater to both use cases.

#### Mitigating Pitfalls

To mitigate these challenges, careful architectural planning, thorough performance testing, and a deep understanding of specific use cases are essential.

#### Merging OLTP and Stream Processor SQL Engines

Merging SQL engines between **OLTP** and a stream processor can be easier compared to OLAP due to the following shared characteristics:

- Data Format: OLTP databases use a row-based model, aligning with the row-based format of stream processors, facilitating smoother integration.
- Real-Time Nature: Both systems handle real-time data, simplifying SQL engine merging.
- Transaction Handling: Both involve transactional processing, aiding in better integration for data consistency and updates.
- Event-Driven: Stream processors' event-driven nature aligns with real-time updates in OLTP databases, making integration easier.

### Streaming Debuggability

Data engineers need to verify the logic of their SQL when writing data pipelines. Debugging becomes challenging when materialized views span across multiple distributed systems (stream processor, topic, and OLAP). Although theoretically possible by analyzing input and output topics alone, effective debugging requires examining both topics and external databases simultaneously.

#### Advantages of Streaming Databases for Debugging

Streaming databases simplify debugging compared to classical stream processors by offering advanced materialized views. These views are persisted in a row-based or column-based store, making it easier to verify results in one place. Additionally, indexed data in streaming databases allows for faster, more complex ad hoc queries. In contrast, stream processors like Flink require output to be written to a database before validating results through queries.

#### Key Advantages:
1. Familiar SQL Interface: Streaming databases often use SQL-like languages, making debugging more straightforward for those familiar with SQL.
2. Simpler Logic: Higher-level abstractions simplify stream processing tasks, making the logic easier to debug.
3. Integrated Ecosystem: These databases combine stream processing and storage in one system, providing a holistic view of the data pipeline and aiding in debugging.
4. Built-in Optimizations: Common stream processing patterns are often optimized, improving performance and reducing the need for complex debugging.
5. Easier Deployment: Designed for ease of deployment, streaming databases reduce potential deployment-related debugging issues.

#### Limitations of SQL

While SQL offers abstraction, it may not always be the best tool for debugging in performance-critical situations. Lower-level domain-specific languages (DSLs) like Kafka Streams and Flink's DataStream API offer more expressibility. Although SQL-based streaming databases support user-defined functions (UDFs) to extend functionality, they have limitations.

Moreover, inspecting the actual execution plan derived by stream processing systems (e.g., aggregation operators in a `GROUP BY` statement) is still a challenge, as tooling for this purpose is underdeveloped.

### Streaming Database Implementations

| Name        | License                      | State Store Implementation        | Use Cases                             |
|-------------|------------------------------|----------------------------------|---------------------------------------|
| ksqlDB       | Confluent Community License   | RocksDB (LSM tree key-value storage) | CQRS, push queries                    |
| RisingWave   | Apache 2                     | Row-based                         | CQRS, push queries, single row lookups|
| Materialize  | Business Source License (BSL) | Row-based                         | CQRS, push queries, single row lookups|
| Timeplus (Proton) | Apache 2                  | Column-based                      | Analytical push and pull queries      |


### Streaming Database Architecture

![image](https://github.com/user-attachments/assets/ce481e7c-e6cb-4505-bf0a-6898d2ae5ed9)

### ELT with Streaming Databases

ELT (extract, load, transform) data pipelines do not support real-time use cases because the transformation occurs in the destination database. The database, in this case, places the streaming data at rest, which forces batch semantics for all downstream processing.

However, if the destination database using ELT is a streaming database, then the pipeline can be considered still in real time. This integration between the “loading” and “transformation” parts of ELT is mediated by a topic on a streaming platform from which the streaming database consumes the data.

There is a large ecosystem that supports ELT solutions, for example, dbt. In combination with streaming databases, these tools can support real-time ELT for the first time. And because streaming databases behave, on the surface, like databases and not so much like a stream processor, ELT with streaming databases can actually be implemented by the same teams who have previously worked on ELT in a data warehouse. In this vein, a lot of ELT jobs that now run later in the pipeline (in the data warehouse or lakehouse) can be moved to the real-time streaming layer.

### Summary of Streaming Databases

The term **streaming database** represents a convergence of stream processing and traditional databases. While databases are typically associated with batch processing and data at rest, streaming databases integrate streaming and batching, handling both data in motion and data at rest.

#### Key Concepts

- Convergence: Streaming databases merge the concepts of streaming and batch processing, reintroducing elements like the Write-Ahead Log (WAL) and materialized views into the database environment.

- SQL Engines: Traditional database SQL engines handle data at rest. Streaming databases allow materialized views to run asynchronously, enabling SQL to process both data at rest and in motion.

- Persistence Layer:
  - ksqlDB: Uses RocksDB with primary key indexes.
  - Newer Databases: Implement a more database-like persistence layer for efficient querying.

- Query Types:
  - Push Queries: Handled by the streaming component of the database.
  - Pull Queries: Executed by the database component. The type of storage (columnar vs. row-based) affects the efficiency of pull queries:
    - Columnar Storage: Efficient for analytical queries and fast aggregations.
    - Row-Based Storage: Suited for simple lookups and point queries.

#### Spectrum of Streaming Databases

- Row-Based Databases: Typically handle pull queries driven by applications or events.
- Column-Based Databases: Often support pull queries invoked by humans or dashboards.

![image](https://github.com/user-attachments/assets/510ff81d-e8d5-4770-86d1-40949f9bb06b)

#### Consistency

A crucial property of streaming databases is consistency. This ensures that data adheres to predefined rules and constraints, maintaining accuracy and reliability throughout transactions. Consistency guarantees that each transaction brings the database from one valid state to another without violating integrity rules.

In the next chapter, we will explore the importance of consistency in both streaming databases and stream processors.



















