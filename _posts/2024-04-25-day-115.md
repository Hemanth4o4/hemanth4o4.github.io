---
layout: post
title: (Day 115) Exploring HuggingFace's capabilities and submitting 3rd homework from the ML with Graphs course
categories: [gnn,applying-knowledge,nlp]
---

## Hello :) Today is Day 115!
A quick summary of today:
* submitted 3rd homework of XCS224W:ML with Graphs
* explored different huggingface capabilities with [DeepLearning.AI](https://www.deeplearning.ai/short-courses/open-source-models-hugging-face/)


As for the homework, we are not allowed to share anything from it. But I can happily share

![](https://blogger.googleusercontent.com/img/a/AVvXsEhE9M4fc_R3825bvKN5XTtSIYvAaC6fm4D0RaMPueC18hTpIbVXeTOvkpaWd9LbFdoZ_8gUjxRkkhtdMO6DXVDwy8jJO735lYvrjNBl8i8Fw4FN0KB_3MzuzsuPCoqRtUmvxHbOrDMlaeTvTCfbRO9R0xDvpOSxz5FYDKe8ZI3ynGrnCa4bYx4fvUZdr3LA)

I got full marks ^^

As for the huggingface tutorial

It showcased the different type of models that were available. Below is a summary. 

**Building a chat pipeline**

![](https://blogger.googleusercontent.com/img/a/AVvXsEh97_ytHjDqMXBl9fb6MlSNVoGREpsJ28IVS81_mJAkLdc2iPt-nhKtX_Tn9ZKWUPq0-I1Maz1IHVfo_mulLU9JwDQvhKGiLbw-U6cr5-MdaAmAm-ZvFzs5Z7TMSx9G5sBILjkAms1axNqTPmVMmni3j5VhuHpk-8KpAtLuBFiN3JXkNiIWLYd_GVQB8Fln)
![](https://blogger.googleusercontent.com/img/a/AVvXsEjg5mVtJmv-icGcjQv71Mh5_ZpT76NYrq71BbOvbFTytw-9OJDO1Kwddnk5r-N9p6mkED7Ukp-OAOegnSG32ekEna4CiWnd_Zh2TYurFMl2472ZC_hisLgE1UTMZGkYKBhgb-busN0pw0YSdhg8-MPcWGklygmasUd6SJNp2-1GpY1T8rZHQBS9DSsYtW-g)

**Text translation**

![](https://blogger.googleusercontent.com/img/a/AVvXsEg3On7KwLvgyHO4LKfD8rdi8pPALlHh2s9x-5rf_J_KFffgFdTx0c_tYRdwalqdK2buS-DUPY9GrbOV-FbprvzEQNe4RkZ66kgz6Q3Vq7ECfmUMw96QSG_GfgArVPaLroMOHt1u7t8ud-7mn-DzMOLlUSrIpMURLIQRZDpfJWT3ie0OnJtVYJXKln6Vci1E)

**Text summarisation**

![](https://blogger.googleusercontent.com/img/a/AVvXsEgXkMKCk-fb70BQB1xejafX9V_jv661LKa44irFs84BTo4tQgZPARM28dgj3lQLzE5NcRI8N34AHJFWuyU10xfNuw7EmId0sOj1JhRi7yTryhk1nMeXwnpCfqG2Ye2hLbvWZreY32oBJGgtLDRaWioJbGEbWcZxd0CRdtD03v_0Mquc8-lVYSxw-UezusdX)

**Zero-shot audio classifier**

![](https://blogger.googleusercontent.com/img/a/AVvXsEhjrhgwuMHi25WBia7E0RYKKqcHblA-w6zjRuB7-fWfVTrIqLJtgK_3Y1_WJdVNjF8xMANHJhMcP5eFfTElCKge2CYZHm6-t9y3TMBkuFztzuvfmDFK-YCwM7zd-fNHuWyYQg_2Fh4Zn2P2ZFDd-ILWcE8Vj0k5nr0pGTjDKdBmFR0kmwz9qA_D3yERgh0X)

Apparently, the model seens the audio differently -  1 second of high resolution audio appears to the model as if it is 12 seconds of audio.

![](https://blogger.googleusercontent.com/img/a/AVvXsEgT40BNI74q6Yl8FLQx61TyQlscdUWV0A4BhuwItuE8mPezW0oxZ6r_9GUls3cfBwRDfM0hrC-2cfx7dXylcN623dx45dKyw4MHuRr8vxnVovZ7V8glKdluz4be6SPwFm5JjYrUiF8mD7rwo4ucs5yUCwk2maqLVNYTYaTyzkngb5JEpkbkGOKVFwtuCFmI)

**Text to speech**

![](https://blogger.googleusercontent.com/img/a/AVvXsEiaf41GH8XAs3FSmi3YSArbeiXruOd4n9k_O1dSiTLRz9wPyMGFvvHZqtgp6V0Mkar7wq_2B7aQ8HVVYPgUdPBn5_A9achbF3n4cYJNP6UiTNromurX4JB3ZUzPZ12ZVd081hvqSPLRYXwmFWLfbLMD9MvSWTGc1sqCTRkMR5t33LzT4oIekj6q2iH5KGgC)

**Object detection**

(code before the pic: od_pipe = pipeline("object-detection", "./models/facebook/detr-resnet-50"))

![](https://blogger.googleusercontent.com/img/a/AVvXsEhFWcSfU2JwgXuUKWfM1G_vB_gMm01_aPfMFDMtlaLeFRMS5gev0j7KFVhOI2IwoA8C-eh8kKko8GPCmuCM__B2fWbmukjRF2IyPhDJaz9_UZHN5Ev21EBfaBHhY_Tuml9jC3rZW_Ebu8pfe4fN_9QXv6qiF89pRVLZdzC8FDix6AdVmyqmdt_6DwdCQsSf)
![](https://blogger.googleusercontent.com/img/a/AVvXsEg_YmMNUwcNnWt5KG0LbSu6tvaAT2EEEg_rYxTKjBaWn7B_akQDRnCkXJBeli1nufuTjllorS0i5ATw_FI8BRjS_uBjoSNYlSyP6tnnXevk8hAN2QQU1JpTgwAceOuINHsGg48M-GrJTM6_4F4LQ-UIh7JUSQK9hEuCdSnLDewr5Hllakorit8JPqDfbSTd)

We can use gradio as a sample interface

![](https://blogger.googleusercontent.com/img/a/AVvXsEhiyTQxIoZjDse5q5QR1cb27W16n6oltgUOIKLsQRvMFN7xxr5zFxfYfz-gMsiNeCKWRHbvi1hPOlB9AvBDyy2z0FOK4kZ7HaAtMtcGVidPE1Fkk4bVWJ9Yt1osCNmB32-9iUfJfdlj9qS_BS0pw0_BbGbdTqv9fEJTglaM7dHAyvxHso1cl-LOo53I5ImC)

I passed a picture of mine to check haha

![](https://blogger.googleusercontent.com/img/a/AVvXsEihVyHEfUh4hH8woAdg_m6m44el9oXjBNjdLm8g_ASSKajcpUeaQNsIKCaT3p0BurOsJ_ZykbL7tfD7-2XUJv5vJfxVHO2bPs7bDUn9syV2XEdYqcAhKmo2jQ_uV5kOK31ld1RFQH712hRkS-kR22aJYOv7asgdTIMy8l-LaZxQ6qwiVGIGkEEhdE3m9GIp)

We can also get natural language descriptions

![](https://blogger.googleusercontent.com/img/a/AVvXsEiWO2QCocaHIY1pvywHPB-1cN99kZnAksDGyYDqFSGnr5-IwKYQaSPeiHs7iXvDR1YEmpwhqsppm55sUvMNvCLxrQTytJfU74nBDwP4AQCA5mIrsD8ceCVQpXLmb_Xwxp7p3V_kTYsXyZOQ-2GibBmX1MZ_dE9uYDNzw_0zeictZNJOZYb0XU_kjcadCLNx)

**Image captioning**

![](https://blogger.googleusercontent.com/img/a/AVvXsEhh9wt5QNr3426KXRCVQmr-BrUqhN8xkcsbLSjCQVn7i0jlzDqaT40S6zDM0LlwgagbWV-cC6rZfcHzDylp-RQZ5z_tXfYdzmi8hF4bV-DksvV4VLefCMjcBXicaUUgFdPYB4jMJAF400OnciITfKEPd73sJ8-W1CjE9XlwaU3iyFRbNvGsTXfH9cUd3_HX)

Example image

![](https://blogger.googleusercontent.com/img/a/AVvXsEiAj8b-nAFN-wddmNBrMXJZm3kF2HDvEO5ACtbdx0295mS5bWflhiEIQflYk-NV-eaJ3_knXtTUPxP2sLAovx3NquugP14PFpEFp9W-2mzUQckmzw8wHuwfqAcMhHAHjg2uP9iPQD5ZQh9UdnALSUG82KfI0WrDy6se4yqSCChGTm6F7dUwun8nv64ZGTJH)
![](https://blogger.googleusercontent.com/img/a/AVvXsEhY3c2nuHP4P9fH1xEMtuoHAS-BkcvtYNcOZ1ljgMcNPOjz1nWX_w8wbOPMIZ4v4F_YTKOZUE0ontTHdR2puldOhmgpycCwLkaI-bQfCGX2LSwakr2ZgHzXcET-mvqRDwKjEPLItv13Nh0xmaqRBG2AFhSZ2gfQ9ywarg-cn3Zo6-nq791E0MRvYF5uhLNM)

**Using the dog and woman pic again for multimodal QA**

![](https://blogger.googleusercontent.com/img/a/AVvXsEjFuVOIUDgOyOQihNA9A3PUmtRnkQfS8eIZXfT6EFctJo_Co8endo-Hp2Djlo_6vXo5cH2KlpqPCA1ROmbSv9RyOLcv1uW2weyovyYmyn9Im3Wa3eG1XUgtpoyWpU8ZFxShIIIajv8PpmCd1oUO_fCV83448I7SQ31dnVcvXGZ6sGUoB3WPXkFGQVh9JncN)
![](https://blogger.googleusercontent.com/img/a/AVvXsEgdUO4Kw-au4-if9oQg2aDdib9wRIcD4u4HINyg1ToGAVhcLAAaQLAypG2kkvsp9ptTFXNAExgV8N6L7Ido5nc8d10bpLev0rhv0CopfrghbtQpX8i8uRWUcokjW6jNUsdXMhlR5FtQTSSZTDUHT9S9VCFRRh6akiOlbGGzYPOZQZN0_aYYg9zgkg3Dv-Lr)

**Zero-shot image classification**

![](https://blogger.googleusercontent.com/img/a/AVvXsEgPYeMDEmeKVPiDr0BIOCfDOvPcyoYsV8i4EAjHqzTAjTXPie733C1Oj4ynRy9GNUH4oPJs2otf9IbAAO6VKTmPrII25f8FU6PyjSUdWp5nc4j1Jwch6u7n5sTeoBOWd-_FN5OBXyvPZOPevcoD0YFyLp30KHw7nC5yYz08q7TVzwBWX_0Vr7hg-SQx0COt)
![](https://blogger.googleusercontent.com/img/a/AVvXsEjbLVmOA_bp2LYt1UBRfGvA94zhwMa91y9R0KA-Gy5xWWKoD-uXBiidg4G6zXG4XP7LBGa4F_sf-E02k-_QiZYVeTJFtDX7AYQStKcH5iSOgCwB81__GRS7VOfS3Jj_ZfBORkWiIRxOt4VGL531C-auxP6kOVbu9-VWIbjx7bGVUdwYSeyU6EypjrdaZxom)

Giving labels: a photo of a cat, and a photo of a dog, we can get the models' probs for each label.

![](https://blogger.googleusercontent.com/img/a/AVvXsEhiWtEW5Q57Avylo0IwJ2ivYFH9CmiJBT32N6YKN7IOTZEpQh8zWKGHL6zDHwXBhCmJUDwN_h_gTeMcEbSt7ocfJGbkKkGGKHa3SMiuPg17V8vY-gPpr2f90eoioxGAAIN-1y3jXqa9Im377RzjMcCW0HYvLI27nN9McI-oycW72TTpl3u00kA0MKa776IL)
![](https://blogger.googleusercontent.com/img/a/AVvXsEjokzDB5h2wz7Yx0eV33ZcsZGj09usdaCYBjPleu-ookdmaQlD20qfHLYzxnjM0FXfCCbs12Sfa-SSqLtw9ugTtCXqGbG5zDqfvTvJy1AvsKmx9qELqfJgUhpHEq1OG9BA6keRIUj7yq8V5nwF78jEZLX4Pk68CnpDA0jC5kFCAlJWVC8lvoUht_h64HCxq)

**Deploying a model**

On hugging face to deploy a model, we need to:

1. Create space

![](https://blogger.googleusercontent.com/img/a/AVvXsEjrXD7-RJuW5XCUN-0hdUSEDaG-wjggF-vKU5S9YhQnPKxu587Pu_8Rc4k5ONOq6wUuZqX8UB7i8XE8m2FkTKN7_Dar2V20iPBNwO1qvqTo4Z0cKluv5Ez9SRhquFU5swI4LCIWZb9t_-HA8pMQ-RK2wzbOumcuomsAFzYxi4hPfUQ_eP8S62RXu6xJLOLm)

2. Create requirements.txt and app.py files

![](https://blogger.googleusercontent.com/img/a/AVvXsEh_tm4i9pvsJN5Oj0xEZLV-MDefZ7ChqMReVosxKHAV1vuwP6bW6KdzEyM-_fq4pTfPMIUPHQL5cNQJEGvBPCRI10ZPItMXqLp2Ng_NIeyh9ucKBKSrJBADAFcpvjHWP7Bws3WLdIqpGaiD2gfwkV58ue2031_-iPlWGzohHZZGwpGMX22O8IglT7Aa5NHl)
![](https://blogger.googleusercontent.com/img/a/AVvXsEidJZGQSNZhwHqCbwvJ81ns5qSFdzFnDTlxRiBv1Cq7__2rl_7noqhKpUwWPGC-NE8bsIO1ULM7Are8uC1cwko7EbMUfaj6FKi19hzff9rph4gGSF54bD5nHVGd0UnGlG-4FwzgJha6X6Govh3N2K-YJStjNyMX2kfCasGR_8Jw8dEIkgSbKJRw7907o2Q-)

After I created the two files, after a few minutes, the model was deployed and ready to be used.

In huggingface, the interface shows

![](https://blogger.googleusercontent.com/img/a/AVvXsEhfxErNcC2FpJByH0m6QDBtMcFMebUVlM8V4tC5ax0URERo4VrNYGzi5NMJhy3hbvA8PeQ3UlGMSNLG4nka4dNMh0COqWCDMcwQG018fijQ3Ik5ogcGD5XCIl6p1qw3OQeNTleyjnpC4oFFVkUvWr1cS74j0059opOSWbZjDuJoaFYzhAJ4Qx6t0T1tPqDw)

I tested it with 2 images

![](https://blogger.googleusercontent.com/img/a/AVvXsEi-DnA8lpU0JNheS8J1O4-14DJB1phOGYepOjKF8JA_zIxBa_DIW97fKLBzZ08De4z3CqiSUMRTsipTJt786aTStqanBfdMPmjxq5TNv8SXFZd7HzpZ_1yzQeWH8Y-dvmyteNEoTQd-zpbz0pn_TvhLQVIy2VO20R2fiHYlYTFKnQ2dwspdXovWYcShTqxi)

Output: 'a stone path'

![](https://blogger.googleusercontent.com/img/a/AVvXsEhk-dZhxXHVKEqeTRsryEhMt9hx8CPCMOcPdCe3MbxUZNPEzhxT65Ig74-iwo2dO_jEVtJBDZ6WPQDUvhSxxUkEXM4MIlq9HI6uJscqaT0YnnMo8XUYDm11NCojXi4I1RvXwzEfllucHLMQH2bxFTe0DfyKGper-Rw-5TSOT2YZONiFRGCigTD4RXV1h6AM)

Output: ‘a group of dogs sitting in a row’

It was a useful tutorial showcasing the code needed to run various open source models and perform various tasks. 

That is all for today!

See you tomorrow :) 
