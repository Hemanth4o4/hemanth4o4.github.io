---
layout: post
title: (Day 110) Learning about Graph Transformers
categories: [gnn,theory]
---

## Hello :) Today is Day 110!
A quick summary of today:
* watched a hackaton held by the best CS high school in Bulgaria - TUES
* learned about Graph Transformers ([slides](https://web.stanford.edu/class/cs224w/slides/14-graph-transformer.pdf), [blog](https://thegradient.pub/transformers-are-graph-neural-networks/))

chnical School Electronic Systems (TUES) unfortunately does not have an english website. [This](https://www.youtube.com/@TUES/videos) is a link to their youtube channel where student video projects are uploaded.

Every year, they hold a hackaton for ~3 days, and I saw a recording of this year's from last month. Understandably many projects involve AI, and one of the projects from a team of school graduates (my best friend participates in it. I saw him randomly, I did not know he participated :D) is below

![image](https://github.com/user-attachments/assets/c262c74b-576b-4150-96e6-38d0288907a0)

Essentially, it is a knowledge graph that takes laws, bills and policies and a user can browse through nodes (different laws), find connections and references to other nodes, and also on the right ask questions, and when a result is given, the user is also pointed to the nodes that refer to the GPT response. 

There were many projects in the finals (it was a ~10hr stream) but I think the topic was to make projects related to the law (like my friend) or to help the environment because there was one that helps to detect when a body of water is clean, or with petrol/other chemicals.

### As for Graph Transformers

I checked my XCS224W: ML with Graphs course and the topic of Graph Transformers was not included, so I went to check the most recent official CS224W: ML with Graphs course and there were slides to it, so I decided to go over them and learn about GTs.

Covered topics:

Transformers vs GNNs, self-attention vs message passing, designing graph transformers (nodes as tokens, positional encoding, edge features in self-attention), Laplacian positional encodings, eigenvector sign ambiguity, SignNet

![image](https://github.com/user-attachments/assets/3c592f25-8412-420e-9ebc-a46f54ec6969)
![image](https://github.com/user-attachments/assets/0360edfb-85be-4d80-926c-ecb8c78526e9)
![image](https://github.com/user-attachments/assets/a5cea408-a307-4e4c-bf00-ca810ab0c1b4)
![image](https://github.com/user-attachments/assets/b6370f6f-0819-4f69-be53-a63dcb35607a)
![image](https://github.com/user-attachments/assets/f7f9aeca-8d4b-4806-a840-f29ec9c4f42f)
![image](https://blogger.googleusercontent.com/img/a/AVvXsEjZdomXhigdDmCDoIYivQA2XsoxOj8jMDkNA-j1MZtZcF-sf-XiappEjAMsY_uF7kau9t_VFSrsXWJTyF3Now80G7vfKVBgDTDxsv5P9rkrd4PKmi-sbvHBSybLrORmvTtVFSd7LRHUYbWJ7rcegMXnwbDVHOU9Fz0ayUV1rshke6Z8ERDOY59DmnOz5FWZ)
![image](https://blogger.googleusercontent.com/img/a/AVvXsEiy1QyXJO026_3ZC1zmuGo5b4oZkmja5WUeMh0JMzY-0sCjvC47RZYXgLr-VPrKwDYcz7eUFz49g9EwjTb2nAnNYrRhe7rZ2dkseM8sWkJ4tMPIgyY9_qpjgAincqWwtl6CGk7m9W4LyroTka96BlQX8NZ16hM0fgdh7BRcljejYGXOoln7hGOONKsr4lgM)

That is all for today!

See you tomorrow :)
