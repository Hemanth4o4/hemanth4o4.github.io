---
layout: post
title: (Day 253) Designing effective ML monitoring with EvidentlyAI Part 2
categories: [nlp,mlops]
---

# Hello :) Today is Day 253!
A quick summary of today:
* finished module 4 of evidenyliAI's course


## 4.4. How to choose a reference dataset in ML monitoring

### Why use a reference dataset?

There are two main uses for a reference dataset.

1. You can use it to derive test conditions automatically, saving time and effort in setting up tests manually.

For example, you can use a reference dataset to generate conditions for data quality checks (to track feature ranges, share of missing values, etc.) and model quality checks (to keep tabs on metrics like precision and accuracy) by passing a previous batch of data as a reference.

2. You can use a reference dataset as a baseline to detect data and prediction drift in production by comparing new data distributions against reference distributions.

Reference dataset also can be used to detect training-serving skew as it provides a baseline to detect changes between training and production data.

![image](https://github.com/user-attachments/assets/2747fea3-49e7-4554-90a8-e3d74efceeff)

#### What makes a good reference dataset?

Characteristics of a good reference dataset:

* Reflects realistic data patterns, including cycles and seasonality.

* Contains a large enough sample to derive meaningful statistics.

* Includes realistic scenarios (e.g., sensor outages) to validate against new data.

What a reference dataset is not:

* It is not the same as a training dataset. You can sometimes choose training data to be your reference, but they are not synonymous.

* It is not a “golden dataset,” which serves a different purpose.

#### Using training data as a reference

![image](https://github.com/user-attachments/assets/ed1a72ac-0152-4f35-ba2a-06d3bfbd67c0)

![image](https://github.com/user-attachments/assets/a6947c38-8c77-4d39-b48a-551079ba505a)

![image](https://github.com/user-attachments/assets/ddcb82c4-759a-433c-a922-6efb10063b90)

![image](https://github.com/user-attachments/assets/8c46691f-be96-4775-9273-d181286cbfe9)

![image](https://github.com/user-attachments/assets/8dbfd2b4-9452-490c-97fe-9a048017663f)

#### Summing up

* There is no “universal” reference dataset. It should be tailored to the specific use case and expectations of similarity to current data.

* Hold-out validation data is preferred over training data for creating reference datasets, especially for drift detection. Use training data only if there is nothing else.

* It is crucial to account for seasonality and historical patterns when choosing the reference dataset to ensure that it accurately represents the expected variations in data.

* Historical data is a valuable resource for informing reference dataset selection.

## 4.5. Custom metrics in ML monitoring

### Types of custom metrics


While there is no strict division between “standard” and “custom” metrics, there is some consensus on evaluating, for example, classification model quality using metrics like precision and recall. They are fairly “standard.”

However, you often need to implement “custom” metrics to reflect specific aspects of model performance. They typically refer to business objectives or domain requirements and help capture the impact of an ML model within its operational context.

Here are some examples.

Business and product KPIs (or proxies). These metrics are aligned with key performance indicators that reflect the business goals and product performance.

Examples include:

* Manufacturing optimization: raw materials saved.

* Chatbots: number of successful chat completions.

* Fraud detection: number of detected fraud cases over $50,000.

* Recommender systems: share of recommendation blocks without clicks.

![image](https://github.com/user-attachments/assets/97a38da2-8490-480b-b198-d58311f1b0f2)

![image](https://github.com/user-attachments/assets/19bc947a-b9a0-4fd9-8945-9be0d52b3495)

![image](https://github.com/user-attachments/assets/87c97cc5-14fb-4a89-b59f-afc874cc582c)

## 4.7. How to choose the ML monitoring deployment architecture

![image](https://github.com/user-attachments/assets/1fee27fd-9faf-4fa6-b32e-ce54db8015a2)

![image](https://github.com/user-attachments/assets/4a3efca2-9a69-466d-91b2-c21cff1e27a8)

![image](https://github.com/user-attachments/assets/120dc5c0-b33e-4d28-a486-d3dc5e462472)

![image](https://github.com/user-attachments/assets/386e1620-18c5-4652-9880-0525c1fa4751)

![image](https://github.com/user-attachments/assets/97743eb8-2053-4a11-aedb-6acde07bcd64)

![image](https://github.com/user-attachments/assets/d5e9f214-4fa2-475e-a58f-4dd80afdd6f7)

![image](https://github.com/user-attachments/assets/94dd2464-64a2-441d-8516-ebbf30a320d7)

![image](https://github.com/user-attachments/assets/b4504571-d446-4201-8b2e-dde2efe0d64a)

---

I had my exam today, but after it I am glad I found time to complete this module.

That is all for today!

See you tomorrow :)
