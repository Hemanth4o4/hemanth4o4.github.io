---
layout: post
title: (Day 65) Stanford CS224N (NLP with DL) - Multimodal DL and Model analysis and explanation
categories: [nlp,theory]
---

## Hello :) Today is Day 65!
A quick summary of today:
* [Lecture 16](https://youtu.be/5vfIT5LOkR0?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4): Multimodal deep learning
* [Lecture 17](https://youtu.be/f_qmSSBWV_E?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4): Model analysis and explanation
* (just watched) Lecture [18](https://youtu.be/2t7Q9WVUaf8?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4) and [19](https://youtu.be/cd3pRpEtjLs?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4): Future of NLP and Model Interpretability & Editing

First I will share my notes, and then just a quick summary and thoughts on the course.

**Lecture 16: Multimodal deep learning**

![image](https://github.com/user-attachments/assets/eb163233-4173-4231-aedf-4c55bad95445)
![image](https://github.com/user-attachments/assets/58d4c876-bb55-459e-8adc-3a20272c4cda)
![image](https://github.com/user-attachments/assets/fb612ca6-8119-4249-a156-00c4607f198a)
![image](https://github.com/user-attachments/assets/dbc51024-2933-4b93-9569-2865c5a7bba9)
![image](https://github.com/user-attachments/assets/70939ef3-8093-485b-a4bc-e8dc605f4f2d)
![image](https://github.com/user-attachments/assets/cb7de133-9884-4494-bbc8-0787ac9fe112)
![image](https://github.com/user-attachments/assets/8cc6fc3d-6c10-4991-af8d-80190c193bf4)
![image](https://github.com/user-attachments/assets/8e4274cf-bebd-4b56-9543-f006508241ab)
![image](https://github.com/user-attachments/assets/6bf6f95c-7f01-472a-b178-5afa26264663)

I  got a lot of research papers to read from this lecture. Two are:
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; and Learning Transferable Visual Models From Natural Language Supervision.

**Lecture 17: Model analysis and explanation**

![image](https://github.com/user-attachments/assets/f988e8f6-1c1f-4d74-9a89-f691b6299eeb)
![image](https://github.com/user-attachments/assets/a93bbc85-5707-41df-8877-4c603b673f58)
![image](https://github.com/user-attachments/assets/879c0bc6-7d05-4575-af92-1b4dc6359c5e)
![image](https://github.com/user-attachments/assets/b539e6bf-10d3-45ba-bf1e-13c93bb2cb77)
![image](https://github.com/user-attachments/assets/3fdb0218-57ea-46d8-96e7-8b2c214614a6)
![image](https://github.com/user-attachments/assets/2ff89a4e-876a-4127-a9f9-180b581e8a58)
![image](https://github.com/user-attachments/assets/e579f5c2-0852-4217-8bf9-98d4b84abe1d)
![image](https://github.com/user-attachments/assets/b96045a4-8ce3-44e7-bf86-13dd2c0c63fc)
![image](https://github.com/user-attachments/assets/23baf3aa-209b-4d06-8844-bdaa36de8dbe)
![image](https://github.com/user-attachments/assets/8d2ec363-7855-41e9-aea4-828e44d303f4)

**Quick summary and thoughts on the course**

![image](https://github.com/user-attachments/assets/9db51c25-ac3d-488d-a25e-edf5c84b47d0)
![](https://blogger.googleusercontent.com/img/a/AVvXsEgNTPcact3KWIxuix3mLLllc5TduuZ2d-9e0UjE-pdDhvefRENGCD6hN0QYlPPA-4obp_WzCgu8I9Ets0SV1txqNa0c9SIzSvlik6EJBSsn6bJphFdjLcRjweMlI3DczvBfaHRH1a0kRGmsDk1MvLkSJg6s-7sMZK4Jb4b9zdd5pGozapOQXroO-rsqq3W3)

I wrote a lot of notes, and I am glad - writing something down with my hand definitely helps me remember it better.

Definitely a very comprehensive course and high quality, and I am glad I decided to commit to doing it. Going back to the start of NLP, through human languages > word vectors > word2vec > seq2seq > RNNs > LSTMs > Transformers, but also learning code generation, natural language generation, how LLMs word - pretraining, finetuning, evaluation metrics. Amazing.  

Maybe someday I might pay for the XCS224N if I have the money, so that I can see lectures about *current* advances in the field. But this course (half of which was from 2021, half from 2023) definitely helped me create a sturd base for my future NLP journey. Thank you to Professor Chris Manning and head TA John Hewitt - they were top-notch ^^



That is all for today!

See you tomorrow :)
