---
layout: post
title: (Day 133) Gathering data for the Scottish dataset project + Factor analysis + Grokking ML + MLxFundamentals Day 4 (2)
categories: [theory,deep-learning,applying-knowledge]
---

## Hello :) Today is Day 133!
A quick summary of today:
* started putting together some audio clips + transcription for the [Scottish dataset project](https://50daysml.blogspot.com/2024/05/day-129-ai-with-scottish-accent-mle.html)
* saw how eigenvalues play a role in factor analysis
* started reading Grokking Machine Learning on Manning.com
* finished [MLx Fundamentals](https://www.oxfordml.school/fundamentals)' final session

### Firstly, about the audio data

My Scottish partner for this project has recorded various phrases in Glaswegian in the past and uploaded them to youtube. Today I did 4 of the 10 videos. 

To cut the clips I ended up using an app called VideoPad, and even though it is a paid app, it allows me to just cut an audio clips in smaller pieces and save them as new files. 

This is a sample audio waveform of one of the 4 videos

![image](https://github.com/user-attachments/assets/e4ba1d62-ac6f-4eff-a0c5-966a9d1b0e91)

What I did was, make short clips around each expression. I am not sure what these waves are called when there is speech. So for example from the above, I ended up with 36 clips, and uploaded them all (along with the other 3 videos' audio clips) to our project's drive. And the total amount we have so far is 4.75 minutes.  

### Secondly, today I read about eigenvalues' role in factor analysis

In my stats class at uni, we learned about factor analysis, and at the end of the chapter I saw the word eigenvalues, and I am glad because once again I will see their real world impact (after my dive into multicollinearity). 

Firstly, about factor analysis, here are the results after using unstandardized variables

![image](https://github.com/user-attachments/assets/0eccd38a-2aea-4e7d-b4b9-d1dec0ec6f20)

And after standardization

![image](https://github.com/user-attachments/assets/47e30f74-f69b-42b3-87ac-8367a7efcda3)

Why standardize?

* result interpretability
* helps with linearity
* treats variables equally

Where are the eigenvalues? The overall under each loading. 1.981 and 1.008. Which are sum of the square of each of the 3 values above it. 

![image](https://github.com/user-attachments/assets/07fe6713-40b1-447c-8b6f-d68ea1aec256)

To interpret this, we take the 1st row and F1: 0.00089222 - this is F1 accounts for 0.0089% of the variance in Y1 which is Finance, while F2 accounts for 99.90%. And in total, the 2 factor space accounts for 99.99% of the variance in Y1 Finance. 

 The eigenvalues can help to determine which factors to keep (i.e. using scree plots).
Love it when I see the math I studied for ML being used in practice, and *where* it is used.

### Thirdly, about Grokking ML

I decided to subscribe to manning.com and the 1st book I decided to read was Grokking ML (as it is one of the most recommended and popular ones). Today I managed to read the 1st 4 chapters(what is ML, types of ML, linear regression, optimization), and I can definitely see why it is popular for beginners, and I am excited to keep reading. 

### Finally, the last session from MLxFundamentals was delivered by Wenhan Han a PhD candidate from TU Eindhoven.

It was about loading and using an LLM, and a diffusion model

Some interesting bits from both parts are:

Question to an LLM:

How many kinds of human beings are there in the history?

We saw the top answers from the model. 

![image](https://github.com/user-attachments/assets/825a4c31-1f23-40ad-87ef-527517ffe9a9)

Various prompt strategies
Zero-shot

![image](https://github.com/user-attachments/assets/bb3bc5d6-fe04-49ab-8361-12cc93a57eff)

Few shot

![image](https://github.com/user-attachments/assets/28e0cbad-5f40-4cb4-a8f6-f7a38c915e2d)

Chain-of-thought

![image](https://github.com/user-attachments/assets/e5f39894-ac9f-4425-af28-2d499d52a1d1)

Then, how to finetune an LLM with 'unsloth'

![image](https://github.com/user-attachments/assets/9e641220-6cdf-4722-bea7-80b5e68d8440)

We add LoRA adapters

![image](https://github.com/user-attachments/assets/24f4cbd3-3e4a-4ce3-b43f-214abbd5a2b8)

Prepare the data

![image](https://github.com/user-attachments/assets/70b57dff-4493-44ec-9308-6e5116cb96ef)

Data was from huggingface

![image](https://github.com/user-attachments/assets/32f629b3-4a2c-4d80-a07d-dea72d53be6e)

Train the model

![image](https://github.com/user-attachments/assets/a0d3ac66-04b2-4a23-a779-8f72348b650f)

And then inference

![image](https://github.com/user-attachments/assets/e510e563-d72b-411d-af74-e6b5f409d9a2)

For finetuning I need to try unsloth by myself. For some part of the tutorial, an openai api key was required which was unfortunate because I do not have one, so I just watched that part. 

As for diffusion models

![image](https://github.com/user-attachments/assets/ee417c96-8777-427e-afa0-cbe420160585)

I saw the library 'diffusers' was used which I did not know.

![](https://blogger.googleusercontent.com/img/a/AVvXsEgflXnxK36P1DZbSvRH14jYJ-jcomrsGNeMrV-J1lPczAqwZJlIIC5oL-xlSMxVJGz-wMVEhLZrxD9Cko_f2xM2CfWDrHyOcDNFF5sL1-RSYTywH_poMSNtJ8F4pMsllY1ECPuH2pvn8J-JlW6PBA39ILlqCrj_lhYWbowhl_zXloy9qUZorQOltcolta08)

And I played around with it a bit, a picture I tried to get and failed was 'a horse riding an astronaut'

![](https://blogger.googleusercontent.com/img/a/AVvXsEhv9fWL3JUd3w50xfBAE5dTnd86eOly7-gX2CF1dSeqkFeoK1s2pnvTKRqaSXwGI4BRc1hw2905wcrLiiSOkzT3sfrl06hpAmfw4FGvSllGbIrXAlz8dxCz7boBaSkEMiIPZoP8PtuqWmSKtppAW7drYeEv-2zJopeQkD23Uc0A3mBIqybFgVV7pbFuNMr4)

That is all for today!

See you tomorrow :)
